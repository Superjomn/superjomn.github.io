<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=stylesheet type=text/css href=/css/bootstrap.min.css><link rel=stylesheet type=text/css href=/css/style.css><title>Superjomn's blog | Reduce kernel in CUDA</title>
<meta name=google-site-verification content="kO2XszgjIr1OyaOubGpeb0M34ADBz27vyI1dLETarCM"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-Z60BDN3JGH"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Z60BDN3JGH")</script><script src=https://cdn.jsdelivr.net/npm/cash-dom/dist/cash.min.js></script><script>$(function(){$("article table").addClass("table")})</script><body><div id=nav-border class=container><nav id=nav class="nav justify-content-center"><a class=nav-link href=/><i data-feather=home></i>
Home
</a><a class=nav-link href=/tags/tech><i data-feather=terminal></i>
Tech
</a><a class=nav-link href=/tags/life><i data-feather=coffee></i>
Life
</a><a class=nav-link href=/posts/><i data-feather=archive></i>
Archive
</a><a class=nav-link href=/tags/><i data-feather=tag></i>
Tags
</a><a class=nav-link href=/posts/about/><i data-feather=info></i>
About</a></nav></div><div class=container><main id=main><main><article><h1>Reduce kernel in CUDA</h1><i data-feather=calendar></i> <time datetime=2024-03-25>Mar 25, 2024</time>
<span id=social></span><i data-feather=tag></i>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/cuda>cuda</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/basics>basics</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/tech>tech</a><br><br><p><b>Table of Contents</b></p><aside><nav id=TableOfContents><ol><li><a href=#question-definition>Question definition</a></li><li><a href=#solutions>Solutions</a><ol><li><a href=#naive-version-with-atomicadd>Naive version with atomicAdd</a></li><li><a href=#tiled-reduction-with-shared-memory>Tiled reduction with shared memory</a><ol><li><a href=#basic-version>Basic version</a></li><li><a href=#avoid-thread-divergence>Avoid thread divergence</a></li><li><a href=#read-two-elements-one-time>Read two elements one time</a></li></ol></li><li><a href=#tiled-reduction-with-warp-shuffle>Tiled Reduction with Warp Shuffle</a></li><li><a href=#warp-shuffle-combined-with-atomic-operations>Warp Shuffle Combined with Atomic Operations</a></li></ol></li><li><a href=#benchmark>Benchmark</a></li><li><a href=#reference>Reference</a></li></ol></nav></aside><br><h2 id=question-definition>Question definition</h2><p>Given an array of \(n\) integers, the goal is to compute the sum of all elements within the array.</p><h2 id=solutions>Solutions</h2><p>The implementations for all kernel versions can be found at <a href=https://github.com/Superjomn/cuda-from-scratch/blob/dev/2-reduce.cu>2-reduce.cu on GitHub</a>.</p><h3 id=naive-version-with-atomicadd>Naive version with atomicAdd</h3><p>The simplest approach involves utilizing each thread to perform an <code>atomicAdd</code> operation on the output variable. Here&rsquo;s how the kernel is defined:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C++ data-lang=C++><span style=display:flex><span>__global__ <span style=color:#902000>void</span> <span style=color:#06287e>reduce_naive_atomic</span>(<span style=color:#902000>int</span><span style=color:#666>*</span> g_idata, <span style=color:#902000>int</span><span style=color:#666>*</span> g_odata, <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> n)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> idx <span style=color:#666>=</span> blockIdx.x <span style=color:#666>*</span> blockDim.x <span style=color:#666>+</span> threadIdx.x;
</span></span><span style=display:flex><span>    <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> gridSize <span style=color:#666>=</span> blockDim.x <span style=color:#666>*</span> gridDim.x;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> sum <span style=color:#666>=</span> <span style=color:#40a070>0</span>;
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> (<span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> i <span style=color:#666>=</span> idx; i <span style=color:#666>&lt;</span> n; i <span style=color:#666>+=</span> gridSize)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        sum <span style=color:#666>+=</span> g_idata[i];
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    atomicAdd(g_odata, sum);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>And the kernel launcher is straightforward, invoking the kernel a single time:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C++ data-lang=C++><span style=display:flex><span><span style=color:#902000>int</span> <span style=color:#06287e>launch_reduce</span>(<span style=color:#902000>int</span><span style=color:#666>*</span> g_idata, <span style=color:#902000>int</span><span style=color:#666>*</span> g_odata, <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> n, <span style=color:#902000>int</span> block_size, kernel_fn kernel, cudaStream_t stream)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span><span style=color:#666>*</span> idata <span style=color:#666>=</span> g_idata;
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span><span style=color:#666>*</span> odata <span style=color:#666>=</span> g_odata;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#902000>uint32_t</span> num_warps <span style=color:#666>=</span> block_size <span style=color:#666>/</span> <span style=color:#40a070>32</span>;
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> smem_size <span style=color:#666>=</span> num_warps <span style=color:#666>*</span> <span style=color:#007020;font-weight:700>sizeof</span>(<span style=color:#902000>int</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> num_blocks <span style=color:#666>=</span> ceil(n, block_size);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic>// Launch the kernel
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    kernel<span style=color:#666>&lt;&lt;&lt;</span>num_blocks, block_size, smem_size, stream<span style=color:#666>&gt;&gt;&gt;</span>(idata, odata, n);
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>if</span> (<span style=color:#666>!</span>FLAGS_profile)
</span></span><span style=display:flex><span>        cudaStreamSynchronize(stream);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic>// Copy the final result back to the host
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    <span style=color:#902000>int</span> h_out;
</span></span><span style=display:flex><span>    NVCHECK(cudaMemcpyAsync(<span style=color:#666>&amp;</span>h_out, odata, <span style=color:#007020;font-weight:700>sizeof</span>(<span style=color:#902000>int</span>), cudaMemcpyDeviceToHost, stream));
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>return</span> h_out;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>When tested on a GTX 4080, this method achieved a throughput of approximately 82GB/s.</p><h3 id=tiled-reduction-with-shared-memory>Tiled reduction with shared memory</h3><p>A classical approach involves leveraging a thread block to perform local reductions on a tile within shared memory.
This method encompasses several kernel versions, each with different optimizations.</p><h4 id=basic-version>Basic version</h4><p>The initial implementation is as below:</p><ol><li>A tile of data is collaboratively loaded into shared memory.</li><li>A partial reduction on this data tile is executed within a thread block, get the sum of the tile.</li><li>The sum is then written to a designated spot in the global memory&rsquo;s output slot. It&rsquo;s important to note that this kernel requires a temporary buffer for writing partial results from each thread block.</li><li>The process repeats with the size \(n\) reduced to \(\frac{n}{blockSize}\), continuing until \(n=1\).</li></ol><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C++ data-lang=C++><span style=display:flex><span>__global__ <span style=color:#902000>void</span> <span style=color:#06287e>reduce_smem_naive</span>(<span style=color:#902000>int</span><span style=color:#666>*</span> g_idata, <span style=color:#902000>int</span><span style=color:#666>*</span> g_odata, <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> n)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>extern</span> __shared__ <span style=color:#902000>int</span> sdata[];
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> tid <span style=color:#666>=</span> threadIdx.x;
</span></span><span style=display:flex><span>    <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> i <span style=color:#666>=</span> blockIdx.x <span style=color:#666>*</span> blockDim.x <span style=color:#666>+</span> threadIdx.x;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic>// Read a block of data into shared memory collectively
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    sdata[tid] <span style=color:#666>=</span> (i <span style=color:#666>&lt;</span> n) <span style=color:#666>?</span> g_idata[i] <span style=color:#666>:</span> <span style=color:#40a070>0</span>;
</span></span><span style=display:flex><span>    __syncthreads();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> (<span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> stride <span style=color:#666>=</span> <span style=color:#40a070>1</span>; stride <span style=color:#666>&lt;</span> blockDim.x; stride <span style=color:#666>*=</span> <span style=color:#40a070>2</span>)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#60a0b0;font-style:italic>// ISSUE: divergent warps
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>        <span style=color:#007020;font-weight:700>if</span> (tid <span style=color:#666>%</span> (<span style=color:#40a070>2</span> <span style=color:#666>*</span> stride) <span style=color:#666>==</span> <span style=color:#40a070>0</span>)
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            sdata[tid] <span style=color:#666>+=</span> sdata[tid <span style=color:#666>+</span> stride];
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        __syncthreads(); <span style=color:#60a0b0;font-style:italic>// need to sync per level
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic>// Write the result for this block to global memory
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    <span style=color:#007020;font-weight:700>if</span> (tid <span style=color:#666>==</span> <span style=color:#40a070>0</span>)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        g_odata[blockIdx.x] <span style=color:#666>=</span> sdata[<span style=color:#40a070>0</span>];
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Launching this kernel multiple times involves a slightly more complex launcher:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C++ data-lang=C++><span style=display:flex><span><span style=color:#902000>int</span> <span style=color:#06287e>launch_reduce</span>(<span style=color:#902000>int</span><span style=color:#666>*</span> g_idata, <span style=color:#902000>int</span><span style=color:#666>*</span> g_odata, <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> n, <span style=color:#902000>int</span> block_size, kernel_fn kernel, cudaStream_t stream,
</span></span><span style=display:flex><span>    <span style=color:#902000>uint32_t</span> num_blocks, <span style=color:#902000>uint32_t</span> smem_size)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span><span style=color:#666>*</span> idata <span style=color:#666>=</span> g_idata;
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span><span style=color:#666>*</span> odata <span style=color:#666>=</span> g_odata;
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>if</span> (smem_size <span style=color:#666>==</span> <span style=color:#40a070>0</span>)
</span></span><span style=display:flex><span>        smem_size <span style=color:#666>=</span> block_size <span style=color:#666>*</span> <span style=color:#007020;font-weight:700>sizeof</span>(<span style=color:#902000>int</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic>// Calculate the number of blocks
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    num_blocks <span style=color:#666>=</span> (num_blocks <span style=color:#666>&gt;</span> <span style=color:#40a070>0</span>) <span style=color:#666>?</span> <span style=color:#002070;font-weight:700>num_blocks</span> : (n <span style=color:#666>+</span> block_size <span style=color:#666>-</span> <span style=color:#40a070>1</span>) <span style=color:#666>/</span> block_size;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>if</span> (<span style=color:#666>!</span>FLAGS_profile)
</span></span><span style=display:flex><span>        printf(<span style=color:#4070a0>&#34;- launching: num_blocks: %d, block_size:%d, n:%d</span><span style=color:#4070a0;font-weight:700>\n</span><span style=color:#4070a0>&#34;</span>, num_blocks, block_size, n);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> level <span style=color:#666>=</span> <span style=color:#40a070>0</span>;
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic>// Launch the kernel
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    kernel<span style=color:#666>&lt;&lt;&lt;</span>num_blocks, block_size, smem_size, stream<span style=color:#666>&gt;&gt;&gt;</span>(idata, odata, n);
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>if</span> (<span style=color:#666>!</span>FLAGS_profile)
</span></span><span style=display:flex><span>        cudaStreamSynchronize(stream);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    level<span style=color:#666>++</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic>// Recursively reduce the partial sums
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    <span style=color:#007020;font-weight:700>while</span> (num_blocks <span style=color:#666>&gt;</span> <span style=color:#40a070>1</span>)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        std<span style=color:#666>::</span>swap(idata, odata);
</span></span><span style=display:flex><span>        n <span style=color:#666>=</span> num_blocks;
</span></span><span style=display:flex><span>        num_blocks <span style=color:#666>=</span> (n <span style=color:#666>+</span> block_size <span style=color:#666>-</span> <span style=color:#40a070>1</span>) <span style=color:#666>/</span> block_size;
</span></span><span style=display:flex><span>        kernel<span style=color:#666>&lt;&lt;&lt;</span>num_blocks, block_size, smem_size, stream<span style=color:#666>&gt;&gt;&gt;</span>(idata, odata, n);
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>if</span> (<span style=color:#666>!</span>FLAGS_profile)
</span></span><span style=display:flex><span>            cudaStreamSynchronize(stream);
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic>// Copy the final result back to the host
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    <span style=color:#902000>int</span> h_out;
</span></span><span style=display:flex><span>    NVCHECK(cudaMemcpyAsync(<span style=color:#666>&amp;</span>h_out, odata, <span style=color:#007020;font-weight:700>sizeof</span>(<span style=color:#902000>int</span>), cudaMemcpyDeviceToHost, stream));
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>return</span> h_out;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>All tiled reduction kenrels utilize the aforementioned launcher, achieving a throughput of 54GB/s. This is less efficient compared to the atomic naive version, which reaches 82GB/s.</p><h4 id=avoid-thread-divergence>Avoid thread divergence</h4><p>The basic version encounters significant thread divergence, particularly noticeable at <code>if (tid % (2 * stride) == 0)</code>.
Here is an optimized variant:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C++ data-lang=C++><span style=display:flex><span>__global__ <span style=color:#902000>void</span> <span style=color:#06287e>reduce_smem_1_avoid_divergent_warps</span>(<span style=color:#902000>int</span><span style=color:#666>*</span> g_idata, <span style=color:#902000>int</span><span style=color:#666>*</span> g_odata, <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> n)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>extern</span> __shared__ <span style=color:#902000>int</span> sdata[];
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> tid <span style=color:#666>=</span> threadIdx.x;
</span></span><span style=display:flex><span>    <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> i <span style=color:#666>=</span> blockIdx.x <span style=color:#666>*</span> blockDim.x <span style=color:#666>+</span> threadIdx.x;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    sdata[tid] <span style=color:#666>=</span> (i <span style=color:#666>&lt;</span> n) <span style=color:#666>?</span> g_idata[i] <span style=color:#666>:</span> <span style=color:#40a070>0</span>;
</span></span><span style=display:flex><span>    __syncthreads();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> (<span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> stride <span style=color:#666>=</span> <span style=color:#40a070>1</span>; stride <span style=color:#666>&lt;</span> blockDim.x; stride <span style=color:#666>*=</span> <span style=color:#40a070>2</span>)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#902000>int</span> index <span style=color:#666>=</span> <span style=color:#40a070>2</span> <span style=color:#666>*</span> stride <span style=color:#666>*</span> tid;
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>if</span> (index <span style=color:#666>&lt;</span> blockDim.x)
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#60a0b0;font-style:italic>// Issue: bank conflict
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>            sdata[index] <span style=color:#666>+=</span> sdata[index <span style=color:#666>+</span> stride];
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        __syncthreads();
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>if</span> (tid <span style=color:#666>==</span> <span style=color:#40a070>0</span>)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        g_odata[blockIdx.x] <span style=color:#666>=</span> sdata[<span style=color:#40a070>0</span>];
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The optimization yields a 70GB/s throughput, marking a 29% improvement over the basic version.</p><h4 id=read-two-elements-one-time>Read two elements one time</h4><p>The preceding version&rsquo;s DRAM throughput was only 20.63%, likely due to</p><ol><li>Insufficient grid size for small inputs, leading to underutilized thread resources.</li><li>Each thread reading a single element at a time, given the fixed number of resident thread blocks per SM for a specific kernel, results in a limited number of load instructions issued.</li></ol><p>To enhance DRAM throughput, especially for smaller grid sizes, threads can be configured to read more than one element at a time.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C++ data-lang=C++><span style=display:flex><span>__global__ <span style=color:#902000>void</span> <span style=color:#06287e>reduce_smem_3_read_two</span>(<span style=color:#902000>int</span><span style=color:#666>*</span> g_idata, <span style=color:#902000>int</span><span style=color:#666>*</span> g_odata, <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> n)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>extern</span> __shared__ <span style=color:#902000>int</span> sdata[];
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> tid <span style=color:#666>=</span> threadIdx.x;
</span></span><span style=display:flex><span>    <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> i <span style=color:#666>=</span> blockIdx.x <span style=color:#666>*</span> (blockDim.x <span style=color:#666>*</span> <span style=color:#40a070>2</span>) <span style=color:#666>+</span> threadIdx.x;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007020>#define GET_ELEM(__idx) ((__idx) &lt; n ? g_idata[(__idx)] : 0)
</span></span></span><span style=display:flex><span><span style=color:#007020></span>
</span></span><span style=display:flex><span>    sdata[tid] <span style=color:#666>=</span> GET_ELEM(i) <span style=color:#666>+</span> GET_ELEM(i <span style=color:#666>+</span> blockDim.x);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    __syncthreads();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> (<span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> stride <span style=color:#666>=</span> <span style=color:#40a070>1</span>; stride <span style=color:#666>&lt;</span> blockDim.x; stride <span style=color:#666>*=</span> <span style=color:#40a070>2</span>)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#902000>int</span> index <span style=color:#666>=</span> <span style=color:#40a070>2</span> <span style=color:#666>*</span> stride <span style=color:#666>*</span> tid;
</span></span><span style=display:flex><span>        <span style=color:#007020;font-weight:700>if</span> (index <span style=color:#666>&lt;</span> blockDim.x)
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#60a0b0;font-style:italic>// Issue: bank conflict
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>            sdata[index] <span style=color:#666>+=</span> sdata[index <span style=color:#666>+</span> stride];
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        __syncthreads();
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>if</span> (tid <span style=color:#666>==</span> <span style=color:#40a070>0</span>)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        g_odata[blockIdx.x] <span style=color:#666>=</span> sdata[<span style=color:#40a070>0</span>];
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>This approach improves the DRAM Throughput to 33.78%, a significant 63.72% increase over the previous method.
The overall throughput reaches 96.51GB/s, demonstrating 37.87% enhancement from the 70GB/s achieved earlier.</p><h3 id=tiled-reduction-with-warp-shuffle>Tiled Reduction with Warp Shuffle</h3><p>Modern GPUs facilitate direct data exchange within a warp, bypassing the need for shared memory.</p><p>The function below demonstrates how to conduct a reduction within a single warp using the warp shuffle instruction, as highlighted in the book &lt;Professional CUDA C Programming>.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C++ data-lang=C++><span style=display:flex><span><span style=color:#60a0b0;font-style:italic>// using warp shuffle instruction
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic>// From book &lt;Professional CUDA C Programming&gt;
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>__inline__ __device__ <span style=color:#902000>int</span> <span style=color:#06287e>warpReduce</span>(<span style=color:#902000>int</span> mySum)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    mySum <span style=color:#666>+=</span> __shfl_xor(mySum, <span style=color:#40a070>16</span>);
</span></span><span style=display:flex><span>    mySum <span style=color:#666>+=</span> __shfl_xor(mySum, <span style=color:#40a070>8</span>);
</span></span><span style=display:flex><span>    mySum <span style=color:#666>+=</span> __shfl_xor(mySum, <span style=color:#40a070>4</span>);
</span></span><span style=display:flex><span>    mySum <span style=color:#666>+=</span> __shfl_xor(mySum, <span style=color:#40a070>2</span>);
</span></span><span style=display:flex><span>    mySum <span style=color:#666>+=</span> __shfl_xor(mySum, <span style=color:#40a070>1</span>);
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>return</span> mySum;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>When a thread block contains multiple warps, synchronization becomes essential.
Utilizing shared memory to store the sum computed by each warp and subsequently reducing these sums as previously described enables the calculation of a thread block&rsquo;s total sum.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C++ data-lang=C++><span style=display:flex><span>__global__ <span style=color:#902000>void</span> <span style=color:#06287e>reduce_warp_shlf</span>(<span style=color:#902000>int</span><span style=color:#666>*</span> g_idata, <span style=color:#902000>int</span><span style=color:#666>*</span> g_odata, <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> n)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic>// Helps to share data between warps
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    <span style=color:#60a0b0;font-style:italic>// size should be (blockDim.x / warpSize)
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    <span style=color:#007020;font-weight:700>extern</span> __shared__ <span style=color:#902000>int</span> sdata[];
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> idx <span style=color:#666>=</span> blockIdx.x <span style=color:#666>*</span> blockDim.x <span style=color:#666>+</span> threadIdx.x;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic>// Necessary to make sure shfl instruction is not used with uninitialized data
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    <span style=color:#902000>int</span> mySum <span style=color:#666>=</span> idx <span style=color:#666>&lt;</span> n <span style=color:#666>?</span> g_idata[idx] <span style=color:#666>:</span> <span style=color:#40a070>0</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> lane <span style=color:#666>=</span> threadIdx.x <span style=color:#666>%</span> warpSize;
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> warp <span style=color:#666>=</span> threadIdx.x <span style=color:#666>/</span> warpSize;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    mySum <span style=color:#666>=</span> warpReduce(mySum);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>if</span> (lane <span style=color:#666>==</span> <span style=color:#40a070>0</span>)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        sdata[warp] <span style=color:#666>=</span> mySum;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    __syncthreads();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic>// last warp reduce
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    mySum <span style=color:#666>=</span> (threadIdx.x <span style=color:#666>&lt;</span> blockDim.x <span style=color:#666>/</span> warpSize) <span style=color:#666>?</span> sdata[lane] <span style=color:#666>:</span> <span style=color:#40a070>0</span>;
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>if</span> (warp <span style=color:#666>==</span> <span style=color:#40a070>0</span>)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        mySum <span style=color:#666>=</span> warpReduce(mySum);
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>if</span> (threadIdx.x <span style=color:#666>==</span> <span style=color:#40a070>0</span>)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        g_odata[blockIdx.x] <span style=color:#666>=</span> mySum;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Despite reading only a single element per thread, this kernel can achieve a throughput of 96GB/s, outperforming the shared memory version&rsquo;s 70GB/s.
Furthermore, the kernel can be modified to read \(NT\) elements at a time for enhanced efficiency:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C++ data-lang=C++><span style=display:flex><span><span style=color:#007020;font-weight:700>template</span> <span style=color:#666>&lt;</span><span style=color:#902000>int</span> NT<span style=color:#666>&gt;</span>
</span></span><span style=display:flex><span>__global__ <span style=color:#902000>void</span> reduce_warp_shlf_read_N(<span style=color:#902000>int</span><span style=color:#666>*</span> g_idata, <span style=color:#902000>int</span><span style=color:#666>*</span> g_odata, <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> n)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic>// Helps to share data between warps
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    <span style=color:#60a0b0;font-style:italic>// size should be (blockDim.x / warpSize)
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    <span style=color:#007020;font-weight:700>extern</span> __shared__ <span style=color:#902000>int</span> sdata[];
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> blockSize <span style=color:#666>=</span> NT <span style=color:#666>*</span> blockDim.x;
</span></span><span style=display:flex><span>    <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> idx <span style=color:#666>=</span> blockIdx.x <span style=color:#666>*</span> blockSize <span style=color:#666>+</span> threadIdx.x;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic>// Necessary to make sure shfl instruction is not used with uninitialized data
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span><span style=color:#007020>#define GET_ELEM(__idx) ((__idx) &lt; n ? g_idata[(__idx)] : 0)
</span></span></span><span style=display:flex><span><span style=color:#007020></span>    <span style=color:#902000>int</span> mySum <span style=color:#666>=</span> <span style=color:#40a070>0</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007020>#pragma unroll
</span></span></span><span style=display:flex><span><span style=color:#007020></span>    <span style=color:#007020;font-weight:700>for</span> (<span style=color:#902000>int</span> i <span style=color:#666>=</span> <span style=color:#40a070>0</span>; i <span style=color:#666>&lt;</span> NT; i<span style=color:#666>++</span>)
</span></span><span style=display:flex><span>        mySum <span style=color:#666>+=</span> GET_ELEM(idx <span style=color:#666>+</span> i <span style=color:#666>*</span> blockDim.x);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> lane <span style=color:#666>=</span> threadIdx.x <span style=color:#666>%</span> warpSize;
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> warp <span style=color:#666>=</span> threadIdx.x <span style=color:#666>/</span> warpSize;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    mySum <span style=color:#666>=</span> warpReduce(mySum);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>if</span> (lane <span style=color:#666>==</span> <span style=color:#40a070>0</span>)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        sdata[warp] <span style=color:#666>=</span> mySum;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    __syncthreads();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic>// last warp reduce
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    mySum <span style=color:#666>=</span> (threadIdx.x <span style=color:#666>&lt;</span> blockDim.x <span style=color:#666>/</span> warpSize) <span style=color:#666>?</span> sdata[lane] <span style=color:#666>:</span> <span style=color:#40a070>0</span>;
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>if</span> (warp <span style=color:#666>==</span> <span style=color:#40a070>0</span>)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        mySum <span style=color:#666>=</span> warpReduce(mySum);
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>if</span> (threadIdx.x <span style=color:#666>==</span> <span style=color:#40a070>0</span>)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        g_odata[blockIdx.x] <span style=color:#666>=</span> mySum;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Performance varies with different \(N\) values, as summarized below:</p><table><thead><tr><th>NT</th><th>throughput (GB/s)</th></tr></thead><tbody><tr><td>1</td><td>96.3187</td></tr><tr><td>2</td><td>96.2341</td></tr><tr><td>4</td><td>96.8153</td></tr><tr><td>8</td><td>107.226</td></tr></tbody></table><h3 id=warp-shuffle-combined-with-atomic-operations>Warp Shuffle Combined with Atomic Operations</h3><p>Compared to tiled reduction solutions, utilizing <code>atomicAdd</code> eliminates the need for a temporary buffer and requires only a single kernel launch.
This segment explores combining warp shuffle and atomic operations for efficient reduction.</p><p>The kernel template below demonstrates this approach, utilizing warp shuffle instructions to enhance the warp reduction performance, and leveraging atomic operations to write directly to the output slot without the need for temporary buffer and multiple kernel launches.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C++ data-lang=C++><span style=display:flex><span><span style=color:#007020;font-weight:700>template</span> <span style=color:#666>&lt;</span><span style=color:#902000>int</span> NT<span style=color:#666>&gt;</span>
</span></span><span style=display:flex><span>__global__ <span style=color:#902000>void</span> reduce_warp_shlf_read_N_atomic(<span style=color:#902000>int</span><span style=color:#666>*</span> g_idata, <span style=color:#902000>int</span><span style=color:#666>*</span> g_odata, <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> n)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic>// Helps to share data between warps
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    <span style=color:#60a0b0;font-style:italic>// size should be (blockDim.x / warpSize)
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    <span style=color:#007020;font-weight:700>extern</span> __shared__ <span style=color:#902000>int</span> sdata[];
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> blockSize <span style=color:#666>=</span> NT <span style=color:#666>*</span> blockDim.x;
</span></span><span style=display:flex><span>    <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> idx <span style=color:#666>=</span> blockIdx.x <span style=color:#666>*</span> blockSize <span style=color:#666>+</span> threadIdx.x;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic>// Necessary to make sure shfl instruction is not used with uninitialized data
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic>// This only needs one turn of launch
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span><span style=color:#007020>#define GET_ELEM(__idx) ((__idx) &lt; n ? g_idata[(__idx)] : 0)
</span></span></span><span style=display:flex><span><span style=color:#007020></span>    <span style=color:#902000>int</span> mySum <span style=color:#666>=</span> <span style=color:#40a070>0</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007020>#pragma unroll
</span></span></span><span style=display:flex><span><span style=color:#007020></span>    <span style=color:#007020;font-weight:700>for</span> (<span style=color:#902000>int</span> i <span style=color:#666>=</span> <span style=color:#40a070>0</span>; i <span style=color:#666>&lt;</span> NT; i<span style=color:#666>++</span>)
</span></span><span style=display:flex><span>        mySum <span style=color:#666>+=</span> GET_ELEM(idx <span style=color:#666>+</span> i <span style=color:#666>*</span> blockDim.x);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> lane <span style=color:#666>=</span> threadIdx.x <span style=color:#666>%</span> warpSize;
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> warp <span style=color:#666>=</span> threadIdx.x <span style=color:#666>/</span> warpSize;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    mySum <span style=color:#666>=</span> warpReduce(mySum);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>if</span> (lane <span style=color:#666>==</span> <span style=color:#40a070>0</span>)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        sdata[warp] <span style=color:#666>=</span> mySum;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    __syncthreads();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic>// last warp reduce
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    mySum <span style=color:#666>=</span> (threadIdx.x <span style=color:#666>&lt;</span> blockDim.x <span style=color:#666>/</span> warpSize) <span style=color:#666>?</span> sdata[lane] <span style=color:#666>:</span> <span style=color:#40a070>0</span>;
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>if</span> (warp <span style=color:#666>==</span> <span style=color:#40a070>0</span>)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        mySum <span style=color:#666>=</span> warpReduce(mySum);
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>if</span> (threadIdx.x <span style=color:#666>==</span> <span style=color:#40a070>0</span>)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        atomicAdd(g_odata, mySum);
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Remarkably, this kernel achieves a throughput of 121.777 GB/s under the same conditions.</p><h2 id=benchmark>Benchmark</h2><p>The benchmark results illustrate the performance of different CUDA optimization strategies under varying conditions.</p><figure><img src=/ox-hugo/2024-04-06_16-47-39_screenshot.png></figure><p>Note that the optimal kernel configuration may vary depending on the size of the input data(\(n\)).</p><h2 id=reference>Reference</h2><ul><li><a href=https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf>Optimizing Parallel Reduction in CUDA</a></li><li><a href=https://developer.nvidia.com/blog/faster-parallel-reductions-kepler/>Faster Parallel Reductions on Kepler | NVIDIA Technical Blog</a></li></ul><script src=https://utteranc.es/client.js repo=superjomn/superjomn.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><p class="footer text-center">Copyright (c) 2024 Chunwei Yan</p></main></div><script src=/js/feather.min.js></script><script>feather.replace()</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script></body></html>