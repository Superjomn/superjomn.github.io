<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=stylesheet type=text/css href=/css/bootstrap.min.css><link rel=stylesheet type=text/css href=/css/style.css><title>Superjomn's blog | Memory coalescing in CUDA (1) – VecAdd</title>
<meta name=google-site-verification content="kO2XszgjIr1OyaOubGpeb0M34ADBz27vyI1dLETarCM"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-Z60BDN3JGH"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Z60BDN3JGH")</script><script src=https://cdn.jsdelivr.net/npm/cash-dom/dist/cash.min.js></script><script>$(function(){$("article table").addClass("table")})</script><body><div id=nav-border class=container><nav id=nav class="nav justify-content-center"><a class=nav-link href=/><i data-feather=home></i>
Home
</a><a class=nav-link href=/tags/tech><i data-feather=terminal></i>
Tech
</a><a class=nav-link href=/tags/life><i data-feather=coffee></i>
Life
</a><a class=nav-link href=/posts/><i data-feather=archive></i>
Archive
</a><a class=nav-link href=/tags/><i data-feather=tag></i>
Tags
</a><a class=nav-link href=/posts/about/><i data-feather=info></i>
About</a></nav></div><div class=container><main id=main><main><article><h1>Memory coalescing in CUDA (1) – VecAdd</h1><i data-feather=calendar></i> <time datetime=2024-02-25>Feb 25, 2024</time>
<span id=social></span><i data-feather=tag></i>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/cuda>cuda</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/basics>basics</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/tech>tech</a><br><br><p><b>Table of Contents</b></p><aside><nav id=TableOfContents><ol><li><a href=#background>Background</a></li><li><a href=#vecadd>VecAdd</a><ol><li><a href=#naive-vecadd-kernel-with-memory-coalescing-enabled>Naive VecAdd kernel with memory coalescing enabled</a></li><li><a href=#optimized-one-strided-with-less-threads>Optimized one: strided with less threads</a></li><li><a href=#uncoalesced-memory-accessing-one>Uncoalesced memory accessing one</a></li></ol></li><li><a href=#performance>Performance</a></li><li><a href=#references>References</a></li></ol></nav></aside><br><h2 id=background>Background</h2><p><strong>Memory coalescing</strong> is a crucial optimization technique in CUDA programming that allows optimal usage of the <strong>global memory bandwidth</strong>. When threads in the same warp running the same instruction access to <strong>consecutive locations</strong> in the global memory, the hardware can coalesce these accesses into a single transaction, significantly improving performance.</p><p>Coalescing memory access is vital for achieving high performance. Besides PCIe memory traffic, accessing global memory tends to be the largest bottleneck in GPU&rsquo;s memory hierarchy.
Non-coalesced memory access can lead to underutilization of memory bandwidth.</p><p>In the following post, we will delve deeper into memory coalescing with CUDA code for the classical vector adding.</p><h2 id=vecadd>VecAdd</h2><p>There are three kernels in below. The complete code locates <a href=https://github.com/Superjomn/cuda-from-scratch/blob/dev/0-vecadd-memory-coalesce.cu>here</a>.</p><h3 id=naive-vecadd-kernel-with-memory-coalescing-enabled>Naive VecAdd kernel with memory coalescing enabled</h3><p>The first program is simple but follows the coalescing memory access pattern:</p><table><thead><tr><th>tid</th><th>element</th></tr></thead><tbody><tr><td>0</td><td>0</td></tr><tr><td>1</td><td>1</td></tr><tr><td>2</td><td>2</td></tr><tr><td>3</td><td>3</td></tr><tr><td>&mldr;</td><td>&mldr;</td></tr></tbody></table><p>The thread 0,1,2,3 visits elements 0,1,2,3, which is contiguous, and results in a coalescing memory accessing.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#007020;font-weight:700>template</span> <span style=color:#666>&lt;</span><span style=color:#007020;font-weight:700>typename</span> T<span style=color:#666>&gt;</span>
</span></span><span style=display:flex><span>__global__ <span style=color:#902000>void</span> add_coalesced0(
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>const</span> T<span style=color:#666>*</span> __restrict__ a,
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>const</span> T<span style=color:#666>*</span> __restrict__ b,
</span></span><span style=display:flex><span>    T<span style=color:#666>*</span> __restrict__ c,
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> n) {
</span></span><span style=display:flex><span>  <span style=color:#902000>int</span> i <span style=color:#666>=</span> blockIdx.x <span style=color:#666>*</span> blockDim.x <span style=color:#666>+</span> threadIdx.x;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>if</span> (i <span style=color:#666>&lt;</span> n) {
</span></span><span style=display:flex><span>    c[i] <span style=color:#666>=</span> a[i] <span style=color:#666>+</span> b[i];
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The only issue is that, the number of the elements should be no larger than the number of threads, so the launching parameters of the kernel should be carefully designed.</p><h3 id=optimized-one-strided-with-less-threads>Optimized one: strided with less threads</h3><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#007020;font-weight:700>template</span> <span style=color:#666>&lt;</span><span style=color:#007020;font-weight:700>typename</span> T<span style=color:#666>&gt;</span>
</span></span><span style=display:flex><span>__global__ <span style=color:#902000>void</span> add_coalesced1(
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>const</span> T<span style=color:#666>*</span> __restrict__ a,
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>const</span> T<span style=color:#666>*</span> __restrict__ b,
</span></span><span style=display:flex><span>    T<span style=color:#666>*</span> __restrict__ c,
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> N) {
</span></span><span style=display:flex><span>  <span style=color:#902000>int</span> tid <span style=color:#666>=</span> threadIdx.x <span style=color:#666>+</span> blockIdx.x <span style=color:#666>*</span> blockDim.x;
</span></span><span style=display:flex><span>  <span style=color:#902000>int</span> num_threads <span style=color:#666>=</span> blockDim.x <span style=color:#666>*</span> gridDim.x;
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>while</span> (tid <span style=color:#666>&lt;</span> N) {
</span></span><span style=display:flex><span>    c[tid] <span style=color:#666>=</span> a[tid] <span style=color:#666>+</span> b[tid];
</span></span><span style=display:flex><span>    tid <span style=color:#666>+=</span> num_threads;
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>This one simplifies the calculation of the launch thread number, it should fit any number of elements with a arbitrary number of threads.</p><h3 id=uncoalesced-memory-accessing-one>Uncoalesced memory accessing one</h3><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#007020;font-weight:700>template</span> <span style=color:#666>&lt;</span><span style=color:#007020;font-weight:700>typename</span> T<span style=color:#666>&gt;</span>
</span></span><span style=display:flex><span>__global__ <span style=color:#902000>void</span> add_uncoalesced(
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>const</span> T<span style=color:#666>*</span> __restrict__ a,
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>const</span> T<span style=color:#666>*</span> __restrict__ b,
</span></span><span style=display:flex><span>    T<span style=color:#666>*</span> __restrict__ c,
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> n) {
</span></span><span style=display:flex><span>  <span style=color:#902000>int</span> tid <span style=color:#666>=</span> threadIdx.x <span style=color:#666>+</span> blockIdx.x <span style=color:#666>*</span> blockDim.x;
</span></span><span style=display:flex><span>  <span style=color:#902000>int</span> num_threads <span style=color:#666>=</span> blockDim.x <span style=color:#666>*</span> gridDim.x;
</span></span><span style=display:flex><span>  <span style=color:#902000>int</span> num_tasks <span style=color:#666>=</span> nvceil(n, num_threads);
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>for</span> (<span style=color:#902000>int</span> i <span style=color:#666>=</span> <span style=color:#40a070>0</span>; i <span style=color:#666>&lt;</span> num_tasks; <span style=color:#666>++</span>i) {
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> idx <span style=color:#666>=</span> tid <span style=color:#666>*</span> num_tasks <span style=color:#666>+</span> i;
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>if</span> (idx <span style=color:#666>&lt;</span> n) {
</span></span><span style=display:flex><span>      c[idx] <span style=color:#666>=</span> a[idx] <span style=color:#666>+</span> b[idx];
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>This one doesn&rsquo;t follow the coalescing access pattern, lets assume that we have 4 threads with 8 elements, then the `num_tasks=2`</p><table><thead><tr><th>tid</th><th>0-th element</th><th>1-st element</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>1</td></tr><tr><td>1</td><td>2</td><td>3</td></tr><tr><td>2</td><td>4</td><td>5</td></tr><tr><td>3</td><td>6</td><td>7</td></tr></tbody></table><p>In the first step of the for-loop, these four threads visit 0,2,4,6 elements, which is not contiguous, this results in an uncoalesced memory accessing.</p><h2 id=performance>Performance</h2><p>All the kernels are tested with double data type, and the block size is 256, for the last kernels, each thread are setted to consume 8 elements.
The performance is tested on GTX 3090, with the clocks locked as below:</p><table><thead><tr><th>GPU clocks</th><th>Memory clocks</th></tr></thead><tbody><tr><td>2100 MHZ</td><td>9501MHZ</td></tr></tbody></table><p>The latency of each kernel:</p><table><thead><tr><th>kernel</th><th>latency</th></tr></thead><tbody><tr><td>coalesced0</td><td>0.04</td></tr><tr><td>coalesced1</td><td>0.04</td></tr><tr><td>uncoalesced</td><td>0.14</td></tr></tbody></table><p>The uncoalesced kernel is 3x slower than the two coalesced kernel.</p><p>The Nsight also report the Uncoalescing Global Accesses in the uncoalesced kernel:</p><figure><img src=/ox-hugo/2024-02-28_19-37-47_screenshot.png></figure><p>It reports that 75% of the sectors are excessive, IIUC, since only 8 bytes(a double) out each 32 byte transition is valid, so the overall efficiency is \(\frac{8}{32}=25\%\) .</p><h2 id=references>References</h2><ul><li>Professional CUDA C Programming</li></ul><script src=https://utteranc.es/client.js repo=superjomn/superjomn.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><p class="footer text-center">Copyright (c) 2024 Chunwei Yan</p></main></div><script src=/js/feather.min.js></script><script>feather.replace()</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script></body></html>