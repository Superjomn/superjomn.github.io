<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=stylesheet type=text/css href=/css/bootstrap.min.css><link rel=stylesheet type=text/css href=/css/style.css><title>Superjomn's blog | Count the parameters in LLaMA V1 model</title>
<meta name=google-site-verification content="kO2XszgjIr1OyaOubGpeb0M34ADBz27vyI1dLETarCM"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-Z60BDN3JGH"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Z60BDN3JGH")</script><script src=https://cdn.jsdelivr.net/npm/cash-dom/dist/cash.min.js></script><script>$(function(){$("article table").addClass("table")})</script><body><div id=nav-border class=container><nav id=nav class="nav justify-content-center"><a class=nav-link href=/><i data-feather=home></i>
Home
</a><a class=nav-link href=/tags/tech><i data-feather=terminal></i>
Tech
</a><a class=nav-link href=/tags/life><i data-feather=coffee></i>
Life
</a><a class=nav-link href=/posts/><i data-feather=archive></i>
Archive
</a><a class=nav-link href=/tags/><i data-feather=tag></i>
Tags
</a><a class=nav-link href=/posts/about/><i data-feather=info></i>
About</a></nav></div><div class=container><main id=main><main><article><h1>Count the parameters in LLaMA V1 model</h1><i data-feather=calendar></i> <time datetime=2024-03-21>Mar 21, 2024</time>
<span id=social></span><i data-feather=tag></i>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/llm>LLM</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/tech>tech</a><br><br><p><b>Table of Contents</b></p><aside><nav id=TableOfContents><ol><li><a href=#the-basic-setting-of-the-7b-model>The basic setting of the 7B model</a></li><li><a href=#layer-by-layer-parameter-count>Layer-by-Layer Parameter Count</a><ol><li><a href=#embedding-layer>Embedding layer</a></li><li><a href=#transformer-layers>Transformer layers</a><ol><li><a href=#input-layernorm-and-post-attention-layernorm><code>input_layernorm</code> and <code>post_attention_layernorm</code></a></li><li><a href=#multi-head-self-attention>multi-head self-attention</a></li><li><a href=#mlp>mlp</a></li></ol></li></ol></li><li><a href=#total-count-of-parameters>Total count of parameters</a></li><li><a href=#references>References</a></li></ol></nav></aside><br><p>Let&rsquo;s load the model</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#007020;font-weight:700>from</span> <span style=color:#0e84b5;font-weight:700>transformers</span> <span style=color:#007020;font-weight:700>import</span> LlamaModel, LlamaConfig
</span></span><span style=display:flex><span>model <span style=color:#666>=</span> LlamaModel<span style=color:#666>.</span>from_pretrained(<span style=color:#4070a0>&#34;llama-7b-hf-path&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>def</span> <span style=color:#06287e>count_params</span>(model, is_human: <span style=color:#007020>bool</span> <span style=color:#666>=</span> <span style=color:#007020;font-weight:700>False</span>):
</span></span><span style=display:flex><span>    params: <span style=color:#007020>int</span> <span style=color:#666>=</span> <span style=color:#007020>sum</span>(p<span style=color:#666>.</span>numel() <span style=color:#007020;font-weight:700>for</span> p <span style=color:#007020;font-weight:700>in</span> model<span style=color:#666>.</span>parameters() <span style=color:#007020;font-weight:700>if</span> p<span style=color:#666>.</span>requires_grad)
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>return</span> <span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;</span><span style=color:#70a0d0>{</span>params <span style=color:#666>/</span> <span style=color:#40a070>1e6</span><span style=color:#70a0d0>:</span><span style=color:#4070a0>.2f</span><span style=color:#70a0d0>}</span><span style=color:#4070a0>M&#34;</span> <span style=color:#007020;font-weight:700>if</span> is_human <span style=color:#007020;font-weight:700>else</span> params
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007020>print</span>(model)
</span></span><span style=display:flex><span><span style=color:#007020>print</span>(<span style=color:#4070a0>&#34;Total # of params:&#34;</span>, count_params(model, is_human<span style=color:#666>=</span><span style=color:#007020;font-weight:700>True</span>))
</span></span></code></pre></div><p>Print out the layers:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>LlamaModel(
</span></span><span style=display:flex><span>  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
</span></span><span style=display:flex><span>  (layers): ModuleList(
</span></span><span style=display:flex><span>    (0-31): 32 x LlamaDecoderLayer(
</span></span><span style=display:flex><span>      (self_attn): LlamaSdpaAttention(
</span></span><span style=display:flex><span>        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
</span></span><span style=display:flex><span>        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
</span></span><span style=display:flex><span>        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
</span></span><span style=display:flex><span>        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
</span></span><span style=display:flex><span>        (rotary_emb): LlamaRotaryEmbedding()
</span></span><span style=display:flex><span>      )
</span></span><span style=display:flex><span>      (mlp): LlamaMLP(
</span></span><span style=display:flex><span>        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
</span></span><span style=display:flex><span>        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
</span></span><span style=display:flex><span>        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
</span></span><span style=display:flex><span>        (act_fn): SiLU()
</span></span><span style=display:flex><span>      )
</span></span><span style=display:flex><span>      (input_layernorm): LlamaRMSNorm()
</span></span><span style=display:flex><span>      (post_attention_layernorm): LlamaRMSNorm()
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (norm): LlamaRMSNorm()
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>Total # of params: 6607.34M
</span></span></code></pre></div><p>The Transformers shows that there are 6607.34M float16 parameters, roughly 13GB, that is aligned to the actual weight size.</p><h2 id=the-basic-setting-of-the-7b-model>The basic setting of the 7B model</h2><ul><li>model dimension \(d_{model}=4096\)</li><li>number of heads \(n_{head}=32\)</li><li>head size \(d_{head} = \frac{d_{model}}{n_{head}}\)</li><li>dimension of the feed-forward network&rsquo;s inner layer \(d_{ff}=11008\)</li><li>number of tokens \(n_{token}=32000\)</li><li>number of transformer layers \(n_{layer}=32\)</li></ul><h2 id=layer-by-layer-parameter-count>Layer-by-Layer Parameter Count</h2><h3 id=embedding-layer>Embedding layer</h3><p>For vocabulary embedding, \(n_{token}\times d_{model}=131.072M\), while for position embedding, since RoPE doesn&rsquo;t need a separate embedding, so that is 0.</p><h3 id=transformer-layers>Transformer layers</h3><h4 id=input-layernorm-and-post-attention-layernorm><code>input_layernorm</code> and <code>post_attention_layernorm</code></h4><p>Both are RMSNorm whose parameters are \(d_{model}\), so both sum to \(2\times d_{model}=8M\)</p><h4 id=multi-head-self-attention>multi-head self-attention</h4><p>For Q,K,V and O, each is a Linear layer of size \(d_{model} \times d_{model}\), so in total, there are \(4\times d_{model}^2=67.1M\).</p><p>There is one tiny issue here, why a linear layer could generate Q, while in the original transformer paper, each head is calculated separately, for example, \(Q_i=QW^Q_i\) where \(i\) is the head id. That is because, if we concatenate all all the heads, that is identical to a linear of \(d_{model} \times (n_{head} \times d_{head})\), that is \(d_{model} \times d_{model}\) in llama v1.</p><p>The self-attention doesn&rsquo;t have extra parameters since they simply applies the following formula</p><p>\[
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
\]</p><h4 id=mlp>mlp</h4><p>The LlamaMLP layer contains three separate Linear layers:</p><ol><li><code>gate_proj</code>: \(d_{model} \times d_{ff}\)</li><li><code>up_proj</code>: \(d_{model} \times d_{ff}\)</li><li><code>down_proj</code>: \(d_{ff} \times d_{model}\)</li></ol><p>So in total, they have \(3\times d_{model} \times d_{ff} = 135.27M\) parameters.</p><h2 id=total-count-of-parameters>Total count of parameters</h2><p>The overall parameters are composed of two major parts, the vocabulary embedding, and the transformer layers, that is <code>embed + 32 * (mha + mlp + norm)</code>:</p><ul><li>\(embed=n_{token}\times d_{model}=131.07M\)</li><li>\(mha=4* d_{model}^2=67.1M\)</li><li>\(mlp=3* d_{model}\times d_{ff}=135.27M\)</li><li>\(norm=2*d_{model}=8.19M\)</li></ul><p>And the count of the parameters is 6607.3M, which is aligned to the number from Transformers.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#007020;font-weight:700>def</span> <span style=color:#06287e>count_llama_params</span>(d_model, d_ff, n_tokens, n_layers):
</span></span><span style=display:flex><span>    embed <span style=color:#666>=</span> n_tokens <span style=color:#666>*</span> d_model
</span></span><span style=display:flex><span>    mha <span style=color:#666>=</span> <span style=color:#40a070>4</span> <span style=color:#666>*</span> d_model<span style=color:#666>**</span><span style=color:#40a070>2</span>
</span></span><span style=display:flex><span>    mlp <span style=color:#666>=</span> <span style=color:#40a070>3</span> <span style=color:#666>*</span> d_moel <span style=color:#666>*</span> d_ff
</span></span><span style=display:flex><span>    norm <span style=color:#666>=</span> <span style=color:#40a070>2</span> <span style=color:#666>*</span> d_model
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>return</span> embed <span style=color:#666>+</span> n_layers <span style=color:#666>*</span> (mha <span style=color:#666>+</span> mlp <span style=color:#666>+</span> norm)
</span></span></code></pre></div><p>For example, the Llama 65B model</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>LlamaModel(
</span></span><span style=display:flex><span>  (embed_tokens): Embedding(32000, 8192, padding_idx=0)
</span></span><span style=display:flex><span>  (layers): ModuleList(
</span></span><span style=display:flex><span>    (0-79): 80 x LlamaDecoderLayer(
</span></span><span style=display:flex><span>      (self_attn): LlamaSdpaAttention(
</span></span><span style=display:flex><span>        (q_proj): Linear(in_features=8192, out_features=8192, bias=False)
</span></span><span style=display:flex><span>        (k_proj): Linear(in_features=8192, out_features=8192, bias=False)
</span></span><span style=display:flex><span>        (v_proj): Linear(in_features=8192, out_features=8192, bias=False)
</span></span><span style=display:flex><span>        (o_proj): Linear(in_features=8192, out_features=8192, bias=False)
</span></span><span style=display:flex><span>        (rotary_emb): LlamaRotaryEmbedding()
</span></span><span style=display:flex><span>      )
</span></span><span style=display:flex><span>      (mlp): LlamaMLP(
</span></span><span style=display:flex><span>        (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)
</span></span><span style=display:flex><span>        (up_proj): Linear(in_features=8192, out_features=22016, bias=False)
</span></span><span style=display:flex><span>        (down_proj): Linear(in_features=22016, out_features=8192, bias=False)
</span></span><span style=display:flex><span>        (act_fn): SiLU()
</span></span><span style=display:flex><span>      )
</span></span><span style=display:flex><span>      (input_layernorm): LlamaRMSNorm()
</span></span><span style=display:flex><span>      (post_attention_layernorm): LlamaRMSNorm()
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>  )
</span></span><span style=display:flex><span>  (norm): LlamaRMSNorm()
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>Total # of params: 65023.52M
</span></span></code></pre></div><p>And let&rsquo;s use the function</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>count_llama_params(d_model<span style=color:#666>=</span><span style=color:#40a070>8192</span>,
</span></span><span style=display:flex><span>    d_ff<span style=color:#666>=</span><span style=color:#40a070>22016</span>,
</span></span><span style=display:flex><span>    n_tokens<span style=color:#666>=</span><span style=color:#40a070>32000</span>,
</span></span><span style=display:flex><span>    n_layers<span style=color:#666>=</span><span style=color:#40a070>80</span>)
</span></span></code></pre></div><p>It gives 65023.5M, is is roughly aligned.</p><h2 id=references>References</h2><ul><li><a href=https://michaelwornow.net/2024/01/18/counting-params-in-transformer>Transformer Math (Part 1) - Counting Model Parameters</a></li><li><a href=https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py>modeling_llama.py from huggingface transformers</a></li></ul><script src=https://utteranc.es/client.js repo=superjomn/superjomn.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><p class="footer text-center">Copyright (c) 2025 Chunwei Yan</p></main></div><script src=/js/feather.min.js></script><script>feather.replace()</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script></body></html>