<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=stylesheet type=text/css href=/css/bootstrap.min.css><link rel=stylesheet type=text/css href=/css/style.css><title>Superjomn's blog | Reduce Kernel in CUDA (2)</title><meta name=google-site-verification content="kO2XszgjIr1OyaOubGpeb0M34ADBz27vyI1dLETarCM"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-Z60BDN3JGH"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Z60BDN3JGH")</script><script src=https://cdn.jsdelivr.net/npm/cash-dom/dist/cash.min.js></script><script>$(function(){$("article table").addClass("table")})</script><body><div id=nav-border class=container><nav id=nav class="nav justify-content-center"><a class=nav-link href=/><i data-feather=home></i>
Home
</a><a class=nav-link href=/tags/tech><i data-feather=terminal></i>
Tech
</a><a class=nav-link href=/tags/life><i data-feather=coffee></i>
Life
</a><a class=nav-link href=/posts/><i data-feather=archive></i>
Archive
</a><a class=nav-link href=/tags/><i data-feather=tag></i>
Tags
</a><a class=nav-link href=/posts/about/><i data-feather=info></i>
About</a></nav></div><div class=container><main id=main><main><article><h1>Reduce Kernel in CUDA (2)</h1><i data-feather=calendar></i> <time datetime=2025-08-12>Aug 12, 2025</time>
<span id=social></span><i data-feather=tag></i>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/cuda>cuda</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/basics>basics</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/tech>tech</a><br><br><p><b>Table of Contents</b></p><aside><nav id=TableOfContents><ol><li><a href=#background>Background</a></li><li><a href=#environment-setting>Environment setting</a><ol><li><a href=#pytorch>PyTorch</a></li><li><a href=#hardware>Hardware</a></li><li><a href=#kernel-launching-utils>Kernel Launching Utils</a></li></ol></li><li><a href=#kernels>Kernels</a><ol><li><a href=#basic-kernel>Basic kernel</a><ol><li><a href=#explain>Explain</a></li></ol></li><li><a href=#floatx4-vector-load>floatx4 vector load</a></li><li><a href=#fp16x2-kernel>fp16x2 kernel</a></li><li><a href=#fp16x8-kernel>fp16x8 kernel</a></li><li><a href=#register-the-kernels-and-benchmark>Register the kernels and benchmark</a></li></ol></li></ol></nav></aside><br><p>In this post, I will walk through the reduce kernels in <a href=https://github.com/xlite-dev/LeetCUDA/tree/main/kernels>LeetCUDA</a> and implement them interactively in this file using org-executor.</p><h2 id=background>Background</h2><p>In CUDA programming, an elementwise kernel is a fundamental building block that applies a given operation independently to each element of an input array (or arrays), producing an output array of the same shape. This is highly parallelizable, as each thread can process a single element without dependencies on others.</p><p>Elementwise kernels are commonly used for operations such as vector addition, scaling, activation functions in neural networks, and more. Understanding how to implement an efficient elementwise kernel is essential before moving on to more complex patterns like reductions.</p><p>In the following sections, we will review how to write a basic elementwise kernel in CUDA, discuss its memory access patterns, and explore best practices for maximizing performance.</p><h2 id=environment-setting>Environment setting</h2><p>Just follow the LeetCUDA&rsquo;s settings, we will expose all the kernels to PyTorch and use its facilities to do performance and precession evaluation.</p><h3 id=pytorch>PyTorch</h3><p>The python version:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#007020;font-weight:700>import</span> <span style=color:#0e84b5;font-weight:700>torch</span>
</span></span><span style=display:flex><span><span style=color:#007020>print</span>(torch<span style=color:#666>.</span>__version__)
</span></span></code></pre></div><p>2.8.0a0+5228986c39.nv25.06</p><h3 id=hardware>Hardware</h3><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>  NVIDIA H100 80GB HBM3, 81559 MiB, 81080 MiB, 0 MiB, 575.57.08, 23, 0 %
</span></span></code></pre></div><h3 id=kernel-launching-utils>Kernel Launching Utils</h3><p>Common C++ header content:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#007020>#pragma once
</span></span></span><span style=display:flex><span><span style=color:#007020>#include</span> <span style=color:#007020>&lt;algorithm&gt;</span><span style=color:#007020>
</span></span></span><span style=display:flex><span><span style=color:#007020>#include</span> <span style=color:#007020>&lt;cuda_bf16.h&gt;</span><span style=color:#007020>
</span></span></span><span style=display:flex><span><span style=color:#007020>#include</span> <span style=color:#007020>&lt;cuda_fp16.h&gt;</span><span style=color:#007020>
</span></span></span><span style=display:flex><span><span style=color:#007020>#include</span> <span style=color:#007020>&lt;cuda_fp8.h&gt;</span><span style=color:#007020>
</span></span></span><span style=display:flex><span><span style=color:#007020>#include</span> <span style=color:#007020>&lt;cuda_runtime.h&gt;</span><span style=color:#007020>
</span></span></span><span style=display:flex><span><span style=color:#007020>#include</span> <span style=color:#007020>&lt;float.h&gt;</span><span style=color:#007020>
</span></span></span><span style=display:flex><span><span style=color:#007020>#include</span> <span style=color:#007020>&lt;stdio.h&gt;</span><span style=color:#007020>
</span></span></span><span style=display:flex><span><span style=color:#007020>#include</span> <span style=color:#007020>&lt;stdlib.h&gt;</span><span style=color:#007020>
</span></span></span><span style=display:flex><span><span style=color:#007020>#include</span> <span style=color:#007020>&lt;torch/extension.h&gt;</span><span style=color:#007020>
</span></span></span><span style=display:flex><span><span style=color:#007020>#include</span> <span style=color:#007020>&lt;torch/types.h&gt;</span><span style=color:#007020>
</span></span></span><span style=display:flex><span><span style=color:#007020>#include</span> <span style=color:#007020>&lt;vector&gt;</span><span style=color:#007020>
</span></span></span></code></pre></div><p>There are some common code for launching kernel with torch facilities.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#007020>#define CEIL(x, y) (((x) + (y) - 1) / (y))
</span></span></span><span style=display:flex><span><span style=color:#007020></span>
</span></span><span style=display:flex><span><span style=color:#007020>#define WARP_SIZE 32
</span></span></span><span style=display:flex><span><span style=color:#007020>#define INT4(value) (reinterpret_cast&lt;int4*&gt;(&amp;(value))[0])
</span></span></span><span style=display:flex><span><span style=color:#007020>#define FLOAT4(value) (reinterpret_cast&lt;float4*&gt;(&amp;(value))[0])
</span></span></span><span style=display:flex><span><span style=color:#007020>#define FLOAT4(value) (reinterpret_cast&lt;float4*&gt;(&amp;(value))[0])
</span></span></span><span style=display:flex><span><span style=color:#007020>#define HALF2(value) (reinterpret_cast&lt;half2*&gt;(&amp;(value))[0])
</span></span></span><span style=display:flex><span><span style=color:#007020>#define BFLOAT2(value) (reinterpret_cast&lt;__nv_bfloat162*&gt;(&amp;(value))[0])
</span></span></span><span style=display:flex><span><span style=color:#007020>#define LDST128BITS(value) (reinterpret_cast&lt;float4*&gt;(&amp;(value))[0])
</span></span></span><span style=display:flex><span><span style=color:#007020></span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>inline</span> <span style=color:#902000>void</span> <span style=color:#06287e>check_torch_dtype</span>(torch<span style=color:#666>::</span>Tensor tensor,
</span></span><span style=display:flex><span>                              torch<span style=color:#666>::</span>ScalarType expected_dtype) {
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>if</span> (tensor.dtype() <span style=color:#666>!=</span> expected_dtype) {
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>throw</span> std<span style=color:#666>::</span>runtime_error(<span style=color:#4070a0>&#34;Tensor dtype mismatch&#34;</span>);
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>inline</span> std<span style=color:#666>::</span>tuple<span style=color:#666>&lt;</span>dim3, dim3<span style=color:#666>&gt;</span>
</span></span><span style=display:flex><span>get_launch_dimensions(<span style=color:#902000>int</span> N, <span style=color:#902000>int</span> elements_per_block <span style=color:#666>=</span> <span style=color:#40a070>256</span>,
</span></span><span style=display:flex><span>                      <span style=color:#902000>int</span> element_per_thread <span style=color:#666>=</span> <span style=color:#40a070>1</span>) {
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>const</span> <span style=color:#902000>int</span> threads_per_block <span style=color:#666>=</span> elements_per_block <span style=color:#666>/</span> element_per_thread;
</span></span><span style=display:flex><span>  dim3 <span style=color:#06287e>block_size</span>(threads_per_block);
</span></span><span style=display:flex><span>  dim3 <span style=color:#06287e>grid_size</span>(CEIL(N, elements_per_block));
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>return</span> {grid_size, block_size};
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007020>#define TORCH_BINDING_COMMON_EXTENSION(func) m.def(#func, &amp;func, #func);
</span></span></span><span style=display:flex><span><span style=color:#007020></span>
</span></span><span style=display:flex><span><span style=color:#007020>#define TORCH_BINDING_ELEM_ADD(packed_type, th_type, element_type,             \
</span></span></span><span style=display:flex><span><span style=color:#007020>                               elements_per_thread)                            \
</span></span></span><span style=display:flex><span><span style=color:#007020>  __global__ void elementwise_add_##packed_type##_kernel(                     \
</span></span></span><span style=display:flex><span><span style=color:#007020>      element_type* __restrict__ a,                                            \
</span></span></span><span style=display:flex><span><span style=color:#007020>      element_type* __restrict__ b,                                            \
</span></span></span><span style=display:flex><span><span style=color:#007020>      element_type* __restrict__ c, int N);                                    \
</span></span></span><span style=display:flex><span><span style=color:#007020>  \
</span></span></span><span style=display:flex><span><span style=color:#007020>  void elementwise_add_##packed_type(torch::Tensor A, torch::Tensor B,         \
</span></span></span><span style=display:flex><span><span style=color:#007020>                                     torch::Tensor C,                          \
</span></span></span><span style=display:flex><span><span style=color:#007020>                                     int elements_per_block) {                 \
</span></span></span><span style=display:flex><span><span style=color:#007020>    check_torch_dtype(A, th_type);                                             \
</span></span></span><span style=display:flex><span><span style=color:#007020>    check_torch_dtype(B, th_type);                                             \
</span></span></span><span style=display:flex><span><span style=color:#007020>    check_torch_dtype(C, th_type);                                             \
</span></span></span><span style=display:flex><span><span style=color:#007020>    auto [grid, block] = get_launch_dimensions(A.numel(), elements_per_block,  \
</span></span></span><span style=display:flex><span><span style=color:#007020>                                               elements_per_thread);           \
</span></span></span><span style=display:flex><span><span style=color:#007020>    elementwise_add_##packed_type##_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(                   \
</span></span></span><span style=display:flex><span><span style=color:#007020>        reinterpret_cast&lt;element_type*&gt;(A.data_ptr()),                         \
</span></span></span><span style=display:flex><span><span style=color:#007020>        reinterpret_cast&lt;element_type*&gt;(B.data_ptr()),                         \
</span></span></span><span style=display:flex><span><span style=color:#007020>        reinterpret_cast&lt;element_type*&gt;(C.data_ptr()), A.numel());             \
</span></span></span><span style=display:flex><span><span style=color:#007020>  }
</span></span></span></code></pre></div><h2 id=kernels>Kernels</h2><h3 id=basic-kernel>Basic kernel</h3><p>This kernel demonstrates a basic elementwise addition operation in CUDA, where each thread adds two corresponding elements from the input arrays:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#007020>#include</span> <span style=color:#007020>&#34;elementwise_add.cuh&#34;</span><span style=color:#007020>
</span></span></span><span style=display:flex><span><span style=color:#007020></span>
</span></span><span style=display:flex><span>__global__ <span style=color:#902000>void</span> <span style=color:#06287e>elementwise_add_f32_kernel</span>(<span style=color:#902000>float</span><span style=color:#666>*</span> __restrict__ a,
</span></span><span style=display:flex><span>                                           <span style=color:#902000>float</span><span style=color:#666>*</span> __restrict__ b,
</span></span><span style=display:flex><span>                                           <span style=color:#902000>float</span><span style=color:#666>*</span> __restrict__ c, <span style=color:#902000>int</span> N) {
</span></span><span style=display:flex><span>  <span style=color:#902000>int</span> tid <span style=color:#666>=</span> blockIdx.x <span style=color:#666>*</span> blockDim.x <span style=color:#666>+</span> threadIdx.x;
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>if</span> (tid <span style=color:#666>&lt;</span> N) {
</span></span><span style=display:flex><span>    c[tid] <span style=color:#666>=</span> a[tid] <span style=color:#666>+</span> b[tid];
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h4 id=explain>Explain</h4><p>Each thread loads one float (4 bytes) independently, this should result in poor memory coalescing.</p><h3 id=floatx4-vector-load>floatx4 vector load</h3><p>This kernel introduces vectorized load and store operations using `float4`, which allows each thread to process four floats at once. By loading 16 bytes (128 bits) per memory transaction instead of 4 bytes, this approach significantly improves memory bandwidth utilization and coalescing efficiency. Each thread processes 4 elements simultaneously, reducing the total number of memory transactions by 4x compared to the basic kernel:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#007020>#include</span> <span style=color:#007020>&#34;elementwise_add.cuh&#34;</span><span style=color:#007020>
</span></span></span><span style=display:flex><span><span style=color:#007020></span>
</span></span><span style=display:flex><span>__global__ <span style=color:#902000>void</span> <span style=color:#06287e>elementwise_add_f32x4_kernel</span>(<span style=color:#902000>float</span><span style=color:#666>*</span> __restrict__ a,
</span></span><span style=display:flex><span>                                             <span style=color:#902000>float</span><span style=color:#666>*</span> __restrict__ b,
</span></span><span style=display:flex><span>                                             <span style=color:#902000>float</span><span style=color:#666>*</span> __restrict__ c, <span style=color:#902000>int</span> N) {
</span></span><span style=display:flex><span>  <span style=color:#902000>int</span> idx <span style=color:#666>=</span> <span style=color:#40a070>4</span> <span style=color:#666>*</span> (blockIdx.x <span style=color:#666>*</span> blockDim.x <span style=color:#666>+</span> threadIdx.x);
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>if</span> (idx <span style=color:#666>+</span> <span style=color:#40a070>3</span> <span style=color:#666>&lt;</span> N) {
</span></span><span style=display:flex><span>    float4 reg_a <span style=color:#666>=</span> FLOAT4(a[idx]);
</span></span><span style=display:flex><span>    float4 reg_b <span style=color:#666>=</span> FLOAT4(b[idx]);
</span></span><span style=display:flex><span>    float4 reg_c;
</span></span><span style=display:flex><span>    reg_c.x <span style=color:#666>=</span> reg_a.x <span style=color:#666>+</span> reg_b.x;
</span></span><span style=display:flex><span>    reg_c.y <span style=color:#666>=</span> reg_a.y <span style=color:#666>+</span> reg_b.y;
</span></span><span style=display:flex><span>    reg_c.z <span style=color:#666>=</span> reg_a.z <span style=color:#666>+</span> reg_b.z;
</span></span><span style=display:flex><span>    reg_c.w <span style=color:#666>=</span> reg_a.w <span style=color:#666>+</span> reg_b.w;
</span></span><span style=display:flex><span>    FLOAT4(c[idx]) <span style=color:#666>=</span> reg_c;
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h3 id=fp16x2-kernel>fp16x2 kernel</h3><p>This kernel leverages half-precision (fp16) data types with vectorized operations using `half2`. Each thread processes 2 half-precision values simultaneously using CUDA&rsquo;s native half2 intrinsics. This provides both memory bandwidth improvements (loading 4 bytes per transaction) and computational efficiency through packed arithmetic operations:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#007020>#include</span> <span style=color:#007020>&#34;elementwise_add.cuh&#34;</span><span style=color:#007020>
</span></span></span><span style=display:flex><span><span style=color:#007020></span>
</span></span><span style=display:flex><span>__global__ <span style=color:#902000>void</span> <span style=color:#06287e>elementwise_add_f16x2_kernel</span>(half<span style=color:#666>*</span> __restrict__ a, half<span style=color:#666>*</span> __restrict__ b, half<span style=color:#666>*</span> __restrict__ c, <span style=color:#902000>int</span> N) {
</span></span><span style=display:flex><span>  <span style=color:#902000>int</span> idx <span style=color:#666>=</span> <span style=color:#40a070>2</span> <span style=color:#666>*</span> (blockIdx.x <span style=color:#666>*</span> blockDim.x <span style=color:#666>+</span> threadIdx.x);
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>if</span> (idx <span style=color:#666>+</span> <span style=color:#40a070>1</span> <span style=color:#666>&lt;</span> N) {
</span></span><span style=display:flex><span>    half2 reg_a <span style=color:#666>=</span> HALF2(a[idx]);
</span></span><span style=display:flex><span>    half2 reg_b <span style=color:#666>=</span> HALF2(b[idx]);
</span></span><span style=display:flex><span>    half2 reg_c <span style=color:#666>=</span> __hadd2(reg_a, reg_b);
</span></span><span style=display:flex><span>    HALF2(c[idx]) <span style=color:#666>=</span> reg_c;
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h3 id=fp16x8-kernel>fp16x8 kernel</h3><p>This kernel extends the vectorization approach to process 8 half-precision values per thread, using four `half2` packed operations.
This maximizes memory throughput by loading 16 bytes (128 bits) per thread while maintaining efficient packed arithmetic.
The kernel includes proper bounds checking for each half2 pair to handle cases where the array size is not perfectly divisible by 8:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#007020>#include</span> <span style=color:#007020>&#34;elementwise_add.cuh&#34;</span><span style=color:#007020>
</span></span></span><span style=display:flex><span><span style=color:#007020></span>
</span></span><span style=display:flex><span>__global__ <span style=color:#902000>void</span> <span style=color:#06287e>elementwise_add_f16x8_kernel</span>(half<span style=color:#666>*</span> __restrict__ a,
</span></span><span style=display:flex><span>                                             half<span style=color:#666>*</span> __restrict__ b,
</span></span><span style=display:flex><span>                                             half<span style=color:#666>*</span> __restrict__ c, <span style=color:#902000>int</span> N) {
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>const</span> <span style=color:#902000>int</span> linearThreadId <span style=color:#666>=</span> blockIdx.x <span style=color:#666>*</span> blockDim.x <span style=color:#666>+</span> threadIdx.x;
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>const</span> <span style=color:#902000>int</span> idx <span style=color:#666>=</span> linearThreadId <span style=color:#666>*</span> <span style=color:#40a070>8</span>;
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>const</span> <span style=color:#902000>int</span> remaining <span style=color:#666>=</span> N <span style=color:#666>-</span> idx;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>if</span> (remaining <span style=color:#666>&lt;=</span> <span style=color:#40a070>0</span>) {
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>return</span>;
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#60a0b0;font-style:italic>// Fast path: full 8 elements
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>  <span style=color:#007020;font-weight:700>if</span> (remaining <span style=color:#666>&gt;=</span> <span style=color:#40a070>8</span>) {
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic>// Single 128-bit loads for A and B
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    float4 vec_a <span style=color:#666>=</span> LDST128BITS(a[idx]);
</span></span><span style=display:flex><span>    float4 vec_b <span style=color:#666>=</span> LDST128BITS(b[idx]);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic>// Reinterpret as four half2 lanes, compute, then store as 128-bit
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    <span style=color:#007020;font-weight:700>union</span> <span style=color:#0e84b5;font-weight:700>Pack16</span> {
</span></span><span style=display:flex><span>      float4 f4;
</span></span><span style=display:flex><span>      half2  h2[<span style=color:#40a070>4</span>];
</span></span><span style=display:flex><span>    } pa, pb, pc;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    pa.f4 <span style=color:#666>=</span> vec_a;
</span></span><span style=display:flex><span>    pb.f4 <span style=color:#666>=</span> vec_b;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    pc.h2[<span style=color:#40a070>0</span>] <span style=color:#666>=</span> __hadd2(pa.h2[<span style=color:#40a070>0</span>], pb.h2[<span style=color:#40a070>0</span>]);
</span></span><span style=display:flex><span>    pc.h2[<span style=color:#40a070>1</span>] <span style=color:#666>=</span> __hadd2(pa.h2[<span style=color:#40a070>1</span>], pb.h2[<span style=color:#40a070>1</span>]);
</span></span><span style=display:flex><span>    pc.h2[<span style=color:#40a070>2</span>] <span style=color:#666>=</span> __hadd2(pa.h2[<span style=color:#40a070>2</span>], pb.h2[<span style=color:#40a070>2</span>]);
</span></span><span style=display:flex><span>    pc.h2[<span style=color:#40a070>3</span>] <span style=color:#666>=</span> __hadd2(pa.h2[<span style=color:#40a070>3</span>], pb.h2[<span style=color:#40a070>3</span>]);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic>// Single 128-bit store for C
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>    LDST128BITS(c[idx]) <span style=color:#666>=</span> pc.f4;
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>return</span>;
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#60a0b0;font-style:italic>// Tail path: handle &lt;8 remaining elements
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>  <span style=color:#902000>int</span> i <span style=color:#666>=</span> <span style=color:#40a070>0</span>;
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>for</span> (; i <span style=color:#666>+</span> <span style=color:#40a070>1</span> <span style=color:#666>&lt;</span> remaining; i <span style=color:#666>+=</span> <span style=color:#40a070>2</span>) {
</span></span><span style=display:flex><span>    half2 ra <span style=color:#666>=</span> HALF2(a[idx <span style=color:#666>+</span> i]);
</span></span><span style=display:flex><span>    half2 rb <span style=color:#666>=</span> HALF2(b[idx <span style=color:#666>+</span> i]);
</span></span><span style=display:flex><span>    HALF2(c[idx <span style=color:#666>+</span> i]) <span style=color:#666>=</span> __hadd2(ra, rb);
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>if</span> (i <span style=color:#666>&lt;</span> remaining) {
</span></span><span style=display:flex><span>    c[idx <span style=color:#666>+</span> i] <span style=color:#666>=</span> __hadd(a[idx <span style=color:#666>+</span> i], b[idx <span style=color:#666>+</span> i]);
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h3 id=register-the-kernels-and-benchmark>Register the kernels and benchmark</h3><p>Register the kernel:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#007020>#include</span> <span style=color:#007020>&#34;elementwise_add.cuh&#34;</span><span style=color:#007020>
</span></span></span><span style=display:flex><span><span style=color:#007020></span>
</span></span><span style=display:flex><span>TORCH_BINDING_ELEM_ADD(f32, torch<span style=color:#666>::</span>kFloat32, <span style=color:#902000>float</span>, <span style=color:#40a070>1</span>)
</span></span><span style=display:flex><span>TORCH_BINDING_ELEM_ADD(f32x4, torch<span style=color:#666>::</span>kFloat32, <span style=color:#902000>float</span>, <span style=color:#40a070>4</span>)
</span></span><span style=display:flex><span>TORCH_BINDING_ELEM_ADD(f16x2, torch<span style=color:#666>::</span>kFloat16, half, <span style=color:#40a070>2</span>)
</span></span><span style=display:flex><span>TORCH_BINDING_ELEM_ADD(f16x8, torch<span style=color:#666>::</span>kFloat16, half, <span style=color:#40a070>8</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
</span></span><span style=display:flex><span>  TORCH_BINDING_COMMON_EXTENSION(elementwise_add_f32)
</span></span><span style=display:flex><span>  TORCH_BINDING_COMMON_EXTENSION(elementwise_add_f32x4)
</span></span><span style=display:flex><span>  TORCH_BINDING_COMMON_EXTENSION(elementwise_add_f16x2)
</span></span><span style=display:flex><span>  TORCH_BINDING_COMMON_EXTENSION(elementwise_add_f16x8)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Compile PyTorch module</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#007020;font-weight:700>from</span> <span style=color:#0e84b5;font-weight:700>setuptools</span> <span style=color:#007020;font-weight:700>import</span> setup
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>from</span> <span style=color:#0e84b5;font-weight:700>torch.utils.cpp_extension</span> <span style=color:#007020;font-weight:700>import</span> BuildExtension, CppExtension
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>source_files <span style=color:#666>=</span> [
</span></span><span style=display:flex><span>  <span style=color:#4070a0>&#34;elementwise_add_basic.cu&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#4070a0>&#34;elementwise_add_f32x4.cu&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#4070a0>&#34;elementwise_add_f16x2.cu&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#4070a0>&#34;elementwise_add_f16x8.cu&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#4070a0>&#34;elementwise_add_lib.cu&#34;</span>,
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>setup(
</span></span><span style=display:flex><span>    name<span style=color:#666>=</span><span style=color:#4070a0>&#39;elementwise_lib&#39;</span>,  <span style=color:#60a0b0;font-style:italic># The name of your module</span>
</span></span><span style=display:flex><span>      ext_modules<span style=color:#666>=</span>[
</span></span><span style=display:flex><span>          CppExtension(
</span></span><span style=display:flex><span>              <span style=color:#4070a0>&#39;elementwise_lib&#39;</span>,
</span></span><span style=display:flex><span>              source_files
</span></span><span style=display:flex><span>          ),
</span></span><span style=display:flex><span>      ],
</span></span><span style=display:flex><span>    cmdclass<span style=color:#666>=</span>{
</span></span><span style=display:flex><span>        <span style=color:#4070a0>&#39;build_ext&#39;</span>: BuildExtension
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Launching in PyTorch:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#007020;font-weight:700>import</span> <span style=color:#0e84b5;font-weight:700>time</span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>from</span> <span style=color:#0e84b5;font-weight:700>functools</span> <span style=color:#007020;font-weight:700>import</span> partial
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>from</span> <span style=color:#0e84b5;font-weight:700>typing</span> <span style=color:#007020;font-weight:700>import</span> Optional
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>import</span> <span style=color:#0e84b5;font-weight:700>torch</span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>import</span> <span style=color:#0e84b5;font-weight:700>os</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>import</span> <span style=color:#0e84b5;font-weight:700>sys</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>workspace <span style=color:#666>=</span> os<span style=color:#666>.</span>environ[<span style=color:#4070a0>&#34;__WORKSPACE__&#34;</span>]
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># The built torch lib is in the following path</span>
</span></span><span style=display:flex><span>lib_dir <span style=color:#666>=</span> <span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;</span><span style=color:#70a0d0>{</span>workspace<span style=color:#70a0d0>}</span><span style=color:#4070a0>/build/lib.linux-x86_64-cpython-312&#34;</span>
</span></span><span style=display:flex><span><span style=color:#007020>print</span>(<span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;lib: </span><span style=color:#70a0d0>{</span>lib_dir<span style=color:#70a0d0>}</span><span style=color:#4070a0>&#34;</span>)
</span></span><span style=display:flex><span>sys<span style=color:#666>.</span>path<span style=color:#666>.</span>append(lib_dir)
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>import</span> <span style=color:#0e84b5;font-weight:700>elementwise_lib</span> <span style=color:#007020;font-weight:700>as</span> <span style=color:#0e84b5;font-weight:700>lib</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>torch<span style=color:#666>.</span>set_grad_enabled(<span style=color:#007020;font-weight:700>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007020>print</span>(<span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;Compiling Torch kernel&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic># Load the CUDA kernel as a python module</span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>import</span> <span style=color:#0e84b5;font-weight:700>hashlib</span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>import</span> <span style=color:#0e84b5;font-weight:700>os</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>def</span> <span style=color:#06287e>get_file_hash</span>(filepath):
</span></span><span style=display:flex><span>  <span style=color:#4070a0>&#34;&#34;&#34;Get MD5 hash of file content&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>with</span> <span style=color:#007020>open</span>(filepath, <span style=color:#4070a0>&#39;rb&#39;</span>) <span style=color:#007020;font-weight:700>as</span> f:
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>return</span> hashlib<span style=color:#666>.</span>md5(f<span style=color:#666>.</span>read())<span style=color:#666>.</span>hexdigest()[:<span style=color:#40a070>8</span>]  <span style=color:#60a0b0;font-style:italic># Use first 8 chars</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007020>print</span>(<span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;running benchmark&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>def</span> <span style=color:#06287e>run_benchmark</span>(
</span></span><span style=display:flex><span>    perf_func: callable,
</span></span><span style=display:flex><span>    a: torch<span style=color:#666>.</span>Tensor,
</span></span><span style=display:flex><span>    b: torch<span style=color:#666>.</span>Tensor,
</span></span><span style=display:flex><span>    tag: <span style=color:#007020>str</span>,
</span></span><span style=display:flex><span>    out: Optional[torch<span style=color:#666>.</span>Tensor] <span style=color:#666>=</span> <span style=color:#007020;font-weight:700>None</span>,
</span></span><span style=display:flex><span>    warmup: <span style=color:#007020>int</span> <span style=color:#666>=</span> <span style=color:#40a070>10</span>,
</span></span><span style=display:flex><span>    iters: <span style=color:#007020>int</span> <span style=color:#666>=</span> <span style=color:#40a070>1000</span>,
</span></span><span style=display:flex><span>    show_all: <span style=color:#007020>bool</span> <span style=color:#666>=</span> <span style=color:#007020;font-weight:700>False</span>,
</span></span><span style=display:flex><span>    elements_per_block <span style=color:#666>=</span> <span style=color:#40a070>256</span>,
</span></span><span style=display:flex><span>):
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>if</span> out <span style=color:#007020;font-weight:700>is</span> <span style=color:#007020;font-weight:700>not</span> <span style=color:#007020;font-weight:700>None</span>:
</span></span><span style=display:flex><span>        out<span style=color:#666>.</span>fill_(<span style=color:#40a070>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic># Warmup</span>
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> _ <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(warmup):
</span></span><span style=display:flex><span>        perf_func(a, b, out, elements_per_block)
</span></span><span style=display:flex><span>    torch<span style=color:#666>.</span>cuda<span style=color:#666>.</span>synchronize()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic># Benchmark</span>
</span></span><span style=display:flex><span>    start_event <span style=color:#666>=</span> torch<span style=color:#666>.</span>cuda<span style=color:#666>.</span>Event(enable_timing<span style=color:#666>=</span><span style=color:#007020;font-weight:700>True</span>)
</span></span><span style=display:flex><span>    end_event <span style=color:#666>=</span> torch<span style=color:#666>.</span>cuda<span style=color:#666>.</span>Event(enable_timing<span style=color:#666>=</span><span style=color:#007020;font-weight:700>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    start_event<span style=color:#666>.</span>record()
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> _ <span style=color:#007020;font-weight:700>in</span> <span style=color:#007020>range</span>(iters):
</span></span><span style=display:flex><span>        perf_func(a, b, out, elements_per_block)
</span></span><span style=display:flex><span>    end_event<span style=color:#666>.</span>record()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    torch<span style=color:#666>.</span>cuda<span style=color:#666>.</span>synchronize()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    total_time <span style=color:#666>=</span> start_event<span style=color:#666>.</span>elapsed_time(end_event)  <span style=color:#60a0b0;font-style:italic># ms</span>
</span></span><span style=display:flex><span>    mean_time <span style=color:#666>=</span> total_time <span style=color:#666>/</span> iters
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    out_info <span style=color:#666>=</span> <span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;out_</span><span style=color:#70a0d0>{</span>tag<span style=color:#70a0d0>}</span><span style=color:#4070a0>&#34;</span>
</span></span><span style=display:flex><span>    out_val <span style=color:#666>=</span> out<span style=color:#666>.</span>flatten()<span style=color:#666>.</span>detach()<span style=color:#666>.</span>cpu()<span style=color:#666>.</span>numpy()<span style=color:#666>.</span>tolist()[:<span style=color:#40a070>2</span>]
</span></span><span style=display:flex><span>    out_val <span style=color:#666>=</span> [<span style=color:#007020>round</span>(v, <span style=color:#40a070>8</span>) <span style=color:#007020;font-weight:700>for</span> v <span style=color:#007020;font-weight:700>in</span> out_val]
</span></span><span style=display:flex><span>    <span style=color:#007020>print</span>(<span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;</span><span style=color:#70a0d0>{</span>out_info<span style=color:#70a0d0>:</span><span style=color:#4070a0>&gt;18</span><span style=color:#70a0d0>}</span><span style=color:#4070a0>: </span><span style=color:#70a0d0>{</span>out_val<span style=color:#70a0d0>}</span><span style=color:#4070a0>, time:</span><span style=color:#70a0d0>{</span>mean_time<span style=color:#70a0d0>:</span><span style=color:#4070a0>.8f</span><span style=color:#70a0d0>}</span><span style=color:#4070a0>ms&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>if</span> show_all:
</span></span><span style=display:flex><span>        <span style=color:#007020>print</span>(out)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>return</span> out, mean_time
</span></span></code></pre></div><p>Run the benchmark:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>shapes <span style=color:#666>=</span> [
</span></span><span style=display:flex><span>  (<span style=color:#40a070>2096</span>, <span style=color:#40a070>4096</span>), (<span style=color:#40a070>2048</span>, <span style=color:#40a070>2048</span>), (<span style=color:#40a070>2048</span>, <span style=color:#40a070>1024</span>), (<span style=color:#40a070>1024</span>, <span style=color:#40a070>1024</span>), (<span style=color:#40a070>512</span>, <span style=color:#40a070>512</span>), (<span style=color:#40a070>256</span>, <span style=color:#40a070>256</span>)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#007020;font-weight:700>for</span> shape <span style=color:#007020;font-weight:700>in</span> shapes:
</span></span><span style=display:flex><span>    <span style=color:#007020>print</span>(<span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;Running benchmark for shape: </span><span style=color:#70a0d0>{</span>shape<span style=color:#70a0d0>}</span><span style=color:#4070a0>&#34;</span>)
</span></span><span style=display:flex><span>    A <span style=color:#666>=</span> torch<span style=color:#666>.</span>randn(<span style=color:#666>*</span>shape, dtype<span style=color:#666>=</span>torch<span style=color:#666>.</span>float32, device<span style=color:#666>=</span><span style=color:#4070a0>&#34;cuda&#34;</span>)<span style=color:#666>.</span>contiguous()
</span></span><span style=display:flex><span>    B <span style=color:#666>=</span> torch<span style=color:#666>.</span>randn(<span style=color:#666>*</span>shape, <span style=color:#40a070>1024</span>, dtype<span style=color:#666>=</span>torch<span style=color:#666>.</span>float32, device<span style=color:#666>=</span><span style=color:#4070a0>&#34;cuda&#34;</span>)<span style=color:#666>.</span>contiguous()
</span></span><span style=display:flex><span>    C <span style=color:#666>=</span> torch<span style=color:#666>.</span>zeros_like(A)<span style=color:#666>.</span>contiguous()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic># Create fp16 tensors for fp16 kernels</span>
</span></span><span style=display:flex><span>    A_fp16 <span style=color:#666>=</span> A<span style=color:#666>.</span>half()<span style=color:#666>.</span>contiguous()
</span></span><span style=display:flex><span>    B_fp16 <span style=color:#666>=</span> B<span style=color:#666>.</span>half()<span style=color:#666>.</span>contiguous()
</span></span><span style=display:flex><span>    C_fp16 <span style=color:#666>=</span> torch<span style=color:#666>.</span>zeros_like(A_fp16)<span style=color:#666>.</span>contiguous()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    elements_per_block <span style=color:#666>=</span> <span style=color:#40a070>256</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#007020>print</span>(<span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;elements_per_block: </span><span style=color:#70a0d0>{</span>elements_per_block<span style=color:#70a0d0>}</span><span style=color:#4070a0>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#60a0b0;font-style:italic># Increase elements_per_block to make sure that each kernel has same threads_per_block</span>
</span></span><span style=display:flex><span>    run_benchmark(lib<span style=color:#666>.</span>elementwise_add_f32, A, B, <span style=color:#4070a0>&#34;basic&#34;</span>, C, elements_per_block)
</span></span><span style=display:flex><span>    run_benchmark(lib<span style=color:#666>.</span>elementwise_add_f32x4, A, B, <span style=color:#4070a0>&#34;f32x4&#34;</span>, C, elements_per_block <span style=color:#666>*</span> <span style=color:#40a070>4</span>)
</span></span><span style=display:flex><span>    run_benchmark(lib<span style=color:#666>.</span>elementwise_add_f16x2, A_fp16, B_fp16, <span style=color:#4070a0>&#34;f16x2&#34;</span>, C_fp16, elements_per_block <span style=color:#666>*</span> <span style=color:#40a070>2</span>)
</span></span><span style=display:flex><span>    run_benchmark(lib<span style=color:#666>.</span>elementwise_add_f16x8, A_fp16, B_fp16, <span style=color:#4070a0>&#34;f16x8&#34;</span>, C_fp16, elements_per_block <span style=color:#666>*</span> <span style=color:#40a070>8</span>)
</span></span><span style=display:flex><span>    <span style=color:#007020>print</span>(<span style=color:#4070a0>f</span><span style=color:#4070a0>&#34;--&#34;</span>)
</span></span></code></pre></div><p>Results:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>  lib: /workspace/project/superjomn.github.io/content-org/_build/build/lib.linux-x86_64-cpython-312
</span></span><span style=display:flex><span>  Compiling Torch kernel
</span></span><span style=display:flex><span>  running benchmark
</span></span><span style=display:flex><span>  Running benchmark for shape: (2096, 4096)
</span></span><span style=display:flex><span>  elements_per_block: 256
</span></span><span style=display:flex><span>           out_basic: [0.62899578, -3.16506243], time:0.04013546ms
</span></span><span style=display:flex><span>           out_f32x4: [0.62899578, -3.16506243], time:0.03716669ms
</span></span><span style=display:flex><span>           out_f16x2: [0.62890625, -3.1640625], time:0.02376186ms
</span></span><span style=display:flex><span>           out_f16x8: [0.62890625, -3.1640625], time:0.02382634ms
</span></span><span style=display:flex><span>  --
</span></span><span style=display:flex><span>  Running benchmark for shape: (2048, 2048)
</span></span><span style=display:flex><span>  elements_per_block: 256
</span></span><span style=display:flex><span>           out_basic: [0.98603237, -2.21596098], time:0.02142691ms
</span></span><span style=display:flex><span>           out_f32x4: [0.98603237, -2.21596098], time:0.01876467ms
</span></span><span style=display:flex><span>           out_f16x2: [0.98632812, -2.21484375], time:0.01224410ms
</span></span><span style=display:flex><span>           out_f16x8: [0.98632812, -2.21484375], time:0.01231584ms
</span></span><span style=display:flex><span>  --
</span></span><span style=display:flex><span>  Running benchmark for shape: (2048, 1024)
</span></span><span style=display:flex><span>  elements_per_block: 256
</span></span><span style=display:flex><span>           out_basic: [-1.68364513, 0.07630849], time:0.00754973ms
</span></span><span style=display:flex><span>           out_f32x4: [-1.68364513, 0.07630849], time:0.00768909ms
</span></span><span style=display:flex><span>           out_f16x2: [-1.68359375, 0.07666016], time:0.00720714ms
</span></span><span style=display:flex><span>           out_f16x8: [-1.68359375, 0.07666016], time:0.00725242ms
</span></span><span style=display:flex><span>  --
</span></span><span style=display:flex><span>  Running benchmark for shape: (1024, 1024)
</span></span><span style=display:flex><span>  elements_per_block: 256
</span></span><span style=display:flex><span>           out_basic: [0.41730967, -2.56410193], time:0.00473962ms
</span></span><span style=display:flex><span>           out_f32x4: [0.41730967, -2.56410193], time:0.00490102ms
</span></span><span style=display:flex><span>           out_f16x2: [0.41723633, -2.56445312], time:0.00471936ms
</span></span><span style=display:flex><span>           out_f16x8: [0.41723633, -2.56445312], time:0.00484266ms
</span></span><span style=display:flex><span>  --
</span></span><span style=display:flex><span>  Running benchmark for shape: (512, 512)
</span></span><span style=display:flex><span>  elements_per_block: 256
</span></span><span style=display:flex><span>           out_basic: [-0.84098238, -0.51086581], time:0.00448192ms
</span></span><span style=display:flex><span>           out_f32x4: [-0.84098238, -0.51086581], time:0.00438221ms
</span></span><span style=display:flex><span>           out_f16x2: [-0.84130859, -0.51074219], time:0.00443571ms
</span></span><span style=display:flex><span>           out_f16x8: [-0.84130859, -0.51074219], time:0.00444208ms
</span></span><span style=display:flex><span>  --
</span></span><span style=display:flex><span>  Running benchmark for shape: (256, 256)
</span></span><span style=display:flex><span>  elements_per_block: 256
</span></span><span style=display:flex><span>           out_basic: [2.76621795, 1.71955645], time:0.00438445ms
</span></span><span style=display:flex><span>           out_f32x4: [2.76621795, 1.71955645], time:0.00440973ms
</span></span><span style=display:flex><span>           out_f16x2: [2.765625, 1.71972656], time:0.00444272ms
</span></span><span style=display:flex><span>           out_f16x8: [2.765625, 1.71972656], time:0.00445354ms
</span></span><span style=display:flex><span>  --
</span></span></code></pre></div><script src=https://utteranc.es/client.js repo=superjomn/superjomn.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><p class="footer text-center">Copyright (c) 2025 Chunwei Yan</p></main></div><script src=/js/feather.min.js></script><script>feather.replace()</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script></body></html>