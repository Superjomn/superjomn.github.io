<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=stylesheet type=text/css href=/css/bootstrap.min.css><link rel=stylesheet type=text/css href=/css/style.css><title>Superjomn's blog | Tech</title>
<meta name=google-site-verification content="kO2XszgjIr1OyaOubGpeb0M34ADBz27vyI1dLETarCM"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-Z60BDN3JGH"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Z60BDN3JGH")</script><script src=https://cdn.jsdelivr.net/npm/cash-dom/dist/cash.min.js></script><script>$(function(){$("article table").addClass("table")})</script><body><div id=nav-border class=container><nav id=nav class="nav justify-content-center"><a class=nav-link href=/><i data-feather=home></i>
Home
</a><a class=nav-link href=/tags/tech><i data-feather=terminal></i>
Tech
</a><a class=nav-link href=/tags/life><i data-feather=coffee></i>
Life
</a><a class=nav-link href=/posts/><i data-feather=archive></i>
Archive
</a><a class=nav-link href=/tags/><i data-feather=tag></i>
Tags
</a><a class=nav-link href=/posts/about/><i data-feather=info></i>
About</a></nav></div><div class=container><main id=main><h1>Tech</h1><p><h3><a class=title href=/posts/llm_notes/>Notes on LLM technologies (keep updating)</a></h3><i data-feather=calendar></i> <time datetime=2024-03-10>Mar 10, 2024</time>
<span id=social></span><i data-feather=tag></i>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/llm>LLM</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/tech>tech</a><p>Brief notes on LLM technologies.
Models GPT2 Model structure The GPT model employs a repeated structure of Transformer Blocks, each containing two sub-layers: a Masked Multi-Head Attention (MMHA) layer and a Position-wise Feed-Forward Network.
The MMHA is a central component of the model. It operates by splitting the input into multiple &lsquo;heads&rsquo;, each of which learns to attend to different positions within the input sequence, allowing the model to focus on different aspects of the input simultaneously.</p></p><p><h3><a class=title href=/posts/cuda-memory-coalescing-access-matrix-transpose/>Memory coalescing in CUDA (2) – Matrix Transpose</a></h3><i data-feather=calendar></i> <time datetime=2024-03-05>Mar 5, 2024</time>
<span id=social></span><i data-feather=tag></i>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/cuda>cuda</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/basics>basics</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/tech>tech</a><p>Background In the VecAdd page, we&rsquo;ve introduced the memory coalescing in global memory access. This post will follow the topic with another interesting application: Matrix transposing.
The following content will briefly touch on the following topics:
Tiles in matrix, this is the basis of optimization matrix computation A simple trick to avoid bank conflict in shared memory access Kernels The code for all the kernels locates in 1-matrix-transpose-coalesce.cu.
Read coalesced template &lt;typename T> __global__ void transpose_read_coalesce( const T* __restrict__ input, T* __restrict__ output, int n, int m) { int i = blockIdx.</p></p><p><h3><a class=title href=/posts/cuda-memory-coalescing-access/>Memory coalescing in CUDA (1) – VecAdd</a></h3><i data-feather=calendar></i> <time datetime=2024-02-25>Feb 25, 2024</time>
<span id=social></span><i data-feather=tag></i>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/cuda>cuda</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/basics>basics</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/tech>tech</a><p>Background Memory coalescing is a crucial optimization technique in CUDA programming that allows optimal usage of the global memory bandwidth. When threads in the same warp running the same instruction access to consecutive locations in the global memory, the hardware can coalesce these accesses into a single transaction, significantly improving performance.
Coalescing memory access is vital for achieving high performance. Besides PCIe memory traffic, accessing global memory tends to be the largest bottleneck in GPU&rsquo;s memory hierarchy.</p></p><p><h3><a class=title href=/posts/llvm-utils/>LLVM Utilities (keep updating)</a></h3><i data-feather=calendar></i> <time datetime=2023-10-17>Oct 17, 2023</time>
<span id=social></span><i data-feather=tag></i>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/llvm>llvm</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/cpp>cpp</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/tech>tech</a><p>There are many handy functions or data structures in LLVM project, which are widely used by other projects that rely on LLVM. In this page, I will introduce some common utilities that are worthy of using in your own project or frequently used in LLVM code that you should be familiar with.
Basic data type llvm::StringRef It is a lightweight, non-owning reference to a sequence of characters. It is similar to std::string_view introduced in C++17.</p></p><p><h3><a class=title href=/posts/python-best-practices/>Best Practices for Python Programming (Continuously Updated)</a></h3><i data-feather=calendar></i> <time datetime=2023-02-22>Feb 22, 2023</time>
<span id=social></span><i data-feather=tag></i>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/python>python</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/tech>tech</a><p><p>When delving into the codebases of some successful large Python projects such as PyTorch, I am consistently impressed by their code &ndash; whether it&rsquo;s clean yet precise, or leveraging lesser-known built-in or third-party packages to significantly enhance functionality.</p><p>High-quality code snippets, handy packages, and modules have greatly facilitated my work. In this blog, I&rsquo;ll be sharing noteworthy findings and insights learned from the open-source codebase.</p></p></p><p><h3><a class=title href=/posts/triton-mlir-publish/>OpenAI/Triton MLIR 迁移工作简介</a></h3><i data-feather=calendar></i> <time datetime=2022-11-15>Nov 15, 2022</time>
<span id=social></span><i data-feather=tag></i>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/triton>triton</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/system>system</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/tech>tech</a><p><p>经过几个月的不懈努力，OpenAI Triton已经成功完成了面向MLIR Infra的迁移/重构工作，并将其最新的基于MLIR的代码合并至主分支。这个工作是由OpenAI和NVIDIA相关团队近几个月来深入合作完成的，而我也有幸参与其中。在这篇文章中，我将分享一些技术总结，记录一些收获和思考。</p><p>尽管Triton目前的开源开发非常迅速，但本文将主要聚焦于基于MLIR Infra进行重构的第一个版本的<a href=https://github.com/openai/triton/tree/ca05ef8e5b0b4d4834957bc31e7581b09d35c530>代码</a>（这应该也是两三个月前的）</p></p></p><p><h3><a class=title href=/posts/emacs-essentials/>Emacs Essentials</a></h3><i data-feather=calendar></i> <time datetime=2022-10-15>Oct 15, 2022</time>
<span id=social></span><i data-feather=tag></i>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/emacs>emacs</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/tech>tech</a><p>It is a steep learning curve to master Emacs lisp, there are mainly two issues in it from my experience
the lisp syntax and functional programming the fragmented methods and libraries For the 1st issue, it is easy to master the syntax after writing several programs and getting used to them, but for the 2nd one, one needs to take notes or remember something.
In this blog, I focus on the 2nd point and keep updating the notes of some methods and libraries that I think are essential for writing Emacs lisp packages.</p></p><p class="footer text-center">Copyright (c) 2024 Chunwei Yan</p></main></div><script src=/js/feather.min.js></script><script>feather.replace()</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script></body></html>