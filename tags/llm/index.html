<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=stylesheet type=text/css href=/css/bootstrap.min.css><link rel=stylesheet type=text/css href=/css/style.css><title>Superjomn's blog | LLM</title>
<meta name=google-site-verification content="kO2XszgjIr1OyaOubGpeb0M34ADBz27vyI1dLETarCM"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-Z60BDN3JGH"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Z60BDN3JGH")</script><script src=https://cdn.jsdelivr.net/npm/cash-dom/dist/cash.min.js></script><script>$(function(){$("article table").addClass("table")})</script><body><div id=nav-border class=container><nav id=nav class="nav justify-content-center"><a class=nav-link href=/><i data-feather=home></i>
Home
</a><a class=nav-link href=/tags/tech><i data-feather=terminal></i>
Tech
</a><a class=nav-link href=/tags/life><i data-feather=coffee></i>
Life
</a><a class=nav-link href=/posts/><i data-feather=archive></i>
Archive
</a><a class=nav-link href=/tags/><i data-feather=tag></i>
Tags
</a><a class=nav-link href=/posts/about/><i data-feather=info></i>
About</a></nav></div><div class=container><main id=main><h1>LLM</h1><p><h3><a class=title href=/posts/count-parameters-in-llama/>Count the parameters in LLaMA V1 model</a></h3><i data-feather=calendar></i> <time datetime=2024-03-21>Mar 21, 2024</time>
<span id=social></span><i data-feather=tag></i>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/llm>LLM</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/tech>tech</a><p>Let&rsquo;s load the model
from transformers import LlamaModel, LlamaConfig model = LlamaModel.from_pretrained("llama-7b-hf-path") def count_params(model, is_human: bool = False): params: int = sum(p.numel() for p in model.parameters() if p.requires_grad) return f"{params / 1e6:.2f}M" if is_human else params print(model) print("Total # of params:", count_params(model, is_human=True)) Print out the layers:
LlamaModel( (embed_tokens): Embedding(32000, 4096, padding_idx=0) (layers): ModuleList( (0-31): 32 x LlamaDecoderLayer( (self_attn): LlamaSdpaAttention( (q_proj): Linear(in_features=4096, out_features=4096, bias=False) (k_proj): Linear(in_features=4096, out_features=4096, bias=False) (v_proj): Linear(in_features=4096, out_features=4096, bias=False) (o_proj): Linear(in_features=4096, out_features=4096, bias=False) (rotary_emb): LlamaRotaryEmbedding() ) (mlp): LlamaMLP( (gate_proj): Linear(in_features=4096, out_features=11008, bias=False) (up_proj): Linear(in_features=4096, out_features=11008, bias=False) (down_proj): Linear(in_features=11008, out_features=4096, bias=False) (act_fn): SiLU() ) (input_layernorm): LlamaRMSNorm() (post_attention_layernorm): LlamaRMSNorm() ) ) (norm): LlamaRMSNorm() ) Total # of params: 6607.</p></p><p><h3><a class=title href=/posts/llm_notes/>Notes on LLM technologies (keep updating)</a></h3><i data-feather=calendar></i> <time datetime=2024-03-10>Mar 10, 2024</time>
<span id=social></span><i data-feather=tag></i>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/llm>LLM</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/tech>tech</a><p>Brief notes on LLM technologies.
Models GPT2 Model structure The GPT model employs a repeated structure of Transformer Blocks, each containing two sub-layers: a Masked Multi-Head Attention (MMHA) layer and a Position-wise Feed-Forward Network.
The MMHA is a central component of the model. It operates by splitting the input into multiple &lsquo;heads&rsquo;, each of which learns to attend to different positions within the input sequence, allowing the model to focus on different aspects of the input simultaneously.</p></p><p class="footer text-center">Copyright (c) 2024 Chunwei Yan</p></main></div><script src=/js/feather.min.js></script><script>feather.replace()</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script></body></html>