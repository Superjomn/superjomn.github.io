<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLM on Superjomn's blog</title><link>/tags/llm/</link><description>Recent content in LLM on Superjomn's blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 21 Mar 2024 00:00:00 +0000</lastBuildDate><atom:link href="/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>Count the parameters in LLaMA V1 model</title><link>/posts/count-parameters-in-llama/</link><pubDate>Thu, 21 Mar 2024 00:00:00 +0000</pubDate><guid>/posts/count-parameters-in-llama/</guid><description>&lt;p>Let&amp;rsquo;s load the model&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020;font-weight:bold">from&lt;/span> &lt;span style="color:#0e84b5;font-weight:bold">transformers&lt;/span> &lt;span style="color:#007020;font-weight:bold">import&lt;/span> LlamaModel, LlamaConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>model &lt;span style="color:#666">=&lt;/span> LlamaModel&lt;span style="color:#666">.&lt;/span>from_pretrained(&lt;span style="color:#4070a0">&amp;#34;llama-7b-hf-path&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020;font-weight:bold">def&lt;/span> &lt;span style="color:#06287e">count_params&lt;/span>(model, is_human: &lt;span style="color:#007020">bool&lt;/span> &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#007020;font-weight:bold">False&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> params: &lt;span style="color:#007020">int&lt;/span> &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#007020">sum&lt;/span>(p&lt;span style="color:#666">.&lt;/span>numel() &lt;span style="color:#007020;font-weight:bold">for&lt;/span> p &lt;span style="color:#007020;font-weight:bold">in&lt;/span> model&lt;span style="color:#666">.&lt;/span>parameters() &lt;span style="color:#007020;font-weight:bold">if&lt;/span> p&lt;span style="color:#666">.&lt;/span>requires_grad)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">return&lt;/span> &lt;span style="color:#4070a0">f&lt;/span>&lt;span style="color:#4070a0">&amp;#34;&lt;/span>&lt;span style="color:#70a0d0">{&lt;/span>params &lt;span style="color:#666">/&lt;/span> &lt;span style="color:#40a070">1e6&lt;/span>&lt;span style="color:#70a0d0">:&lt;/span>&lt;span style="color:#4070a0">.2f&lt;/span>&lt;span style="color:#70a0d0">}&lt;/span>&lt;span style="color:#4070a0">M&amp;#34;&lt;/span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> is_human &lt;span style="color:#007020;font-weight:bold">else&lt;/span> params
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020">print&lt;/span>(model)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020">print&lt;/span>(&lt;span style="color:#4070a0">&amp;#34;Total # of params:&amp;#34;&lt;/span>, count_params(model, is_human&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#007020;font-weight:bold">True&lt;/span>))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Print out the layers:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>LlamaModel(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (embed_tokens): Embedding(32000, 4096, padding_idx=0)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (layers): ModuleList(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (0-31): 32 x LlamaDecoderLayer(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (self_attn): LlamaSdpaAttention(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (rotary_emb): LlamaRotaryEmbedding()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (mlp): LlamaMLP(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (act_fn): SiLU()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (input_layernorm): LlamaRMSNorm()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (post_attention_layernorm): LlamaRMSNorm()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (norm): LlamaRMSNorm()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Total # of params: 6607.34M
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The Transformers shows that there are 6607.34M float16 parameters, roughly 13GB, that is aligned to the actual weight size.&lt;/p>
&lt;h2 id="the-basic-setting-of-the-7b-model">The basic setting of the 7B model&lt;/h2>
&lt;ul>
&lt;li>model dimension \(d_{model}=4096\)&lt;/li>
&lt;li>number of heads \(n_{head}=32\)&lt;/li>
&lt;li>head size \(d_{head} = \frac{d_{model}}{n_{head}}\)&lt;/li>
&lt;li>dimension of the feed-forward network&amp;rsquo;s inner layer \(d_{ff}=11008\)&lt;/li>
&lt;li>number of tokens \(n_{token}=32000\)&lt;/li>
&lt;li>number of transformer layers \(n_{layer}=32\)&lt;/li>
&lt;/ul>
&lt;h2 id="layer-by-layer-parameter-count">Layer-by-Layer Parameter Count&lt;/h2>
&lt;h3 id="embedding-layer">Embedding layer&lt;/h3>
&lt;p>For vocabulary embedding, \(n_{token}\times d_{model}=131.072M\), while for position embedding, since RoPE doesn&amp;rsquo;t need a separate embedding, so that is 0.&lt;/p>
&lt;h3 id="transformer-layers">Transformer layers&lt;/h3>
&lt;h4 id="input-layernorm-and-post-attention-layernorm">&lt;code>input_layernorm&lt;/code> and &lt;code>post_attention_layernorm&lt;/code>&lt;/h4>
&lt;p>Both are RMSNorm whose parameters are \(d_{model}\), so both sum to \(2\times d_{model}=8M\)&lt;/p>
&lt;h4 id="multi-head-self-attention">multi-head self-attention&lt;/h4>
&lt;p>For Q,K,V and O, each is a Linear layer of size \(d_{model} \times d_{model}\), so in total, there are \(4\times d_{model}^2=67.1M\).&lt;/p>
&lt;p>There is one tiny issue here, why a linear layer could generate Q, while in the original transformer paper, each head is calculated separately, for example, \(Q_i=QW^Q_i\) where \(i\) is the head id. That is because, if we concatenate all all the heads, that is identical to a linear of \(d_{model} \times (n_{head} \times d_{head})\), that is \(d_{model} \times d_{model}\) in llama v1.&lt;/p>
&lt;p>The self-attention doesn&amp;rsquo;t have extra parameters since they simply applies the following formula&lt;/p>
&lt;p>\[
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
\]&lt;/p>
&lt;h4 id="mlp">mlp&lt;/h4>
&lt;p>The LlamaMLP layer contains three separate Linear layers:&lt;/p>
&lt;ol>
&lt;li>&lt;code>gate_proj&lt;/code>: \(d_{model} \times d_{ff}\)&lt;/li>
&lt;li>&lt;code>up_proj&lt;/code>: \(d_{model} \times d_{ff}\)&lt;/li>
&lt;li>&lt;code>down_proj&lt;/code>: \(d_{ff} \times d_{model}\)&lt;/li>
&lt;/ol>
&lt;p>So in total, they have \(3\times d_{model} \times d_{ff} = 135.27M\) parameters.&lt;/p>
&lt;h2 id="total-count-of-parameters">Total count of parameters&lt;/h2>
&lt;p>The overall parameters are composed of two major parts, the vocabulary embedding, and the transformer layers, that is &lt;code>embed + 32 * (mha + mlp + norm)&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>\(embed=n_{token}\times d_{model}=131.07M\)&lt;/li>
&lt;li>\(mha=4* d_{model}^2=67.1M\)&lt;/li>
&lt;li>\(mlp=3* d_{model}\times d_{ff}=135.27M\)&lt;/li>
&lt;li>\(norm=2*d_{model}=8.19M\)&lt;/li>
&lt;/ul>
&lt;p>And the count of the parameters is 6607.3M, which is aligned to the number from Transformers.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020;font-weight:bold">def&lt;/span> &lt;span style="color:#06287e">count_llama_params&lt;/span>(d_model, d_ff, n_tokens, n_layers):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> embed &lt;span style="color:#666">=&lt;/span> n_tokens &lt;span style="color:#666">*&lt;/span> d_model
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mha &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#40a070">4&lt;/span> &lt;span style="color:#666">*&lt;/span> d_model&lt;span style="color:#666">**&lt;/span>&lt;span style="color:#40a070">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mlp &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#40a070">3&lt;/span> &lt;span style="color:#666">*&lt;/span> d_moel &lt;span style="color:#666">*&lt;/span> d_ff
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> norm &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#40a070">2&lt;/span> &lt;span style="color:#666">*&lt;/span> d_model
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">return&lt;/span> embed &lt;span style="color:#666">+&lt;/span> n_layers &lt;span style="color:#666">*&lt;/span> (mha &lt;span style="color:#666">+&lt;/span> mlp &lt;span style="color:#666">+&lt;/span> norm)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For example, the Llama 65B model&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>LlamaModel(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (embed_tokens): Embedding(32000, 8192, padding_idx=0)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (layers): ModuleList(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (0-79): 80 x LlamaDecoderLayer(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (self_attn): LlamaSdpaAttention(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (q_proj): Linear(in_features=8192, out_features=8192, bias=False)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (k_proj): Linear(in_features=8192, out_features=8192, bias=False)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (v_proj): Linear(in_features=8192, out_features=8192, bias=False)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (o_proj): Linear(in_features=8192, out_features=8192, bias=False)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (rotary_emb): LlamaRotaryEmbedding()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (mlp): LlamaMLP(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (gate_proj): Linear(in_features=8192, out_features=22016, bias=False)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (up_proj): Linear(in_features=8192, out_features=22016, bias=False)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (down_proj): Linear(in_features=22016, out_features=8192, bias=False)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (act_fn): SiLU()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (input_layernorm): LlamaRMSNorm()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (post_attention_layernorm): LlamaRMSNorm()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> (norm): LlamaRMSNorm()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Total # of params: 65023.52M
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And let&amp;rsquo;s use the function&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>count_llama_params(d_model&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#40a070">8192&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> d_ff&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#40a070">22016&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> n_tokens&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#40a070">32000&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> n_layers&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#40a070">80&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It gives 65023.5M, is is roughly aligned.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://michaelwornow.net/2024/01/18/counting-params-in-transformer">Transformer Math (Part 1) - Counting Model Parameters&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py">modeling_llama.py from huggingface transformers&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on LLM technologies (keep updating)</title><link>/posts/llm_notes/</link><pubDate>Sun, 10 Mar 2024 00:00:00 +0000</pubDate><guid>/posts/llm_notes/</guid><description>&lt;p>Brief notes on LLM technologies.&lt;/p>
&lt;h2 id="models">Models&lt;/h2>
&lt;h3 id="gpt2">GPT2&lt;/h3>
&lt;h4 id="model-structure">Model structure&lt;/h4>
&lt;figure>&lt;img src="/ox-hugo/GPT%20model%20structure.png">
&lt;/figure>
&lt;p>The GPT model employs a repeated structure of Transformer Blocks, each containing two sub-layers: a Masked Multi-Head Attention (MMHA) layer and a Position-wise Feed-Forward Network.&lt;/p>
&lt;p>The MMHA is a central component of the model. It operates by splitting the input into multiple &amp;lsquo;heads&amp;rsquo;, each of which learns to attend to different positions within the input sequence, allowing the model to focus on different aspects of the input simultaneously. The output of these heads is then concatenated and linearly transformed to produce the final output.&lt;/p>
&lt;p>The MMHA mechanism can be formally defined as follows:&lt;/p>
&lt;p>\[
MultiHead(Q,K,V) = Concat(head_1, \cdots, head_h)W^O
\]&lt;/p>
&lt;p>where each head is computed as:&lt;/p>
&lt;p>\[
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
\]&lt;/p>
&lt;p>In implementation, the computation of \(Q,K,V\) can be packed together with Linear operations regardless of the number of heads, like&lt;/p>
&lt;figure>&lt;img src="/ox-hugo/qkv.png">
&lt;/figure>
&lt;p>And the computation is as below&lt;/p>
&lt;p>\begin{split}
Q &amp;amp;= xW^Q \\
K &amp;amp;= xW^K \\
V &amp;amp;= xW^V
\end{split}&lt;/p>
&lt;p>The Attention function is defined as:&lt;/p>
&lt;p>\[
Attention(q_i, k_i, v_i) = softmax(\frac{q_i k_i^T}{\sqrt{d_k}})v_i
\]&lt;/p>
&lt;p>Here, \(d_k\) represents the dimension of the keys, which is calculated as \(d_k = \frac{H}{h}\), where \(H\) is the total dimension of the input and \(h\) is the number of heads.&lt;/p>
&lt;p>To ensure the MHA mechanism works correctly with sequences of varying lengths, a Mask is applied. This Mask effectively ignores padding elements by setting their values to \(-\infty\), allowing the Softmax function to handle them appropriately.
The layer output demensions are as below:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Layer&lt;/th>
&lt;th>Dimensions&lt;/th>
&lt;th>Note&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Model input&lt;/td>
&lt;td>&lt;code>[bs, seq_len]&lt;/code>&lt;/td>
&lt;td>Token IDs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Text &amp;amp; PosEmbed&lt;/td>
&lt;td>&lt;code>[bs, seq_len, H]&lt;/code>&lt;/td>
&lt;td>Text embeddings + position embeddings&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Layer Norm (0)&lt;/td>
&lt;td>&lt;code>[bs, seq_len, H]&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Feed Forward&lt;/td>
&lt;td>&lt;code>[bs, seq_len, H]&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Layer Norm (1)&lt;/td>
&lt;td>&lt;code>[bs, seq_len, H]&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>\(head_i\)&lt;/td>
&lt;td>&lt;code>[bs, seq_len, H/h]&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMHA&lt;/td>
&lt;td>&lt;code>[bs, seq_len, H]&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Where&lt;/p>
&lt;ul>
&lt;li>&lt;code>bs&lt;/code> is the batch size&lt;/li>
&lt;li>&lt;code>seq_len&lt;/code> is the max length of the sequence&lt;/li>
&lt;li>&lt;code>H&lt;/code> is the size of the hidden state&lt;/li>
&lt;li>&lt;code>h&lt;/code> is the number of heads&lt;/li>
&lt;/ul>
&lt;h3 id="reference">Reference&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://arxiv.org/abs/2211.17192">Leviathan, Yaniv, Matan Kalman, and Yossi Matias. &amp;ldquo;Fast inference from transformers via speculative decoding.&amp;rdquo; International Conference on Machine Learning. PMLR, 2023.&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html">Multi-head attention in Pytorch&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="lora">Lora&lt;/h2>
&lt;h3 id="algorithm">Algorithm&lt;/h3>
&lt;p>&lt;img src="/ox-hugo/2024-03-17_15-41-23_screenshot.png" alt="">
&lt;em>(image borrowed from &lt;a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms">this page&lt;/a>)&lt;/em>&lt;/p>
&lt;p>A classical workflow to finetune an LLM is to learn an additional parameters denoted as \(\delta W\) as long as frooze the original parameters, just as the left part of the figure below.&lt;/p>
&lt;p>\[
h = W_0 x \Rightarrow (W_0 + \delta W) x
\]&lt;/p>
&lt;p>This could be applied on the \(W_q, W_k, W_v\) and \(W_o\) in the Transformer block, while since the Transformer blocks contains the mojority of the parameters, that workflow could result in significant increase of additional parameters.&lt;/p>
&lt;p>For instance, a Llama V1 7B model, whose hidden size is 4096, and the dimension of the MLP&amp;rsquo;s inner layer is 11008, let&amp;rsquo;s zoom into a Transformer layer, the \(W_q, W_k, W_v, W_o\), each contains \(4096 \times 4096 = 4096 \times 128=16.78M\) parameters, and the MLP contains three Linear layer of \(135.27M\) parameters, and two RMSNorm layers each contains \(4096=4M\) parameters. In total, a single Transformer layer contains \(16.78M \times 4 + 135.27M + 4M\times 2=210.29M\). You can refer to &lt;a href="https://superjomn.github.io/posts/count-parameters-in-llama/">Superjomn&amp;rsquo;s blog | Count the parameters in LLaMA V1 model&lt;/a> for details about counting the parameters.&lt;/p>
&lt;p>The LoRA is for such scenarios, instead of learning the \(\delta W\) itself, it learns decomposed representation of \(\delta W\) directly during finetune training. Since the rank could be \(8\), that could reduce the number of trainable parameters required for adaptation to downstream tasks. Let&amp;rsquo;s revisit the Llama V1 7B example, if we apply LoRA on all the Linear layers within a Transformer layer:&lt;/p>
&lt;ul>
&lt;li>\(W_q, W_k, W_v, W_o\) each will take \(4096*8 + 8*4096=0.065M\) parameters&lt;/li>
&lt;li>MLP have \(3 \times (4096 \times 8 + 8 \times 11008)=0.362M\)&lt;/li>
&lt;/ul>
&lt;p>So in total, the LoRA will bring \(0.362+0.065*4=0.622\) additional parameters, that is only \(\frac{0.622}{210.29}=0.29\%\) of the original parameters.&lt;/p>
&lt;p>So instead of fully-finetune all the original paramters, the LoRA could finetune the LLaMA 7B model with less than 1% parameters, that is quite efficient.&lt;/p>
&lt;h3 id="reference">Reference&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms">Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)&lt;/a>&lt;/li>
&lt;li>Hu, Edward J., et al. &amp;ldquo;Lora: Low-rank adaptation of large language models.&amp;rdquo; arXiv preprint arXiv:2106.09685 (2021).&lt;/li>
&lt;/ul>
&lt;h2 id="speculative-decoding">Speculative decoding&lt;/h2>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;p>Consider a scenerio where we have a prefix such as &amp;ldquo;Geoffrey Hinton did his PhD at the University&amp;rdquo;, and the target suffix is &amp;ldquo;of Edinburgh&amp;rdquo;. When a LLM continues the generation, it is evident that:&lt;/p>
&lt;ol>
&lt;li>The word &amp;ldquo;of&amp;rdquo; is simple to generate and could be produced by a smaller model given the same prefix&lt;/li>
&lt;li>The word &amp;ldquo;Edinburgh&amp;rdquo; is more challenging to generate and may require a larger model with more knowledge&lt;/li>
&lt;/ol>
&lt;p>Speculative decoding addresses this by using a smaller model to generate &amp;ldquo;easy&amp;rdquo; words like &amp;ldquo;of&amp;rdquo; for better throughput, while leaving more challenging words to a larger model for precision.&lt;/p>
&lt;h3 id="algorithm">Algorithm&lt;/h3>
&lt;p>Speculative decoding employs two models:&lt;/p>
&lt;ol>
&lt;li>A draft model, denoted as \(M_p\), which is smaller and much faster (as least 2X) to give a sub-sequence of the next K tokens.&lt;/li>
&lt;li>A target model, denoted as \(M_q\), which is larger and more precise. It evaluates the sub-sequence generated by the draft model.&lt;/li>
&lt;/ol>
&lt;p>Assuming K to be 4, the prefix to be \(pf\), and the draft model generates five tokens based on \(pf\):&lt;/p>
&lt;ol>
&lt;li>Token \(x_1\), the probability is \(p_1(x) = M_p(pf)\)&lt;/li>
&lt;li>Token \(x_2\) with probability of \(p_2(x) = M_p(pf, x_1)\)&lt;/li>
&lt;li>Token \(x_3\) with probability of \(p_3(x) = M_p(pf, x_1, x_2)\)&lt;/li>
&lt;li>Token \(x_4\) with probability of \(p_4(x) = M_p(pf, x_1, x_2, x_3)\)&lt;/li>
&lt;/ol>
&lt;p>The target model evalutes K tokens generated by \(M_p\) with a single model forward pass, similar to the training phase:&lt;/p>
&lt;p>\[
q_1(x), q_2(x), q_3(x), q_4(x) = M_q(pf, x_1, x_2, x_3, x_4)
\]&lt;/p>
&lt;p>Let&amp;rsquo;s consider a real example to illustrate the heuristics. Suppose the draft model generate the following sub-sequence with \(K=4\):&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Token&lt;/th>
&lt;th>x1&lt;/th>
&lt;th>x2&lt;/th>
&lt;th>x3&lt;/th>
&lt;th>x4&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>dogs&lt;/td>
&lt;td>love&lt;/td>
&lt;td>chasing&lt;/td>
&lt;td>after&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>p(x)&lt;/td>
&lt;td>0.8&lt;/td>
&lt;td>0.7&lt;/td>
&lt;td>0.9&lt;/td>
&lt;td>0.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>q(x)&lt;/td>
&lt;td>0.9&lt;/td>
&lt;td>0.8&lt;/td>
&lt;td>0.8&lt;/td>
&lt;td>0.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>q(x)&amp;gt;=p(x)?&lt;/td>
&lt;td>Y&lt;/td>
&lt;td>Y&lt;/td>
&lt;td>N&lt;/td>
&lt;td>N&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>accept prob&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0.8/0.9&lt;/td>
&lt;td>0.3/0.8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The rules is as below:&lt;/p>
&lt;ol>
&lt;li>If \(q(x) &amp;gt;= p(x)\), then accept the token.&lt;/li>
&lt;li>If not, the accept probability is \(\frac{q(x)}{p(x)}\), so the token &amp;ldquo;chasing&amp;rdquo; has a probability of \(\frac{0.8}{0.9}=89\%\), while the next token &amp;ldquo;after&amp;rdquo; has an accept probability of only \(\frac{0.3}{0.8}=37.5\%\).&lt;/li>
&lt;li>If a word is unaccepted, the candidate word after it will be dropped as well. It will be resampled by target model, not the draft model.&lt;/li>
&lt;li>Repeat the steps above from the next position&lt;/li>
&lt;/ol>
&lt;h3 id="reference">Reference&lt;/h3>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=S-8yr_RibJ4">Speculative Decoding: When Two LLMs are Faster than One - YouTube&lt;/a>&lt;/p></description></item></channel></rss>