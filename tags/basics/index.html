<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=stylesheet type=text/css href=/css/bootstrap.min.css><link rel=stylesheet type=text/css href=/css/style.css><title>Superjomn's blog | Basics</title>
<meta name=google-site-verification content="kO2XszgjIr1OyaOubGpeb0M34ADBz27vyI1dLETarCM"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-Z60BDN3JGH"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Z60BDN3JGH")</script><script src=https://cdn.jsdelivr.net/npm/cash-dom/dist/cash.min.js></script><script>$(function(){$("article table").addClass("table")})</script><body><div id=nav-border class=container><nav id=nav class="nav justify-content-center"><a class=nav-link href=/><i data-feather=home></i>
Home
</a><a class=nav-link href=/tags/tech><i data-feather=terminal></i>
Tech
</a><a class=nav-link href=/tags/life><i data-feather=coffee></i>
Life
</a><a class=nav-link href=/posts/><i data-feather=archive></i>
Archive
</a><a class=nav-link href=/tags/><i data-feather=tag></i>
Tags
</a><a class=nav-link href=/posts/about/><i data-feather=info></i>
About</a></nav></div><div class=container><main id=main><h1>Basics</h1><p><h3><a class=title href=/posts/gpu-get-props/>Get GPU Properties</a></h3><i data-feather=calendar></i> <time datetime=2024-03-11>Mar 11, 2024</time>
<span id=social></span><i data-feather=tag></i>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/gpu>gpu</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/basics>basics</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/tech>tech</a><p>In `cuda_runtime.h`, there are several APIs for retrieving properties for the installed GPU.
cudaDeviceGetAttribute(int* value, cudaDeviceAttr attr, int device): a C api cudaGetDeviceProperties ( cudaDeviceProp* prop, int device ) : a C++ api Here is the code of the example.
On a Nvidia GTX 3080 GPU, the properties are as below:
Device 0 properties: Max block dimensions: 1024 x 1024 x 64 Max grid dimensions: 2147483647 x 65535 x 65535 Shared memory bank size: 4 bytes Max shared memory per block: 49152 bytes Max registers per block: 65536 Warp size: 32 Multiprocessor count: 68 Max resident threads per multiprocessor: 1536 = 48 warps L2 cache size: 5242880 bytes Global L1 cache supported: yes Total global memory: 9 GB Processor clock: 1 MHZ Memory clock: 9 MHZ</p></p><p><h3><a class=title href=/posts/cuda-memory-coalescing-access-matrix-transpose/>Memory coalescing in CUDA (2) – Matrix Transpose</a></h3><i data-feather=calendar></i> <time datetime=2024-03-05>Mar 5, 2024</time>
<span id=social></span><i data-feather=tag></i>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/cuda>cuda</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/basics>basics</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/tech>tech</a><p>Background In the VecAdd page, we&rsquo;ve introduced the memory coalescing in global memory access. This post will follow the topic with another interesting application: Matrix transposing.
The following content will briefly touch on the following topics:
Tiles in matrix, this is the basis of optimization matrix computation A simple trick to avoid bank conflict in shared memory access Kernels The code for all the kernels locates in 1-matrix-transpose-coalesce.cu.
Read coalesced template &lt;typename T> __global__ void transpose_read_coalesce( const T* __restrict__ input, T* __restrict__ output, int n, int m) { int i = blockIdx.</p></p><p><h3><a class=title href=/posts/cuda-memory-coalescing-access/>Memory coalescing in CUDA (1) – VecAdd</a></h3><i data-feather=calendar></i> <time datetime=2024-02-25>Feb 25, 2024</time>
<span id=social></span><i data-feather=tag></i>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/cuda>cuda</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/basics>basics</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/tech>tech</a><p>Background Memory coalescing is a crucial optimization technique in CUDA programming that allows optimal usage of the global memory bandwidth. When threads in the same warp running the same instruction access to consecutive locations in the global memory, the hardware can coalesce these accesses into a single transaction, significantly improving performance.
Coalescing memory access is vital for achieving high performance. Besides PCIe memory traffic, accessing global memory tends to be the largest bottleneck in GPU&rsquo;s memory hierarchy.</p></p><p class="footer text-center">Copyright (c) 2024 Chunwei Yan</p></main></div><script src=/js/feather.min.js></script><script>feather.replace()</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script></body></html>