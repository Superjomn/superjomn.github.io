<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Basics on Superjomn's blog</title><link>/tags/basics/</link><description>Recent content in Basics on Superjomn's blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 25 Mar 2024 00:00:00 +0000</lastBuildDate><atom:link href="/tags/basics/index.xml" rel="self" type="application/rss+xml"/><item><title>Reduce kernel in CUDA</title><link>/posts/reduce-cuda/</link><pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate><guid>/posts/reduce-cuda/</guid><description>&lt;h2 id="question-definition">Question definition&lt;/h2>
&lt;p>Given an array of \(n\) integers, the goal is to compute the sum of all elements within the array.&lt;/p>
&lt;h2 id="solutions">Solutions&lt;/h2>
&lt;p>The implementations for all kernel versions can be found at &lt;a href="https://github.com/Superjomn/cuda-from-scratch/blob/dev/2-reduce.cu">2-reduce.cu on GitHub&lt;/a>.&lt;/p>
&lt;h3 id="naive-version-with-atomicadd">Naive version with atomicAdd&lt;/h3>
&lt;p>The simplest approach involves utilizing each thread to perform an &lt;code>atomicAdd&lt;/code> operation on the output variable. Here&amp;rsquo;s how the kernel is defined:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-C++" data-lang="C++">&lt;span style="display:flex;">&lt;span>__global__ &lt;span style="color:#902000">void&lt;/span> &lt;span style="color:#06287e">reduce_naive_atomic&lt;/span>(&lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> g_idata, &lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> g_odata, &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> n)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> idx &lt;span style="color:#666">=&lt;/span> blockIdx.x &lt;span style="color:#666">*&lt;/span> blockDim.x &lt;span style="color:#666">+&lt;/span> threadIdx.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> gridSize &lt;span style="color:#666">=&lt;/span> blockDim.x &lt;span style="color:#666">*&lt;/span> gridDim.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> sum &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#40a070">0&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">for&lt;/span> (&lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> i &lt;span style="color:#666">=&lt;/span> idx; i &lt;span style="color:#666">&amp;lt;&lt;/span> n; i &lt;span style="color:#666">+=&lt;/span> gridSize)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sum &lt;span style="color:#666">+=&lt;/span> g_idata[i];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> atomicAdd(g_odata, sum);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And the kernel launcher is straightforward, invoking the kernel a single time:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-C++" data-lang="C++">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#902000">int&lt;/span> &lt;span style="color:#06287e">launch_reduce&lt;/span>(&lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> g_idata, &lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> g_odata, &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> n, &lt;span style="color:#902000">int&lt;/span> block_size, kernel_fn kernel, cudaStream_t stream)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> idata &lt;span style="color:#666">=&lt;/span> g_idata;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> odata &lt;span style="color:#666">=&lt;/span> g_odata;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">uint32_t&lt;/span> num_warps &lt;span style="color:#666">=&lt;/span> block_size &lt;span style="color:#666">/&lt;/span> &lt;span style="color:#40a070">32&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> smem_size &lt;span style="color:#666">=&lt;/span> num_warps &lt;span style="color:#666">*&lt;/span> &lt;span style="color:#007020;font-weight:bold">sizeof&lt;/span>(&lt;span style="color:#902000">int&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> num_blocks &lt;span style="color:#666">=&lt;/span> ceil(n, block_size);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#60a0b0;font-style:italic">// Launch the kernel
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> kernel&lt;span style="color:#666">&amp;lt;&amp;lt;&amp;lt;&lt;/span>num_blocks, block_size, smem_size, stream&lt;span style="color:#666">&amp;gt;&amp;gt;&amp;gt;&lt;/span>(idata, odata, n);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (&lt;span style="color:#666">!&lt;/span>FLAGS_profile)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cudaStreamSynchronize(stream);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#60a0b0;font-style:italic">// Copy the final result back to the host
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> &lt;span style="color:#902000">int&lt;/span> h_out;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> NVCHECK(cudaMemcpyAsync(&lt;span style="color:#666">&amp;amp;&lt;/span>h_out, odata, &lt;span style="color:#007020;font-weight:bold">sizeof&lt;/span>(&lt;span style="color:#902000">int&lt;/span>), cudaMemcpyDeviceToHost, stream));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">return&lt;/span> h_out;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>When tested on a GTX 4080, this method achieved a throughput of approximately 82GB/s.&lt;/p>
&lt;h3 id="tiled-reduction-with-shared-memory">Tiled reduction with shared memory&lt;/h3>
&lt;p>A classical approach involves leveraging a thread block to perform local reductions on a tile within shared memory.
This method encompasses several kernel versions, each with different optimizations.&lt;/p>
&lt;h4 id="basic-version">Basic version&lt;/h4>
&lt;p>The initial implementation is as below:&lt;/p>
&lt;ol>
&lt;li>A tile of data is collaboratively loaded into shared memory.&lt;/li>
&lt;li>A partial reduction on this data tile is executed within a thread block, get the sum of the tile.&lt;/li>
&lt;li>The sum is then written to a designated spot in the global memory&amp;rsquo;s output slot. It&amp;rsquo;s important to note that this kernel requires a temporary buffer for writing partial results from each thread block.&lt;/li>
&lt;li>The process repeats with the size \(n\) reduced to \(\frac{n}{blockSize}\), continuing until \(n=1\).&lt;/li>
&lt;/ol>
&lt;!-- raw HTML omitted -->
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-C++" data-lang="C++">&lt;span style="display:flex;">&lt;span>__global__ &lt;span style="color:#902000">void&lt;/span> &lt;span style="color:#06287e">reduce_smem_naive&lt;/span>(&lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> g_idata, &lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> g_odata, &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> n)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">extern&lt;/span> __shared__ &lt;span style="color:#902000">int&lt;/span> sdata[];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> tid &lt;span style="color:#666">=&lt;/span> threadIdx.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> i &lt;span style="color:#666">=&lt;/span> blockIdx.x &lt;span style="color:#666">*&lt;/span> blockDim.x &lt;span style="color:#666">+&lt;/span> threadIdx.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#60a0b0;font-style:italic">// Read a block of data into shared memory collectively
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> sdata[tid] &lt;span style="color:#666">=&lt;/span> (i &lt;span style="color:#666">&amp;lt;&lt;/span> n) &lt;span style="color:#666">?&lt;/span> g_idata[i] &lt;span style="color:#666">:&lt;/span> &lt;span style="color:#40a070">0&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> __syncthreads();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">for&lt;/span> (&lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> stride &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#40a070">1&lt;/span>; stride &lt;span style="color:#666">&amp;lt;&lt;/span> blockDim.x; stride &lt;span style="color:#666">*=&lt;/span> &lt;span style="color:#40a070">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#60a0b0;font-style:italic">// ISSUE: divergent warps
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (tid &lt;span style="color:#666">%&lt;/span> (&lt;span style="color:#40a070">2&lt;/span> &lt;span style="color:#666">*&lt;/span> stride) &lt;span style="color:#666">==&lt;/span> &lt;span style="color:#40a070">0&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sdata[tid] &lt;span style="color:#666">+=&lt;/span> sdata[tid &lt;span style="color:#666">+&lt;/span> stride];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> __syncthreads(); &lt;span style="color:#60a0b0;font-style:italic">// need to sync per level
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#60a0b0;font-style:italic">// Write the result for this block to global memory
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (tid &lt;span style="color:#666">==&lt;/span> &lt;span style="color:#40a070">0&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> g_odata[blockIdx.x] &lt;span style="color:#666">=&lt;/span> sdata[&lt;span style="color:#40a070">0&lt;/span>];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Launching this kernel multiple times involves a slightly more complex launcher:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-C++" data-lang="C++">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#902000">int&lt;/span> &lt;span style="color:#06287e">launch_reduce&lt;/span>(&lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> g_idata, &lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> g_odata, &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> n, &lt;span style="color:#902000">int&lt;/span> block_size, kernel_fn kernel, cudaStream_t stream,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">uint32_t&lt;/span> num_blocks, &lt;span style="color:#902000">uint32_t&lt;/span> smem_size)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> idata &lt;span style="color:#666">=&lt;/span> g_idata;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> odata &lt;span style="color:#666">=&lt;/span> g_odata;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (smem_size &lt;span style="color:#666">==&lt;/span> &lt;span style="color:#40a070">0&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> smem_size &lt;span style="color:#666">=&lt;/span> block_size &lt;span style="color:#666">*&lt;/span> &lt;span style="color:#007020;font-weight:bold">sizeof&lt;/span>(&lt;span style="color:#902000">int&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#60a0b0;font-style:italic">// Calculate the number of blocks
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> num_blocks &lt;span style="color:#666">=&lt;/span> (num_blocks &lt;span style="color:#666">&amp;gt;&lt;/span> &lt;span style="color:#40a070">0&lt;/span>) &lt;span style="color:#666">?&lt;/span> &lt;span style="color:#002070;font-weight:bold">num_blocks&lt;/span> : (n &lt;span style="color:#666">+&lt;/span> block_size &lt;span style="color:#666">-&lt;/span> &lt;span style="color:#40a070">1&lt;/span>) &lt;span style="color:#666">/&lt;/span> block_size;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (&lt;span style="color:#666">!&lt;/span>FLAGS_profile)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> printf(&lt;span style="color:#4070a0">&amp;#34;- launching: num_blocks: %d, block_size:%d, n:%d&lt;/span>&lt;span style="color:#4070a0;font-weight:bold">\n&lt;/span>&lt;span style="color:#4070a0">&amp;#34;&lt;/span>, num_blocks, block_size, n);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> level &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#40a070">0&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#60a0b0;font-style:italic">// Launch the kernel
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> kernel&lt;span style="color:#666">&amp;lt;&amp;lt;&amp;lt;&lt;/span>num_blocks, block_size, smem_size, stream&lt;span style="color:#666">&amp;gt;&amp;gt;&amp;gt;&lt;/span>(idata, odata, n);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (&lt;span style="color:#666">!&lt;/span>FLAGS_profile)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cudaStreamSynchronize(stream);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> level&lt;span style="color:#666">++&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#60a0b0;font-style:italic">// Recursively reduce the partial sums
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> &lt;span style="color:#007020;font-weight:bold">while&lt;/span> (num_blocks &lt;span style="color:#666">&amp;gt;&lt;/span> &lt;span style="color:#40a070">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> std&lt;span style="color:#666">::&lt;/span>swap(idata, odata);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> n &lt;span style="color:#666">=&lt;/span> num_blocks;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> num_blocks &lt;span style="color:#666">=&lt;/span> (n &lt;span style="color:#666">+&lt;/span> block_size &lt;span style="color:#666">-&lt;/span> &lt;span style="color:#40a070">1&lt;/span>) &lt;span style="color:#666">/&lt;/span> block_size;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kernel&lt;span style="color:#666">&amp;lt;&amp;lt;&amp;lt;&lt;/span>num_blocks, block_size, smem_size, stream&lt;span style="color:#666">&amp;gt;&amp;gt;&amp;gt;&lt;/span>(idata, odata, n);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (&lt;span style="color:#666">!&lt;/span>FLAGS_profile)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cudaStreamSynchronize(stream);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#60a0b0;font-style:italic">// Copy the final result back to the host
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> &lt;span style="color:#902000">int&lt;/span> h_out;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> NVCHECK(cudaMemcpyAsync(&lt;span style="color:#666">&amp;amp;&lt;/span>h_out, odata, &lt;span style="color:#007020;font-weight:bold">sizeof&lt;/span>(&lt;span style="color:#902000">int&lt;/span>), cudaMemcpyDeviceToHost, stream));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">return&lt;/span> h_out;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>All tiled reduction kenrels utilize the aforementioned launcher, achieving a throughput of 54GB/s. This is less efficient compared to the atomic naive version, which reaches 82GB/s.&lt;/p>
&lt;h4 id="avoid-thread-divergence">Avoid thread divergence&lt;/h4>
&lt;p>The basic version encounters significant thread divergence, particularly noticeable at &lt;code>if (tid % (2 * stride) == 0)&lt;/code>.
Here is an optimized variant:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-C++" data-lang="C++">&lt;span style="display:flex;">&lt;span>__global__ &lt;span style="color:#902000">void&lt;/span> &lt;span style="color:#06287e">reduce_smem_1_avoid_divergent_warps&lt;/span>(&lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> g_idata, &lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> g_odata, &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> n)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">extern&lt;/span> __shared__ &lt;span style="color:#902000">int&lt;/span> sdata[];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> tid &lt;span style="color:#666">=&lt;/span> threadIdx.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> i &lt;span style="color:#666">=&lt;/span> blockIdx.x &lt;span style="color:#666">*&lt;/span> blockDim.x &lt;span style="color:#666">+&lt;/span> threadIdx.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sdata[tid] &lt;span style="color:#666">=&lt;/span> (i &lt;span style="color:#666">&amp;lt;&lt;/span> n) &lt;span style="color:#666">?&lt;/span> g_idata[i] &lt;span style="color:#666">:&lt;/span> &lt;span style="color:#40a070">0&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> __syncthreads();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">for&lt;/span> (&lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> stride &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#40a070">1&lt;/span>; stride &lt;span style="color:#666">&amp;lt;&lt;/span> blockDim.x; stride &lt;span style="color:#666">*=&lt;/span> &lt;span style="color:#40a070">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> index &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#40a070">2&lt;/span> &lt;span style="color:#666">*&lt;/span> stride &lt;span style="color:#666">*&lt;/span> tid;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (index &lt;span style="color:#666">&amp;lt;&lt;/span> blockDim.x)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#60a0b0;font-style:italic">// Issue: bank conflict
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> sdata[index] &lt;span style="color:#666">+=&lt;/span> sdata[index &lt;span style="color:#666">+&lt;/span> stride];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> __syncthreads();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (tid &lt;span style="color:#666">==&lt;/span> &lt;span style="color:#40a070">0&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> g_odata[blockIdx.x] &lt;span style="color:#666">=&lt;/span> sdata[&lt;span style="color:#40a070">0&lt;/span>];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The optimization yields a 70GB/s throughput, marking a 29% improvement over the basic version.&lt;/p>
&lt;h4 id="read-two-elements-one-time">Read two elements one time&lt;/h4>
&lt;p>The preceding version&amp;rsquo;s DRAM throughput was only 20.63%, likely due to&lt;/p>
&lt;ol>
&lt;li>Insufficient grid size for small inputs, leading to underutilized thread resources.&lt;/li>
&lt;li>Each thread reading a single element at a time, given the fixed number of resident thread blocks per SM for a specific kernel, results in a limited number of load instructions issued.&lt;/li>
&lt;/ol>
&lt;p>To enhance DRAM throughput, especially for smaller grid sizes, threads can be configured to read more than one element at a time.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-C++" data-lang="C++">&lt;span style="display:flex;">&lt;span>__global__ &lt;span style="color:#902000">void&lt;/span> &lt;span style="color:#06287e">reduce_smem_3_read_two&lt;/span>(&lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> g_idata, &lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> g_odata, &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> n)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">extern&lt;/span> __shared__ &lt;span style="color:#902000">int&lt;/span> sdata[];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> tid &lt;span style="color:#666">=&lt;/span> threadIdx.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> i &lt;span style="color:#666">=&lt;/span> blockIdx.x &lt;span style="color:#666">*&lt;/span> (blockDim.x &lt;span style="color:#666">*&lt;/span> &lt;span style="color:#40a070">2&lt;/span>) &lt;span style="color:#666">+&lt;/span> threadIdx.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020">#define GET_ELEM(__idx) ((__idx) &amp;lt; n ? g_idata[(__idx)] : 0)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sdata[tid] &lt;span style="color:#666">=&lt;/span> GET_ELEM(i) &lt;span style="color:#666">+&lt;/span> GET_ELEM(i &lt;span style="color:#666">+&lt;/span> blockDim.x);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> __syncthreads();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">for&lt;/span> (&lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> stride &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#40a070">1&lt;/span>; stride &lt;span style="color:#666">&amp;lt;&lt;/span> blockDim.x; stride &lt;span style="color:#666">*=&lt;/span> &lt;span style="color:#40a070">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> index &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#40a070">2&lt;/span> &lt;span style="color:#666">*&lt;/span> stride &lt;span style="color:#666">*&lt;/span> tid;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (index &lt;span style="color:#666">&amp;lt;&lt;/span> blockDim.x)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#60a0b0;font-style:italic">// Issue: bank conflict
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> sdata[index] &lt;span style="color:#666">+=&lt;/span> sdata[index &lt;span style="color:#666">+&lt;/span> stride];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> __syncthreads();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (tid &lt;span style="color:#666">==&lt;/span> &lt;span style="color:#40a070">0&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> g_odata[blockIdx.x] &lt;span style="color:#666">=&lt;/span> sdata[&lt;span style="color:#40a070">0&lt;/span>];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This approach improves the DRAM Throughput to 33.78%, a significant 63.72% increase over the previous method.
The overall throughput reaches 96.51GB/s, demonstrating 37.87% enhancement from the 70GB/s achieved earlier.&lt;/p>
&lt;h3 id="tiled-reduction-with-warp-shuffle">Tiled Reduction with Warp Shuffle&lt;/h3>
&lt;p>Modern GPUs facilitate direct data exchange within a warp, bypassing the need for shared memory.&lt;/p>
&lt;p>The function below demonstrates how to conduct a reduction within a single warp using the warp shuffle instruction, as highlighted in the book &amp;lt;Professional CUDA C Programming&amp;gt;.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-C++" data-lang="C++">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">// using warp shuffle instruction
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">// From book &amp;lt;Professional CUDA C Programming&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span>__inline__ __device__ &lt;span style="color:#902000">int&lt;/span> &lt;span style="color:#06287e">warpReduce&lt;/span>(&lt;span style="color:#902000">int&lt;/span> mySum)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mySum &lt;span style="color:#666">+=&lt;/span> __shfl_xor(mySum, &lt;span style="color:#40a070">16&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mySum &lt;span style="color:#666">+=&lt;/span> __shfl_xor(mySum, &lt;span style="color:#40a070">8&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mySum &lt;span style="color:#666">+=&lt;/span> __shfl_xor(mySum, &lt;span style="color:#40a070">4&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mySum &lt;span style="color:#666">+=&lt;/span> __shfl_xor(mySum, &lt;span style="color:#40a070">2&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mySum &lt;span style="color:#666">+=&lt;/span> __shfl_xor(mySum, &lt;span style="color:#40a070">1&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">return&lt;/span> mySum;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>When a thread block contains multiple warps, synchronization becomes essential.
Utilizing shared memory to store the sum computed by each warp and subsequently reducing these sums as previously described enables the calculation of a thread block&amp;rsquo;s total sum.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-C++" data-lang="C++">&lt;span style="display:flex;">&lt;span>__global__ &lt;span style="color:#902000">void&lt;/span> &lt;span style="color:#06287e">reduce_warp_shlf&lt;/span>(&lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> g_idata, &lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> g_odata, &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> n)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#60a0b0;font-style:italic">// Helps to share data between warps
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> &lt;span style="color:#60a0b0;font-style:italic">// size should be (blockDim.x / warpSize)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> &lt;span style="color:#007020;font-weight:bold">extern&lt;/span> __shared__ &lt;span style="color:#902000">int&lt;/span> sdata[];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> idx &lt;span style="color:#666">=&lt;/span> blockIdx.x &lt;span style="color:#666">*&lt;/span> blockDim.x &lt;span style="color:#666">+&lt;/span> threadIdx.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#60a0b0;font-style:italic">// Necessary to make sure shfl instruction is not used with uninitialized data
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> &lt;span style="color:#902000">int&lt;/span> mySum &lt;span style="color:#666">=&lt;/span> idx &lt;span style="color:#666">&amp;lt;&lt;/span> n &lt;span style="color:#666">?&lt;/span> g_idata[idx] &lt;span style="color:#666">:&lt;/span> &lt;span style="color:#40a070">0&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> lane &lt;span style="color:#666">=&lt;/span> threadIdx.x &lt;span style="color:#666">%&lt;/span> warpSize;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> warp &lt;span style="color:#666">=&lt;/span> threadIdx.x &lt;span style="color:#666">/&lt;/span> warpSize;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mySum &lt;span style="color:#666">=&lt;/span> warpReduce(mySum);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (lane &lt;span style="color:#666">==&lt;/span> &lt;span style="color:#40a070">0&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sdata[warp] &lt;span style="color:#666">=&lt;/span> mySum;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> __syncthreads();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#60a0b0;font-style:italic">// last warp reduce
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> mySum &lt;span style="color:#666">=&lt;/span> (threadIdx.x &lt;span style="color:#666">&amp;lt;&lt;/span> blockDim.x &lt;span style="color:#666">/&lt;/span> warpSize) &lt;span style="color:#666">?&lt;/span> sdata[lane] &lt;span style="color:#666">:&lt;/span> &lt;span style="color:#40a070">0&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (warp &lt;span style="color:#666">==&lt;/span> &lt;span style="color:#40a070">0&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mySum &lt;span style="color:#666">=&lt;/span> warpReduce(mySum);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (threadIdx.x &lt;span style="color:#666">==&lt;/span> &lt;span style="color:#40a070">0&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> g_odata[blockIdx.x] &lt;span style="color:#666">=&lt;/span> mySum;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Despite reading only a single element per thread, this kernel can achieve a throughput of 96GB/s, outperforming the shared memory version&amp;rsquo;s 70GB/s.
Furthermore, the kernel can be modified to read \(NT\) elements at a time for enhanced efficiency:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-C++" data-lang="C++">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020;font-weight:bold">template&lt;/span> &lt;span style="color:#666">&amp;lt;&lt;/span>&lt;span style="color:#902000">int&lt;/span> NT&lt;span style="color:#666">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>__global__ &lt;span style="color:#902000">void&lt;/span> reduce_warp_shlf_read_N(&lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> g_idata, &lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> g_odata, &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> n)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#60a0b0;font-style:italic">// Helps to share data between warps
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> &lt;span style="color:#60a0b0;font-style:italic">// size should be (blockDim.x / warpSize)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> &lt;span style="color:#007020;font-weight:bold">extern&lt;/span> __shared__ &lt;span style="color:#902000">int&lt;/span> sdata[];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> blockSize &lt;span style="color:#666">=&lt;/span> NT &lt;span style="color:#666">*&lt;/span> blockDim.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> idx &lt;span style="color:#666">=&lt;/span> blockIdx.x &lt;span style="color:#666">*&lt;/span> blockSize &lt;span style="color:#666">+&lt;/span> threadIdx.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">// Necessary to make sure shfl instruction is not used with uninitialized data
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span>&lt;span style="color:#007020">#define GET_ELEM(__idx) ((__idx) &amp;lt; n ? g_idata[(__idx)] : 0)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020">&lt;/span> &lt;span style="color:#902000">int&lt;/span> mySum &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#40a070">0&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020">#pragma unroll
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020">&lt;/span> &lt;span style="color:#007020;font-weight:bold">for&lt;/span> (&lt;span style="color:#902000">int&lt;/span> i &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#40a070">0&lt;/span>; i &lt;span style="color:#666">&amp;lt;&lt;/span> NT; i&lt;span style="color:#666">++&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mySum &lt;span style="color:#666">+=&lt;/span> GET_ELEM(idx &lt;span style="color:#666">+&lt;/span> i &lt;span style="color:#666">*&lt;/span> blockDim.x);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> lane &lt;span style="color:#666">=&lt;/span> threadIdx.x &lt;span style="color:#666">%&lt;/span> warpSize;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> warp &lt;span style="color:#666">=&lt;/span> threadIdx.x &lt;span style="color:#666">/&lt;/span> warpSize;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mySum &lt;span style="color:#666">=&lt;/span> warpReduce(mySum);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (lane &lt;span style="color:#666">==&lt;/span> &lt;span style="color:#40a070">0&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sdata[warp] &lt;span style="color:#666">=&lt;/span> mySum;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> __syncthreads();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#60a0b0;font-style:italic">// last warp reduce
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> mySum &lt;span style="color:#666">=&lt;/span> (threadIdx.x &lt;span style="color:#666">&amp;lt;&lt;/span> blockDim.x &lt;span style="color:#666">/&lt;/span> warpSize) &lt;span style="color:#666">?&lt;/span> sdata[lane] &lt;span style="color:#666">:&lt;/span> &lt;span style="color:#40a070">0&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (warp &lt;span style="color:#666">==&lt;/span> &lt;span style="color:#40a070">0&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mySum &lt;span style="color:#666">=&lt;/span> warpReduce(mySum);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (threadIdx.x &lt;span style="color:#666">==&lt;/span> &lt;span style="color:#40a070">0&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> g_odata[blockIdx.x] &lt;span style="color:#666">=&lt;/span> mySum;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Performance varies with different \(N\) values, as summarized below:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>NT&lt;/th>
&lt;th>throughput (GB/s)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>96.3187&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>96.2341&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>96.8153&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>8&lt;/td>
&lt;td>107.226&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="warp-shuffle-combined-with-atomic-operations">Warp Shuffle Combined with Atomic Operations&lt;/h3>
&lt;p>Compared to tiled reduction solutions, utilizing &lt;code>atomicAdd&lt;/code> eliminates the need for a temporary buffer and requires only a single kernel launch.
This segment explores combining warp shuffle and atomic operations for efficient reduction.&lt;/p>
&lt;p>The kernel template below demonstrates this approach, utilizing warp shuffle instructions to enhance the warp reduction performance, and leveraging atomic operations to write directly to the output slot without the need for temporary buffer and multiple kernel launches.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-C++" data-lang="C++">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020;font-weight:bold">template&lt;/span> &lt;span style="color:#666">&amp;lt;&lt;/span>&lt;span style="color:#902000">int&lt;/span> NT&lt;span style="color:#666">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>__global__ &lt;span style="color:#902000">void&lt;/span> reduce_warp_shlf_read_N_atomic(&lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> g_idata, &lt;span style="color:#902000">int&lt;/span>&lt;span style="color:#666">*&lt;/span> g_odata, &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> n)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#60a0b0;font-style:italic">// Helps to share data between warps
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> &lt;span style="color:#60a0b0;font-style:italic">// size should be (blockDim.x / warpSize)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> &lt;span style="color:#007020;font-weight:bold">extern&lt;/span> __shared__ &lt;span style="color:#902000">int&lt;/span> sdata[];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> blockSize &lt;span style="color:#666">=&lt;/span> NT &lt;span style="color:#666">*&lt;/span> blockDim.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">unsigned&lt;/span> &lt;span style="color:#902000">int&lt;/span> idx &lt;span style="color:#666">=&lt;/span> blockIdx.x &lt;span style="color:#666">*&lt;/span> blockSize &lt;span style="color:#666">+&lt;/span> threadIdx.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">// Necessary to make sure shfl instruction is not used with uninitialized data
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">// This only needs one turn of launch
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span>&lt;span style="color:#007020">#define GET_ELEM(__idx) ((__idx) &amp;lt; n ? g_idata[(__idx)] : 0)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020">&lt;/span> &lt;span style="color:#902000">int&lt;/span> mySum &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#40a070">0&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020">#pragma unroll
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020">&lt;/span> &lt;span style="color:#007020;font-weight:bold">for&lt;/span> (&lt;span style="color:#902000">int&lt;/span> i &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#40a070">0&lt;/span>; i &lt;span style="color:#666">&amp;lt;&lt;/span> NT; i&lt;span style="color:#666">++&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mySum &lt;span style="color:#666">+=&lt;/span> GET_ELEM(idx &lt;span style="color:#666">+&lt;/span> i &lt;span style="color:#666">*&lt;/span> blockDim.x);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> lane &lt;span style="color:#666">=&lt;/span> threadIdx.x &lt;span style="color:#666">%&lt;/span> warpSize;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> warp &lt;span style="color:#666">=&lt;/span> threadIdx.x &lt;span style="color:#666">/&lt;/span> warpSize;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mySum &lt;span style="color:#666">=&lt;/span> warpReduce(mySum);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (lane &lt;span style="color:#666">==&lt;/span> &lt;span style="color:#40a070">0&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sdata[warp] &lt;span style="color:#666">=&lt;/span> mySum;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> __syncthreads();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#60a0b0;font-style:italic">// last warp reduce
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> mySum &lt;span style="color:#666">=&lt;/span> (threadIdx.x &lt;span style="color:#666">&amp;lt;&lt;/span> blockDim.x &lt;span style="color:#666">/&lt;/span> warpSize) &lt;span style="color:#666">?&lt;/span> sdata[lane] &lt;span style="color:#666">:&lt;/span> &lt;span style="color:#40a070">0&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (warp &lt;span style="color:#666">==&lt;/span> &lt;span style="color:#40a070">0&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mySum &lt;span style="color:#666">=&lt;/span> warpReduce(mySum);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (threadIdx.x &lt;span style="color:#666">==&lt;/span> &lt;span style="color:#40a070">0&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> atomicAdd(g_odata, mySum);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Remarkably, this kernel achieves a throughput of 121.777 GB/s under the same conditions.&lt;/p>
&lt;h2 id="benchmark">Benchmark&lt;/h2>
&lt;p>The benchmark results illustrate the performance of different CUDA optimization strategies under varying conditions.&lt;/p>
&lt;figure>&lt;img src="/ox-hugo/2024-04-06_16-47-39_screenshot.png">
&lt;/figure>
&lt;p>Note that the optimal kernel configuration may vary depending on the size of the input data(\(n\)).&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf">Optimizing Parallel Reduction in CUDA&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developer.nvidia.com/blog/faster-parallel-reductions-kepler/">Faster Parallel Reductions on Kepler | NVIDIA Technical Blog&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Get GPU Properties</title><link>/posts/gpu-get-props/</link><pubDate>Mon, 11 Mar 2024 00:00:00 +0000</pubDate><guid>/posts/gpu-get-props/</guid><description>&lt;p>In `cuda_runtime.h`, there are several APIs for retrieving properties for the installed GPU.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gb22e8256592b836df9a9cc36c9db7151">cudaDeviceGetAttribute(int* value, cudaDeviceAttr attr, int device)&lt;/a>: a C api&lt;/li>
&lt;li>&lt;a href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g1bf9d625a931d657e08db2b4391170f0">cudaGetDeviceProperties ( cudaDeviceProp* prop, int device ) &lt;/a>: a C++ api&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://github.com/Superjomn/cuda-from-scratch/blob/dev/dump-gpu-props.cpp">Here&lt;/a> is the code of the example.&lt;/p>
&lt;p>On a Nvidia GTX 3080 GPU, the properties are as below:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>Device 0 properties:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Max block dimensions: 1024 x 1024 x 64
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Max grid dimensions: 2147483647 x 65535 x 65535
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Shared memory bank size: 4 bytes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Max shared memory per block: 49152 bytes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Max registers per block: 65536
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Warp size: 32
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Multiprocessor count: 68
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Max resident threads per multiprocessor: 1536 = 48 warps
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> L2 cache size: 5242880 bytes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Global L1 cache supported: yes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Total global memory: 9 GB
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Processor clock: 1 MHZ
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Memory clock: 9 MHZ
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Memory coalescing in CUDA (2)  Matrix Transpose</title><link>/posts/cuda-memory-coalescing-access-matrix-transpose/</link><pubDate>Tue, 05 Mar 2024 00:00:00 +0000</pubDate><guid>/posts/cuda-memory-coalescing-access-matrix-transpose/</guid><description>&lt;h2 id="background">Background&lt;/h2>
&lt;p>In the &lt;a href="https://superjomn.github.io/posts/cuda-memory-coalescing-access/">VecAdd&lt;/a> page, we&amp;rsquo;ve introduced the memory coalescing in global memory access. This post will follow the topic with another interesting application: Matrix transposing.&lt;/p>
&lt;p>The following content will briefly touch on the following topics:&lt;/p>
&lt;ul>
&lt;li>Tiles in matrix, this is the basis of optimization matrix computation&lt;/li>
&lt;li>A simple trick to avoid bank conflict in shared memory access&lt;/li>
&lt;/ul>
&lt;h2 id="kernels">Kernels&lt;/h2>
&lt;p>The code for all the kernels locates in &lt;a href="https://github.com/Superjomn/cuda-from-scratch/blob/dev/1-matrix-transpose-coalesce.cu">1-matrix-transpose-coalesce.cu&lt;/a>.&lt;/p>
&lt;h3 id="read-coalesced">Read coalesced&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-C++" data-lang="C++">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020;font-weight:bold">template&lt;/span> &lt;span style="color:#666">&amp;lt;&lt;/span>&lt;span style="color:#007020;font-weight:bold">typename&lt;/span> T&lt;span style="color:#666">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>__global__ &lt;span style="color:#902000">void&lt;/span> transpose_read_coalesce(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">const&lt;/span> T&lt;span style="color:#666">*&lt;/span> __restrict__ input,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> T&lt;span style="color:#666">*&lt;/span> __restrict__ output,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> n,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> m) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> i &lt;span style="color:#666">=&lt;/span> blockIdx.x &lt;span style="color:#666">*&lt;/span> blockDim.x &lt;span style="color:#666">+&lt;/span> threadIdx.x; &lt;span style="color:#60a0b0;font-style:italic">// the contiguous tid
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> &lt;span style="color:#902000">int&lt;/span> j &lt;span style="color:#666">=&lt;/span> blockIdx.y &lt;span style="color:#666">*&lt;/span> blockDim.y &lt;span style="color:#666">+&lt;/span> threadIdx.y;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (i &lt;span style="color:#666">&amp;lt;&lt;/span> n &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> j &lt;span style="color:#666">&amp;lt;&lt;/span> m) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> output[i &lt;span style="color:#666">*&lt;/span> m &lt;span style="color:#666">+&lt;/span> j] &lt;span style="color:#666">=&lt;/span> input[j &lt;span style="color:#666">*&lt;/span> n &lt;span style="color:#666">+&lt;/span> i];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="write-coalesced">Write coalesced&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-C++" data-lang="C++">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020;font-weight:bold">template&lt;/span> &lt;span style="color:#666">&amp;lt;&lt;/span>&lt;span style="color:#007020;font-weight:bold">typename&lt;/span> T&lt;span style="color:#666">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>__global__ &lt;span style="color:#902000">void&lt;/span> transpose_write_coalesce(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">const&lt;/span> T&lt;span style="color:#666">*&lt;/span> __restrict__ input,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> T&lt;span style="color:#666">*&lt;/span> __restrict__ output,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> n,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> m) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> i &lt;span style="color:#666">=&lt;/span> blockIdx.x &lt;span style="color:#666">*&lt;/span> blockDim.x &lt;span style="color:#666">+&lt;/span> threadIdx.x; &lt;span style="color:#60a0b0;font-style:italic">// the contiguous tid
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> &lt;span style="color:#902000">int&lt;/span> j &lt;span style="color:#666">=&lt;/span> blockIdx.y &lt;span style="color:#666">*&lt;/span> blockDim.y &lt;span style="color:#666">+&lt;/span> threadIdx.y;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (i &lt;span style="color:#666">&amp;lt;&lt;/span> n &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> j &lt;span style="color:#666">&amp;lt;&lt;/span> m) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> output[j &lt;span style="color:#666">*&lt;/span> n &lt;span style="color:#666">+&lt;/span> i] &lt;span style="color:#666">=&lt;/span> input[i &lt;span style="color:#666">*&lt;/span> m &lt;span style="color:#666">+&lt;/span> j];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="both-read-and-write-coalesced-by-tiling-with-shared-memory">Both read and write coalesced by tiling with shared memory&lt;/h3>
&lt;p>The tiling method is a common methodology for optimizing matrix operation. It divides the matrix into smaller, manageable blocks or &amp;ldquo;tiles&amp;rdquo; that can fit into shared memory.&lt;/p>
&lt;p>Let&amp;rsquo;s divide the matrix into tiles of size \(TILE \times TILE\), and the overall transpose could be decoupled into two sub-levels:&lt;/p>
&lt;ol>
&lt;li>the inter-tile transpose, that is move the tile to the target position; and secondly,&lt;/li>
&lt;li>the intra-tile transpose, that is transpose the elements within a single tile&lt;/li>
&lt;/ol>
&lt;h4 id="inter-tile-transpose">Inter-tile transpose&lt;/h4>
&lt;figure>&lt;img src="/ox-hugo/inter-tile.png">
&lt;/figure>
&lt;p>Each tile is processed by a thread block, so the tile coordinate is &lt;code>(blockIdx.y, blockIdx.x)&lt;/code>, and the target coord is &lt;code>(blockIdx.x, blockIdx.y)&lt;/code>.&lt;/p>
&lt;p>We can continue to process the elements within each tile.&lt;/p>
&lt;h4 id="intra-tile-transpose">Intra-tile transpose&lt;/h4>
&lt;figure>&lt;img src="/ox-hugo/intra-tile.png">
&lt;/figure>
&lt;p>Within a tile, we will read the elements, store the transposed version in the shared memory, and then store the tile in global memory, with the coord determined by the intra-tile transpose phase.&lt;/p>
&lt;p>There are two copies:&lt;/p>
&lt;ol>
&lt;li>Copying the tile from the input matrix and storing a transposed version into shared memory&lt;/li>
&lt;li>Copying the tile from shared memory into the output matrix in global memory&lt;/li>
&lt;/ol>
&lt;p>Only one side is in global memory in both copies, so it can perform a memory coalescing access pattern. Both copies are performed by all the threads collectively within a thread block.&lt;/p>
&lt;p>To make a coalesced memory access, in the first copy, a thread reads element of coord of &lt;code>(threadIdx.y, threadIdx.x)&lt;/code>, and the memory offset &lt;code>threadIdx.y * M + threadIdx.x&lt;/code> is contignuous for adjacent threads.
In the second copy, the thread block needs to copy a tile to global memory, similarly, a thread should process the element of &lt;code>(threadIdx.y, threadIdx.x)&lt;/code> in the output tile.&lt;/p>
&lt;h4 id="kernel-with-constant-tile-size">Kernel with constant tile size&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-C++" data-lang="C++">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020;font-weight:bold">template&lt;/span> &lt;span style="color:#666">&amp;lt;&lt;/span>&lt;span style="color:#007020;font-weight:bold">typename&lt;/span> T, &lt;span style="color:#902000">int&lt;/span> TILE&lt;span style="color:#666">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>__global__ &lt;span style="color:#902000">void&lt;/span> transpose_tiled_coalesce0(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">const&lt;/span> T&lt;span style="color:#666">*&lt;/span> __restrict__ input,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> T&lt;span style="color:#666">*&lt;/span> __restrict__ output,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> n,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> m) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> assert(blockDim.x &lt;span style="color:#666">==&lt;/span> blockDim.y &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> blockDim.x &lt;span style="color:#666">==&lt;/span> TILE_DIM);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#60a0b0;font-style:italic">// TILE + 1 to avoid bank conflict
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> &lt;span style="color:#60a0b0;font-style:italic">// By padding the shared memory array with an extra element, the consecutive threads access
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> &lt;span style="color:#60a0b0;font-style:italic">// memory locations that fall into different banks to avoid bank conflict
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#60a0b0;font-style:italic">&lt;/span> __shared__ T tile[TILE][TILE &lt;span style="color:#666">+&lt;/span> &lt;span style="color:#40a070">1&lt;/span>];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> i &lt;span style="color:#666">=&lt;/span> blockIdx.y &lt;span style="color:#666">*&lt;/span> blockDim.y &lt;span style="color:#666">+&lt;/span> threadIdx.y;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> j &lt;span style="color:#666">=&lt;/span> blockIdx.x &lt;span style="color:#666">*&lt;/span> blockDim.x &lt;span style="color:#666">+&lt;/span> threadIdx.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (i &lt;span style="color:#666">&amp;lt;&lt;/span> m &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> j &lt;span style="color:#666">&amp;lt;&lt;/span> n) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tile[threadIdx.x][threadIdx.y] &lt;span style="color:#666">=&lt;/span> input[i &lt;span style="color:#666">*&lt;/span> n &lt;span style="color:#666">+&lt;/span> j];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> __syncthreads();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> i &lt;span style="color:#666">=&lt;/span> blockIdx.x &lt;span style="color:#666">*&lt;/span> blockDim.x &lt;span style="color:#666">+&lt;/span> threadIdx.y;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> j &lt;span style="color:#666">=&lt;/span> blockIdx.y &lt;span style="color:#666">*&lt;/span> blockDim.y &lt;span style="color:#666">+&lt;/span> threadIdx.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (i &lt;span style="color:#666">&amp;lt;&lt;/span> n &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> j &lt;span style="color:#666">&amp;lt;&lt;/span> m) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> output[i &lt;span style="color:#666">*&lt;/span> m &lt;span style="color:#666">+&lt;/span> j] &lt;span style="color:#666">=&lt;/span> tile[threadIdx.y][threadIdx.x];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that, since each thread processes only one element, so both &lt;code>blockDim.x&lt;/code> and &lt;code>blockDim.y&lt;/code> should equal to &lt;code>TILE&lt;/code>, and &lt;code>TILE&lt;/code> is a constant value.&lt;/p>
&lt;h4 id="kernel-with-dynamic-tile-size">Kernel with dynamic tile size&lt;/h4>
&lt;p>It is possible to allocate the shared memory dynamically, making the &lt;code>TILE&lt;/code> a variable that could be assigned with &lt;code>blockDim.x&lt;/code> or &lt;code>blockDim.y&lt;/code> on the fly.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-C++" data-lang="C++">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020;font-weight:bold">template&lt;/span> &lt;span style="color:#666">&amp;lt;&lt;/span>&lt;span style="color:#007020;font-weight:bold">typename&lt;/span> T&lt;span style="color:#666">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>__global__ &lt;span style="color:#902000">void&lt;/span> transpose_tiled_coalesce1(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">const&lt;/span> T&lt;span style="color:#666">*&lt;/span> __restrict__ input,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> T&lt;span style="color:#666">*&lt;/span> __restrict__ output,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> n,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> m) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">const&lt;/span> size_t TILE &lt;span style="color:#666">=&lt;/span> blockDim.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> assert(blockDim.x &lt;span style="color:#666">==&lt;/span> blockDim.y);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">extern&lt;/span> __shared__ T tile[];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> i &lt;span style="color:#666">=&lt;/span> blockIdx.y &lt;span style="color:#666">*&lt;/span> blockDim.y &lt;span style="color:#666">+&lt;/span> threadIdx.y;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> j &lt;span style="color:#666">=&lt;/span> blockIdx.x &lt;span style="color:#666">*&lt;/span> blockDim.x &lt;span style="color:#666">+&lt;/span> threadIdx.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (i &lt;span style="color:#666">&amp;lt;&lt;/span> m &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> j &lt;span style="color:#666">&amp;lt;&lt;/span> n) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tile[threadIdx.x &lt;span style="color:#666">*&lt;/span> (TILE &lt;span style="color:#666">+&lt;/span> &lt;span style="color:#40a070">1&lt;/span>) &lt;span style="color:#666">+&lt;/span> threadIdx.y] &lt;span style="color:#666">=&lt;/span> input[i &lt;span style="color:#666">*&lt;/span> n &lt;span style="color:#666">+&lt;/span> j];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> __syncthreads();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> i &lt;span style="color:#666">=&lt;/span> blockIdx.x &lt;span style="color:#666">*&lt;/span> blockDim.x &lt;span style="color:#666">+&lt;/span> threadIdx.y;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> j &lt;span style="color:#666">=&lt;/span> blockIdx.y &lt;span style="color:#666">*&lt;/span> blockDim.y &lt;span style="color:#666">+&lt;/span> threadIdx.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (i &lt;span style="color:#666">&amp;lt;&lt;/span> n &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> j &lt;span style="color:#666">&amp;lt;&lt;/span> m) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> output[i &lt;span style="color:#666">*&lt;/span> m &lt;span style="color:#666">+&lt;/span> j] &lt;span style="color:#666">=&lt;/span> tile[threadIdx.y &lt;span style="color:#666">*&lt;/span> (TILE &lt;span style="color:#666">+&lt;/span> &lt;span style="color:#40a070">1&lt;/span>) &lt;span style="color:#666">+&lt;/span> threadIdx.x];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="performance">Performance&lt;/h2>
&lt;p>In NVIDIA GTX 3080, these kernels have a pretty close performance:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Kernel&lt;/th>
&lt;th>Latency&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Read coalesced&lt;/td>
&lt;td>0.0476&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Write coalesced&lt;/td>
&lt;td>0.0474&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>tiled&lt;/td>
&lt;td>0.0478&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://leimao.github.io/blog/CUDA-Coalesced-Memory-Access/">CUDA Coalesced Memory Access - blog of Lei Mao&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Memory coalescing in CUDA (1)  VecAdd</title><link>/posts/cuda-memory-coalescing-access/</link><pubDate>Sun, 25 Feb 2024 00:00:00 +0000</pubDate><guid>/posts/cuda-memory-coalescing-access/</guid><description>&lt;h2 id="background">Background&lt;/h2>
&lt;p>&lt;strong>Memory coalescing&lt;/strong> is a crucial optimization technique in CUDA programming that allows optimal usage of the &lt;strong>global memory bandwidth&lt;/strong>. When threads in the same warp running the same instruction access to &lt;strong>consecutive locations&lt;/strong> in the global memory, the hardware can coalesce these accesses into a single transaction, significantly improving performance.&lt;/p>
&lt;p>Coalescing memory access is vital for achieving high performance. Besides PCIe memory traffic, accessing global memory tends to be the largest bottleneck in GPU&amp;rsquo;s memory hierarchy.
Non-coalesced memory access can lead to underutilization of memory bandwidth.&lt;/p>
&lt;p>In the following post, we will delve deeper into memory coalescing with CUDA code for the classical vector adding.&lt;/p>
&lt;h2 id="vecadd">VecAdd&lt;/h2>
&lt;p>There are three kernels in below. The complete code locates &lt;a href="https://github.com/Superjomn/cuda-from-scratch/blob/dev/0-vecadd-memory-coalesce.cu">here&lt;/a>.&lt;/p>
&lt;h3 id="naive-vecadd-kernel-with-memory-coalescing-enabled">Naive VecAdd kernel with memory coalescing enabled&lt;/h3>
&lt;p>The first program is simple but follows the coalescing memory access pattern:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>tid&lt;/th>
&lt;th>element&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&amp;hellip;&lt;/td>
&lt;td>&amp;hellip;&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The thread 0,1,2,3 visits elements 0,1,2,3, which is contiguous, and results in a coalescing memory accessing.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-cpp" data-lang="cpp">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020;font-weight:bold">template&lt;/span> &lt;span style="color:#666">&amp;lt;&lt;/span>&lt;span style="color:#007020;font-weight:bold">typename&lt;/span> T&lt;span style="color:#666">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>__global__ &lt;span style="color:#902000">void&lt;/span> add_coalesced0(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">const&lt;/span> T&lt;span style="color:#666">*&lt;/span> __restrict__ a,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">const&lt;/span> T&lt;span style="color:#666">*&lt;/span> __restrict__ b,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> T&lt;span style="color:#666">*&lt;/span> __restrict__ c,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> n) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> i &lt;span style="color:#666">=&lt;/span> blockIdx.x &lt;span style="color:#666">*&lt;/span> blockDim.x &lt;span style="color:#666">+&lt;/span> threadIdx.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (i &lt;span style="color:#666">&amp;lt;&lt;/span> n) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> c[i] &lt;span style="color:#666">=&lt;/span> a[i] &lt;span style="color:#666">+&lt;/span> b[i];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The only issue is that, the number of the elements should be no larger than the number of threads, so the launching parameters of the kernel should be carefully designed.&lt;/p>
&lt;h3 id="optimized-one-strided-with-less-threads">Optimized one: strided with less threads&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-cpp" data-lang="cpp">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020;font-weight:bold">template&lt;/span> &lt;span style="color:#666">&amp;lt;&lt;/span>&lt;span style="color:#007020;font-weight:bold">typename&lt;/span> T&lt;span style="color:#666">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>__global__ &lt;span style="color:#902000">void&lt;/span> add_coalesced1(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">const&lt;/span> T&lt;span style="color:#666">*&lt;/span> __restrict__ a,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">const&lt;/span> T&lt;span style="color:#666">*&lt;/span> __restrict__ b,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> T&lt;span style="color:#666">*&lt;/span> __restrict__ c,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> N) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> tid &lt;span style="color:#666">=&lt;/span> threadIdx.x &lt;span style="color:#666">+&lt;/span> blockIdx.x &lt;span style="color:#666">*&lt;/span> blockDim.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> num_threads &lt;span style="color:#666">=&lt;/span> blockDim.x &lt;span style="color:#666">*&lt;/span> gridDim.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">while&lt;/span> (tid &lt;span style="color:#666">&amp;lt;&lt;/span> N) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> c[tid] &lt;span style="color:#666">=&lt;/span> a[tid] &lt;span style="color:#666">+&lt;/span> b[tid];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tid &lt;span style="color:#666">+=&lt;/span> num_threads;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This one simplifies the calculation of the launch thread number, it should fit any number of elements with a arbitrary number of threads.&lt;/p>
&lt;h3 id="uncoalesced-memory-accessing-one">Uncoalesced memory accessing one&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-cpp" data-lang="cpp">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007020;font-weight:bold">template&lt;/span> &lt;span style="color:#666">&amp;lt;&lt;/span>&lt;span style="color:#007020;font-weight:bold">typename&lt;/span> T&lt;span style="color:#666">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>__global__ &lt;span style="color:#902000">void&lt;/span> add_uncoalesced(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">const&lt;/span> T&lt;span style="color:#666">*&lt;/span> __restrict__ a,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">const&lt;/span> T&lt;span style="color:#666">*&lt;/span> __restrict__ b,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> T&lt;span style="color:#666">*&lt;/span> __restrict__ c,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> n) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> tid &lt;span style="color:#666">=&lt;/span> threadIdx.x &lt;span style="color:#666">+&lt;/span> blockIdx.x &lt;span style="color:#666">*&lt;/span> blockDim.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> num_threads &lt;span style="color:#666">=&lt;/span> blockDim.x &lt;span style="color:#666">*&lt;/span> gridDim.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> num_tasks &lt;span style="color:#666">=&lt;/span> nvceil(n, num_threads);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">for&lt;/span> (&lt;span style="color:#902000">int&lt;/span> i &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#40a070">0&lt;/span>; i &lt;span style="color:#666">&amp;lt;&lt;/span> num_tasks; &lt;span style="color:#666">++&lt;/span>i) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#902000">int&lt;/span> idx &lt;span style="color:#666">=&lt;/span> tid &lt;span style="color:#666">*&lt;/span> num_tasks &lt;span style="color:#666">+&lt;/span> i;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007020;font-weight:bold">if&lt;/span> (idx &lt;span style="color:#666">&amp;lt;&lt;/span> n) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> c[idx] &lt;span style="color:#666">=&lt;/span> a[idx] &lt;span style="color:#666">+&lt;/span> b[idx];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This one doesn&amp;rsquo;t follow the coalescing access pattern, lets assume that we have 4 threads with 8 elements, then the `num_tasks=2`&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>tid&lt;/th>
&lt;th>0-th element&lt;/th>
&lt;th>1-st element&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>2&lt;/td>
&lt;td>3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>4&lt;/td>
&lt;td>5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>6&lt;/td>
&lt;td>7&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>In the first step of the for-loop, these four threads visit 0,2,4,6 elements, which is not contiguous, this results in an uncoalesced memory accessing.&lt;/p>
&lt;h2 id="performance">Performance&lt;/h2>
&lt;p>All the kernels are tested with double data type, and the block size is 256, for the last kernels, each thread are setted to consume 8 elements.
The performance is tested on GTX 3090, with the clocks locked as below:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>GPU clocks&lt;/th>
&lt;th>Memory clocks&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>2100 MHZ&lt;/td>
&lt;td>9501MHZ&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The latency of each kernel:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>kernel&lt;/th>
&lt;th>latency&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>coalesced0&lt;/td>
&lt;td>0.04&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>coalesced1&lt;/td>
&lt;td>0.04&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uncoalesced&lt;/td>
&lt;td>0.14&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The uncoalesced kernel is 3x slower than the two coalesced kernel.&lt;/p>
&lt;p>The Nsight also report the Uncoalescing Global Accesses in the uncoalesced kernel:&lt;/p>
&lt;figure>&lt;img src="/ox-hugo/2024-02-28_19-37-47_screenshot.png">
&lt;/figure>
&lt;p>It reports that 75% of the sectors are excessive, IIUC, since only 8 bytes(a double) out each 32 byte transition is valid, so the overall efficiency is \(\frac{8}{32}=25\%\) .&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>Professional CUDA C Programming&lt;/li>
&lt;/ul></description></item></channel></rss>