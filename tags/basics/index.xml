<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Basics on Superjomn's blog</title><link>/tags/basics/</link><description>Recent content in Basics on Superjomn's blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 12 Aug 2025 00:00:00 +0000</lastBuildDate><atom:link href="/tags/basics/index.xml" rel="self" type="application/rss+xml"/><item><title>Elementwise Add Kernel in CUDA</title><link>/posts/element-add-kernel-in-cuda-org-executor.executed/</link><pubDate>Tue, 12 Aug 2025 00:00:00 +0000</pubDate><guid>/posts/element-add-kernel-in-cuda-org-executor.executed/</guid><description>&lt;p&gt;In this post, I will walk through the reduce kernels in &lt;a href="https://github.com/xlite-dev/LeetCUDA/tree/main/kernels"&gt;LeetCUDA&lt;/a&gt; and implement them interactively in this file using org-executor.&lt;/p&gt;
&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;In CUDA programming, an elementwise kernel is a fundamental building block that applies a given operation independently to each element of an input array (or arrays), producing an output array of the same shape. This is highly parallelizable, as each thread can process a single element without dependencies on others.&lt;/p&gt;
&lt;p&gt;Elementwise kernels are commonly used for operations such as vector addition, scaling, activation functions in neural networks, and more. Understanding how to implement an efficient elementwise kernel is essential before moving on to more complex patterns like reductions.&lt;/p&gt;
&lt;p&gt;In the following sections, we will review how to write a basic elementwise kernel in CUDA, discuss its memory access patterns, and explore best practices for maximizing performance.&lt;/p&gt;
&lt;h2 id="environment-setting"&gt;Environment setting&lt;/h2&gt;
&lt;p&gt;Just follow the LeetCUDA&amp;rsquo;s settings, we will expose all the kernels to PyTorch and use its facilities to do performance and precession evaluation.&lt;/p&gt;
&lt;h3 id="pytorch"&gt;PyTorch&lt;/h3&gt;
&lt;p&gt;The python version:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;import&lt;/span&gt; &lt;span style="color:#0e84b5;font-weight:bold"&gt;torch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;print&lt;/span&gt;(torch&lt;span style="color:#666"&gt;.&lt;/span&gt;__version__)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;2.8.0a0+5228986c39.nv25.06&lt;/p&gt;
&lt;h3 id="hardware"&gt;Hardware&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; NVIDIA H100 80GB HBM3, 81559 MiB, 81080 MiB, 0 MiB, 575.57.08, 23, 0 %
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="kernel-launching-utils"&gt;Kernel Launching Utils&lt;/h3&gt;
&lt;p&gt;Common C++ header content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-cpp" data-lang="cpp"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#pragma once
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#include&lt;/span&gt; &lt;span style="color:#007020"&gt;&amp;lt;algorithm&amp;gt;&lt;/span&gt;&lt;span style="color:#007020"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#include&lt;/span&gt; &lt;span style="color:#007020"&gt;&amp;lt;cuda_bf16.h&amp;gt;&lt;/span&gt;&lt;span style="color:#007020"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#include&lt;/span&gt; &lt;span style="color:#007020"&gt;&amp;lt;cuda_fp16.h&amp;gt;&lt;/span&gt;&lt;span style="color:#007020"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#include&lt;/span&gt; &lt;span style="color:#007020"&gt;&amp;lt;cuda_fp8.h&amp;gt;&lt;/span&gt;&lt;span style="color:#007020"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#include&lt;/span&gt; &lt;span style="color:#007020"&gt;&amp;lt;cuda_runtime.h&amp;gt;&lt;/span&gt;&lt;span style="color:#007020"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#include&lt;/span&gt; &lt;span style="color:#007020"&gt;&amp;lt;float.h&amp;gt;&lt;/span&gt;&lt;span style="color:#007020"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#include&lt;/span&gt; &lt;span style="color:#007020"&gt;&amp;lt;stdio.h&amp;gt;&lt;/span&gt;&lt;span style="color:#007020"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#include&lt;/span&gt; &lt;span style="color:#007020"&gt;&amp;lt;stdlib.h&amp;gt;&lt;/span&gt;&lt;span style="color:#007020"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#include&lt;/span&gt; &lt;span style="color:#007020"&gt;&amp;lt;torch/extension.h&amp;gt;&lt;/span&gt;&lt;span style="color:#007020"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#include&lt;/span&gt; &lt;span style="color:#007020"&gt;&amp;lt;torch/types.h&amp;gt;&lt;/span&gt;&lt;span style="color:#007020"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#include&lt;/span&gt; &lt;span style="color:#007020"&gt;&amp;lt;vector&amp;gt;&lt;/span&gt;&lt;span style="color:#007020"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;There are some common code for launching kernel with torch facilities.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-cpp" data-lang="cpp"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#define CEIL(x, y) (((x) + (y) - 1) / (y))
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#define WARP_SIZE 32
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#define INT4(value) (reinterpret_cast&amp;lt;int4*&amp;gt;(&amp;amp;(value))[0])
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#define FLOAT4(value) (reinterpret_cast&amp;lt;float4*&amp;gt;(&amp;amp;(value))[0])
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#define FLOAT4(value) (reinterpret_cast&amp;lt;float4*&amp;gt;(&amp;amp;(value))[0])
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#define HALF2(value) (reinterpret_cast&amp;lt;half2*&amp;gt;(&amp;amp;(value))[0])
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#define BFLOAT2(value) (reinterpret_cast&amp;lt;__nv_bfloat162*&amp;gt;(&amp;amp;(value))[0])
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#define LDST128BITS(value) (reinterpret_cast&amp;lt;float4*&amp;gt;(&amp;amp;(value))[0])
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;inline&lt;/span&gt; &lt;span style="color:#902000"&gt;void&lt;/span&gt; &lt;span style="color:#06287e"&gt;check_torch_dtype&lt;/span&gt;(torch&lt;span style="color:#666"&gt;::&lt;/span&gt;Tensor tensor,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; torch&lt;span style="color:#666"&gt;::&lt;/span&gt;ScalarType expected_dtype) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (tensor.dtype() &lt;span style="color:#666"&gt;!=&lt;/span&gt; expected_dtype) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;throw&lt;/span&gt; std&lt;span style="color:#666"&gt;::&lt;/span&gt;runtime_error(&lt;span style="color:#4070a0"&gt;&amp;#34;Tensor dtype mismatch&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;inline&lt;/span&gt; std&lt;span style="color:#666"&gt;::&lt;/span&gt;tuple&lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt;dim3, dim3&lt;span style="color:#666"&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;get_launch_dimensions(&lt;span style="color:#902000"&gt;int&lt;/span&gt; N, &lt;span style="color:#902000"&gt;int&lt;/span&gt; elements_per_block &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;256&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; element_per_thread &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;1&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;const&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; threads_per_block &lt;span style="color:#666"&gt;=&lt;/span&gt; elements_per_block &lt;span style="color:#666"&gt;/&lt;/span&gt; element_per_thread;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; dim3 &lt;span style="color:#06287e"&gt;block_size&lt;/span&gt;(threads_per_block);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; dim3 &lt;span style="color:#06287e"&gt;grid_size&lt;/span&gt;(CEIL(N, elements_per_block));
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;return&lt;/span&gt; {grid_size, block_size};
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#define TORCH_BINDING_COMMON_EXTENSION(func) m.def(#func, &amp;amp;func, #func);
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#define TORCH_BINDING_ELEM_ADD(packed_type, th_type, element_type, \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt; elements_per_thread) \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt; __global__ void elementwise_add_##packed_type##_kernel( \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt; element_type* __restrict__ a, \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt; element_type* __restrict__ b, \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt; element_type* __restrict__ c, int N); \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt; \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt; void elementwise_add_##packed_type(torch::Tensor A, torch::Tensor B, \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt; torch::Tensor C, \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt; int elements_per_block) { \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt; check_torch_dtype(A, th_type); \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt; check_torch_dtype(B, th_type); \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt; check_torch_dtype(C, th_type); \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt; auto [grid, block] = get_launch_dimensions(A.numel(), elements_per_block, \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt; elements_per_thread); \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt; elementwise_add_##packed_type##_kernel&amp;lt;&amp;lt;&amp;lt;grid, block&amp;gt;&amp;gt;&amp;gt;( \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt; reinterpret_cast&amp;lt;element_type*&amp;gt;(A.data_ptr()), \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt; reinterpret_cast&amp;lt;element_type*&amp;gt;(B.data_ptr()), \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt; reinterpret_cast&amp;lt;element_type*&amp;gt;(C.data_ptr()), A.numel()); \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="kernels"&gt;Kernels&lt;/h2&gt;
&lt;h3 id="basic-kernel"&gt;Basic kernel&lt;/h3&gt;
&lt;p&gt;This kernel demonstrates a basic elementwise addition operation in CUDA, where each thread adds two corresponding elements from the input arrays:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-cpp" data-lang="cpp"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#include&lt;/span&gt; &lt;span style="color:#007020"&gt;&amp;#34;elementwise_add.cuh&amp;#34;&lt;/span&gt;&lt;span style="color:#007020"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;__global__ &lt;span style="color:#902000"&gt;void&lt;/span&gt; &lt;span style="color:#06287e"&gt;elementwise_add_f32_kernel&lt;/span&gt;(&lt;span style="color:#902000"&gt;float&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ a,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;float&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ b,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;float&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ c, &lt;span style="color:#902000"&gt;int&lt;/span&gt; N) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; tid &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (tid &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; N) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; c[tid] &lt;span style="color:#666"&gt;=&lt;/span&gt; a[tid] &lt;span style="color:#666"&gt;+&lt;/span&gt; b[tid];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id="explain"&gt;Explain&lt;/h4&gt;
&lt;p&gt;Each thread loads one float (4 bytes) independently, this should result in poor memory coalescing.&lt;/p&gt;
&lt;h3 id="floatx4-vector-load"&gt;floatx4 vector load&lt;/h3&gt;
&lt;p&gt;This kernel introduces vectorized load and store operations using `float4`, which allows each thread to process four floats at once. By loading 16 bytes (128 bits) per memory transaction instead of 4 bytes, this approach significantly improves memory bandwidth utilization and coalescing efficiency. Each thread processes 4 elements simultaneously, reducing the total number of memory transactions by 4x compared to the basic kernel:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-cpp" data-lang="cpp"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#include&lt;/span&gt; &lt;span style="color:#007020"&gt;&amp;#34;elementwise_add.cuh&amp;#34;&lt;/span&gt;&lt;span style="color:#007020"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;__global__ &lt;span style="color:#902000"&gt;void&lt;/span&gt; &lt;span style="color:#06287e"&gt;elementwise_add_f32x4_kernel&lt;/span&gt;(&lt;span style="color:#902000"&gt;float&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ a,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;float&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ b,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;float&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ c, &lt;span style="color:#902000"&gt;int&lt;/span&gt; N) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; idx &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;4&lt;/span&gt; &lt;span style="color:#666"&gt;*&lt;/span&gt; (blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.x);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (idx &lt;span style="color:#666"&gt;+&lt;/span&gt; &lt;span style="color:#40a070"&gt;3&lt;/span&gt; &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; N) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; float4 reg_a &lt;span style="color:#666"&gt;=&lt;/span&gt; FLOAT4(a[idx]);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; float4 reg_b &lt;span style="color:#666"&gt;=&lt;/span&gt; FLOAT4(b[idx]);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; float4 reg_c;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; reg_c.x &lt;span style="color:#666"&gt;=&lt;/span&gt; reg_a.x &lt;span style="color:#666"&gt;+&lt;/span&gt; reg_b.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; reg_c.y &lt;span style="color:#666"&gt;=&lt;/span&gt; reg_a.y &lt;span style="color:#666"&gt;+&lt;/span&gt; reg_b.y;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; reg_c.z &lt;span style="color:#666"&gt;=&lt;/span&gt; reg_a.z &lt;span style="color:#666"&gt;+&lt;/span&gt; reg_b.z;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; reg_c.w &lt;span style="color:#666"&gt;=&lt;/span&gt; reg_a.w &lt;span style="color:#666"&gt;+&lt;/span&gt; reg_b.w;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; FLOAT4(c[idx]) &lt;span style="color:#666"&gt;=&lt;/span&gt; reg_c;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="fp16x2-kernel"&gt;fp16x2 kernel&lt;/h3&gt;
&lt;p&gt;This kernel leverages half-precision (fp16) data types with vectorized operations using `half2`. Each thread processes 2 half-precision values simultaneously using CUDA&amp;rsquo;s native half2 intrinsics. This provides both memory bandwidth improvements (loading 4 bytes per transaction) and computational efficiency through packed arithmetic operations:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-cpp" data-lang="cpp"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#include&lt;/span&gt; &lt;span style="color:#007020"&gt;&amp;#34;elementwise_add.cuh&amp;#34;&lt;/span&gt;&lt;span style="color:#007020"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;__global__ &lt;span style="color:#902000"&gt;void&lt;/span&gt; &lt;span style="color:#06287e"&gt;elementwise_add_f16x2_kernel&lt;/span&gt;(half&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ a, half&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ b, half&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ c, &lt;span style="color:#902000"&gt;int&lt;/span&gt; N) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; idx &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;2&lt;/span&gt; &lt;span style="color:#666"&gt;*&lt;/span&gt; (blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.x);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (idx &lt;span style="color:#666"&gt;+&lt;/span&gt; &lt;span style="color:#40a070"&gt;1&lt;/span&gt; &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; N) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; half2 reg_a &lt;span style="color:#666"&gt;=&lt;/span&gt; HALF2(a[idx]);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; half2 reg_b &lt;span style="color:#666"&gt;=&lt;/span&gt; HALF2(b[idx]);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; half2 reg_c &lt;span style="color:#666"&gt;=&lt;/span&gt; __hadd2(reg_a, reg_b);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; HALF2(c[idx]) &lt;span style="color:#666"&gt;=&lt;/span&gt; reg_c;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="fp16x8-kernel"&gt;fp16x8 kernel&lt;/h3&gt;
&lt;p&gt;This kernel extends the vectorization approach to process 8 half-precision values per thread, using four `half2` packed operations.
This maximizes memory throughput by loading 16 bytes (128 bits) per thread while maintaining efficient packed arithmetic.
The kernel includes proper bounds checking for each half2 pair to handle cases where the array size is not perfectly divisible by 8:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-cpp" data-lang="cpp"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#include&lt;/span&gt; &lt;span style="color:#007020"&gt;&amp;#34;elementwise_add.cuh&amp;#34;&lt;/span&gt;&lt;span style="color:#007020"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;__global__ &lt;span style="color:#902000"&gt;void&lt;/span&gt; &lt;span style="color:#06287e"&gt;elementwise_add_f16x8_kernel&lt;/span&gt;(half&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ a,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; half&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ b,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; half&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ c, &lt;span style="color:#902000"&gt;int&lt;/span&gt; N) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;const&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; linearThreadId &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;const&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; idx &lt;span style="color:#666"&gt;=&lt;/span&gt; linearThreadId &lt;span style="color:#666"&gt;*&lt;/span&gt; &lt;span style="color:#40a070"&gt;8&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;const&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; remaining &lt;span style="color:#666"&gt;=&lt;/span&gt; N &lt;span style="color:#666"&gt;-&lt;/span&gt; idx;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (remaining &lt;span style="color:#666"&gt;&amp;lt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;return&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// Fast path: full 8 elements
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (remaining &lt;span style="color:#666"&gt;&amp;gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;8&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// Single 128-bit loads for A and B
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; float4 vec_a &lt;span style="color:#666"&gt;=&lt;/span&gt; LDST128BITS(a[idx]);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; float4 vec_b &lt;span style="color:#666"&gt;=&lt;/span&gt; LDST128BITS(b[idx]);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// Reinterpret as four half2 lanes, compute, then store as 128-bit
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;union&lt;/span&gt; &lt;span style="color:#0e84b5;font-weight:bold"&gt;Pack16&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; float4 f4;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; half2 h2[&lt;span style="color:#40a070"&gt;4&lt;/span&gt;];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; } pa, pb, pc;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; pa.f4 &lt;span style="color:#666"&gt;=&lt;/span&gt; vec_a;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; pb.f4 &lt;span style="color:#666"&gt;=&lt;/span&gt; vec_b;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; pc.h2[&lt;span style="color:#40a070"&gt;0&lt;/span&gt;] &lt;span style="color:#666"&gt;=&lt;/span&gt; __hadd2(pa.h2[&lt;span style="color:#40a070"&gt;0&lt;/span&gt;], pb.h2[&lt;span style="color:#40a070"&gt;0&lt;/span&gt;]);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; pc.h2[&lt;span style="color:#40a070"&gt;1&lt;/span&gt;] &lt;span style="color:#666"&gt;=&lt;/span&gt; __hadd2(pa.h2[&lt;span style="color:#40a070"&gt;1&lt;/span&gt;], pb.h2[&lt;span style="color:#40a070"&gt;1&lt;/span&gt;]);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; pc.h2[&lt;span style="color:#40a070"&gt;2&lt;/span&gt;] &lt;span style="color:#666"&gt;=&lt;/span&gt; __hadd2(pa.h2[&lt;span style="color:#40a070"&gt;2&lt;/span&gt;], pb.h2[&lt;span style="color:#40a070"&gt;2&lt;/span&gt;]);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; pc.h2[&lt;span style="color:#40a070"&gt;3&lt;/span&gt;] &lt;span style="color:#666"&gt;=&lt;/span&gt; __hadd2(pa.h2[&lt;span style="color:#40a070"&gt;3&lt;/span&gt;], pb.h2[&lt;span style="color:#40a070"&gt;3&lt;/span&gt;]);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// Single 128-bit store for C
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; LDST128BITS(c[idx]) &lt;span style="color:#666"&gt;=&lt;/span&gt; pc.f4;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;return&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// Tail path: handle &amp;lt;8 remaining elements
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; i &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;for&lt;/span&gt; (; i &lt;span style="color:#666"&gt;+&lt;/span&gt; &lt;span style="color:#40a070"&gt;1&lt;/span&gt; &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; remaining; i &lt;span style="color:#666"&gt;+=&lt;/span&gt; &lt;span style="color:#40a070"&gt;2&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; half2 ra &lt;span style="color:#666"&gt;=&lt;/span&gt; HALF2(a[idx &lt;span style="color:#666"&gt;+&lt;/span&gt; i]);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; half2 rb &lt;span style="color:#666"&gt;=&lt;/span&gt; HALF2(b[idx &lt;span style="color:#666"&gt;+&lt;/span&gt; i]);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; HALF2(c[idx &lt;span style="color:#666"&gt;+&lt;/span&gt; i]) &lt;span style="color:#666"&gt;=&lt;/span&gt; __hadd2(ra, rb);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (i &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; remaining) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; c[idx &lt;span style="color:#666"&gt;+&lt;/span&gt; i] &lt;span style="color:#666"&gt;=&lt;/span&gt; __hadd(a[idx &lt;span style="color:#666"&gt;+&lt;/span&gt; i], b[idx &lt;span style="color:#666"&gt;+&lt;/span&gt; i]);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="register-the-kernels-and-benchmark"&gt;Register the kernels and benchmark&lt;/h3&gt;
&lt;p&gt;Register the kernel:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-cpp" data-lang="cpp"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#include&lt;/span&gt; &lt;span style="color:#007020"&gt;&amp;#34;elementwise_add.cuh&amp;#34;&lt;/span&gt;&lt;span style="color:#007020"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;TORCH_BINDING_ELEM_ADD(f32, torch&lt;span style="color:#666"&gt;::&lt;/span&gt;kFloat32, &lt;span style="color:#902000"&gt;float&lt;/span&gt;, &lt;span style="color:#40a070"&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;TORCH_BINDING_ELEM_ADD(f32x4, torch&lt;span style="color:#666"&gt;::&lt;/span&gt;kFloat32, &lt;span style="color:#902000"&gt;float&lt;/span&gt;, &lt;span style="color:#40a070"&gt;4&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;TORCH_BINDING_ELEM_ADD(f16x2, torch&lt;span style="color:#666"&gt;::&lt;/span&gt;kFloat16, half, &lt;span style="color:#40a070"&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;TORCH_BINDING_ELEM_ADD(f16x8, torch&lt;span style="color:#666"&gt;::&lt;/span&gt;kFloat16, half, &lt;span style="color:#40a070"&gt;8&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; TORCH_BINDING_COMMON_EXTENSION(elementwise_add_f32)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; TORCH_BINDING_COMMON_EXTENSION(elementwise_add_f32x4)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; TORCH_BINDING_COMMON_EXTENSION(elementwise_add_f16x2)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; TORCH_BINDING_COMMON_EXTENSION(elementwise_add_f16x8)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Compile PyTorch module&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;from&lt;/span&gt; &lt;span style="color:#0e84b5;font-weight:bold"&gt;setuptools&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;import&lt;/span&gt; setup
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;from&lt;/span&gt; &lt;span style="color:#0e84b5;font-weight:bold"&gt;torch.utils.cpp_extension&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;import&lt;/span&gt; BuildExtension, CppExtension
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;source_files &lt;span style="color:#666"&gt;=&lt;/span&gt; [
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#4070a0"&gt;&amp;#34;elementwise_add_basic.cu&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#4070a0"&gt;&amp;#34;elementwise_add_f32x4.cu&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#4070a0"&gt;&amp;#34;elementwise_add_f16x2.cu&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#4070a0"&gt;&amp;#34;elementwise_add_f16x8.cu&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#4070a0"&gt;&amp;#34;elementwise_add_lib.cu&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;setup(
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; name&lt;span style="color:#666"&gt;=&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;#39;elementwise_lib&amp;#39;&lt;/span&gt;, &lt;span style="color:#60a0b0;font-style:italic"&gt;# The name of your module&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ext_modules&lt;span style="color:#666"&gt;=&lt;/span&gt;[
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; CppExtension(
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#4070a0"&gt;&amp;#39;elementwise_lib&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; source_files
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ),
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; ],
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; cmdclass&lt;span style="color:#666"&gt;=&lt;/span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#4070a0"&gt;&amp;#39;build_ext&amp;#39;&lt;/span&gt;: BuildExtension
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Launching in PyTorch:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;import&lt;/span&gt; &lt;span style="color:#0e84b5;font-weight:bold"&gt;time&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;from&lt;/span&gt; &lt;span style="color:#0e84b5;font-weight:bold"&gt;functools&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;import&lt;/span&gt; partial
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;from&lt;/span&gt; &lt;span style="color:#0e84b5;font-weight:bold"&gt;typing&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;import&lt;/span&gt; Optional
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;import&lt;/span&gt; &lt;span style="color:#0e84b5;font-weight:bold"&gt;torch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;import&lt;/span&gt; &lt;span style="color:#0e84b5;font-weight:bold"&gt;os&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;import&lt;/span&gt; &lt;span style="color:#0e84b5;font-weight:bold"&gt;sys&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;workspace &lt;span style="color:#666"&gt;=&lt;/span&gt; os&lt;span style="color:#666"&gt;.&lt;/span&gt;environ[&lt;span style="color:#4070a0"&gt;&amp;#34;__WORKSPACE__&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;# The built torch lib is in the following path&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;lib_dir &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#4070a0"&gt;f&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;#34;&lt;/span&gt;&lt;span style="color:#70a0d0"&gt;{&lt;/span&gt;workspace&lt;span style="color:#70a0d0"&gt;}&lt;/span&gt;&lt;span style="color:#4070a0"&gt;/build/lib.linux-x86_64-cpython-312&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;print&lt;/span&gt;(&lt;span style="color:#4070a0"&gt;f&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;#34;lib: &lt;/span&gt;&lt;span style="color:#70a0d0"&gt;{&lt;/span&gt;lib_dir&lt;span style="color:#70a0d0"&gt;}&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;sys&lt;span style="color:#666"&gt;.&lt;/span&gt;path&lt;span style="color:#666"&gt;.&lt;/span&gt;append(lib_dir)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;import&lt;/span&gt; &lt;span style="color:#0e84b5;font-weight:bold"&gt;elementwise_lib&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;as&lt;/span&gt; &lt;span style="color:#0e84b5;font-weight:bold"&gt;lib&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;torch&lt;span style="color:#666"&gt;.&lt;/span&gt;set_grad_enabled(&lt;span style="color:#007020;font-weight:bold"&gt;False&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;print&lt;/span&gt;(&lt;span style="color:#4070a0"&gt;f&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;#34;Compiling Torch kernel&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;# Load the CUDA kernel as a python module&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;import&lt;/span&gt; &lt;span style="color:#0e84b5;font-weight:bold"&gt;hashlib&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;import&lt;/span&gt; &lt;span style="color:#0e84b5;font-weight:bold"&gt;os&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;def&lt;/span&gt; &lt;span style="color:#06287e"&gt;get_file_hash&lt;/span&gt;(filepath):
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#4070a0"&gt;&amp;#34;&amp;#34;&amp;#34;Get MD5 hash of file content&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;with&lt;/span&gt; &lt;span style="color:#007020"&gt;open&lt;/span&gt;(filepath, &lt;span style="color:#4070a0"&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;) &lt;span style="color:#007020;font-weight:bold"&gt;as&lt;/span&gt; f:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;return&lt;/span&gt; hashlib&lt;span style="color:#666"&gt;.&lt;/span&gt;md5(f&lt;span style="color:#666"&gt;.&lt;/span&gt;read())&lt;span style="color:#666"&gt;.&lt;/span&gt;hexdigest()[:&lt;span style="color:#40a070"&gt;8&lt;/span&gt;] &lt;span style="color:#60a0b0;font-style:italic"&gt;# Use first 8 chars&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;print&lt;/span&gt;(&lt;span style="color:#4070a0"&gt;f&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;#34;running benchmark&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;def&lt;/span&gt; &lt;span style="color:#06287e"&gt;run_benchmark&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; perf_func: callable,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; a: torch&lt;span style="color:#666"&gt;.&lt;/span&gt;Tensor,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; b: torch&lt;span style="color:#666"&gt;.&lt;/span&gt;Tensor,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; tag: &lt;span style="color:#007020"&gt;str&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out: Optional[torch&lt;span style="color:#666"&gt;.&lt;/span&gt;Tensor] &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; warmup: &lt;span style="color:#007020"&gt;int&lt;/span&gt; &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;10&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; iters: &lt;span style="color:#007020"&gt;int&lt;/span&gt; &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;1000&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; show_all: &lt;span style="color:#007020"&gt;bool&lt;/span&gt; &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;False&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; elements_per_block &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;256&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; out &lt;span style="color:#007020;font-weight:bold"&gt;is&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;not&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out&lt;span style="color:#666"&gt;.&lt;/span&gt;fill_(&lt;span style="color:#40a070"&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;# Warmup&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;for&lt;/span&gt; _ &lt;span style="color:#007020;font-weight:bold"&gt;in&lt;/span&gt; &lt;span style="color:#007020"&gt;range&lt;/span&gt;(warmup):
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; perf_func(a, b, out, elements_per_block)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; torch&lt;span style="color:#666"&gt;.&lt;/span&gt;cuda&lt;span style="color:#666"&gt;.&lt;/span&gt;synchronize()
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;# Benchmark&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; start_event &lt;span style="color:#666"&gt;=&lt;/span&gt; torch&lt;span style="color:#666"&gt;.&lt;/span&gt;cuda&lt;span style="color:#666"&gt;.&lt;/span&gt;Event(enable_timing&lt;span style="color:#666"&gt;=&lt;/span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;True&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; end_event &lt;span style="color:#666"&gt;=&lt;/span&gt; torch&lt;span style="color:#666"&gt;.&lt;/span&gt;cuda&lt;span style="color:#666"&gt;.&lt;/span&gt;Event(enable_timing&lt;span style="color:#666"&gt;=&lt;/span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;True&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; start_event&lt;span style="color:#666"&gt;.&lt;/span&gt;record()
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;for&lt;/span&gt; _ &lt;span style="color:#007020;font-weight:bold"&gt;in&lt;/span&gt; &lt;span style="color:#007020"&gt;range&lt;/span&gt;(iters):
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; perf_func(a, b, out, elements_per_block)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; end_event&lt;span style="color:#666"&gt;.&lt;/span&gt;record()
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; torch&lt;span style="color:#666"&gt;.&lt;/span&gt;cuda&lt;span style="color:#666"&gt;.&lt;/span&gt;synchronize()
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; total_time &lt;span style="color:#666"&gt;=&lt;/span&gt; start_event&lt;span style="color:#666"&gt;.&lt;/span&gt;elapsed_time(end_event) &lt;span style="color:#60a0b0;font-style:italic"&gt;# ms&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; mean_time &lt;span style="color:#666"&gt;=&lt;/span&gt; total_time &lt;span style="color:#666"&gt;/&lt;/span&gt; iters
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_info &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#4070a0"&gt;f&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;#34;out_&lt;/span&gt;&lt;span style="color:#70a0d0"&gt;{&lt;/span&gt;tag&lt;span style="color:#70a0d0"&gt;}&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_val &lt;span style="color:#666"&gt;=&lt;/span&gt; out&lt;span style="color:#666"&gt;.&lt;/span&gt;flatten()&lt;span style="color:#666"&gt;.&lt;/span&gt;detach()&lt;span style="color:#666"&gt;.&lt;/span&gt;cpu()&lt;span style="color:#666"&gt;.&lt;/span&gt;numpy()&lt;span style="color:#666"&gt;.&lt;/span&gt;tolist()[:&lt;span style="color:#40a070"&gt;2&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_val &lt;span style="color:#666"&gt;=&lt;/span&gt; [&lt;span style="color:#007020"&gt;round&lt;/span&gt;(v, &lt;span style="color:#40a070"&gt;8&lt;/span&gt;) &lt;span style="color:#007020;font-weight:bold"&gt;for&lt;/span&gt; v &lt;span style="color:#007020;font-weight:bold"&gt;in&lt;/span&gt; out_val]
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020"&gt;print&lt;/span&gt;(&lt;span style="color:#4070a0"&gt;f&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;#34;&lt;/span&gt;&lt;span style="color:#70a0d0"&gt;{&lt;/span&gt;out_info&lt;span style="color:#70a0d0"&gt;:&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;gt;18&lt;/span&gt;&lt;span style="color:#70a0d0"&gt;}&lt;/span&gt;&lt;span style="color:#4070a0"&gt;: &lt;/span&gt;&lt;span style="color:#70a0d0"&gt;{&lt;/span&gt;out_val&lt;span style="color:#70a0d0"&gt;}&lt;/span&gt;&lt;span style="color:#4070a0"&gt;, time:&lt;/span&gt;&lt;span style="color:#70a0d0"&gt;{&lt;/span&gt;mean_time&lt;span style="color:#70a0d0"&gt;:&lt;/span&gt;&lt;span style="color:#4070a0"&gt;.8f&lt;/span&gt;&lt;span style="color:#70a0d0"&gt;}&lt;/span&gt;&lt;span style="color:#4070a0"&gt;ms&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; show_all:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020"&gt;print&lt;/span&gt;(out)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;return&lt;/span&gt; out, mean_time
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run the benchmark:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;shapes &lt;span style="color:#666"&gt;=&lt;/span&gt; [
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; (&lt;span style="color:#40a070"&gt;2096&lt;/span&gt;, &lt;span style="color:#40a070"&gt;4096&lt;/span&gt;), (&lt;span style="color:#40a070"&gt;2048&lt;/span&gt;, &lt;span style="color:#40a070"&gt;2048&lt;/span&gt;), (&lt;span style="color:#40a070"&gt;2048&lt;/span&gt;, &lt;span style="color:#40a070"&gt;1024&lt;/span&gt;), (&lt;span style="color:#40a070"&gt;1024&lt;/span&gt;, &lt;span style="color:#40a070"&gt;1024&lt;/span&gt;), (&lt;span style="color:#40a070"&gt;512&lt;/span&gt;, &lt;span style="color:#40a070"&gt;512&lt;/span&gt;), (&lt;span style="color:#40a070"&gt;256&lt;/span&gt;, &lt;span style="color:#40a070"&gt;256&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;for&lt;/span&gt; shape &lt;span style="color:#007020;font-weight:bold"&gt;in&lt;/span&gt; shapes:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020"&gt;print&lt;/span&gt;(&lt;span style="color:#4070a0"&gt;f&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;#34;Running benchmark for shape: &lt;/span&gt;&lt;span style="color:#70a0d0"&gt;{&lt;/span&gt;shape&lt;span style="color:#70a0d0"&gt;}&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; A &lt;span style="color:#666"&gt;=&lt;/span&gt; torch&lt;span style="color:#666"&gt;.&lt;/span&gt;randn(&lt;span style="color:#666"&gt;*&lt;/span&gt;shape, dtype&lt;span style="color:#666"&gt;=&lt;/span&gt;torch&lt;span style="color:#666"&gt;.&lt;/span&gt;float32, device&lt;span style="color:#666"&gt;=&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;#34;cuda&amp;#34;&lt;/span&gt;)&lt;span style="color:#666"&gt;.&lt;/span&gt;contiguous()
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; B &lt;span style="color:#666"&gt;=&lt;/span&gt; torch&lt;span style="color:#666"&gt;.&lt;/span&gt;randn(&lt;span style="color:#666"&gt;*&lt;/span&gt;shape, &lt;span style="color:#40a070"&gt;1024&lt;/span&gt;, dtype&lt;span style="color:#666"&gt;=&lt;/span&gt;torch&lt;span style="color:#666"&gt;.&lt;/span&gt;float32, device&lt;span style="color:#666"&gt;=&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;#34;cuda&amp;#34;&lt;/span&gt;)&lt;span style="color:#666"&gt;.&lt;/span&gt;contiguous()
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; C &lt;span style="color:#666"&gt;=&lt;/span&gt; torch&lt;span style="color:#666"&gt;.&lt;/span&gt;zeros_like(A)&lt;span style="color:#666"&gt;.&lt;/span&gt;contiguous()
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;# Create fp16 tensors for fp16 kernels&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; A_fp16 &lt;span style="color:#666"&gt;=&lt;/span&gt; A&lt;span style="color:#666"&gt;.&lt;/span&gt;half()&lt;span style="color:#666"&gt;.&lt;/span&gt;contiguous()
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; B_fp16 &lt;span style="color:#666"&gt;=&lt;/span&gt; B&lt;span style="color:#666"&gt;.&lt;/span&gt;half()&lt;span style="color:#666"&gt;.&lt;/span&gt;contiguous()
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; C_fp16 &lt;span style="color:#666"&gt;=&lt;/span&gt; torch&lt;span style="color:#666"&gt;.&lt;/span&gt;zeros_like(A_fp16)&lt;span style="color:#666"&gt;.&lt;/span&gt;contiguous()
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; elements_per_block &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;256&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020"&gt;print&lt;/span&gt;(&lt;span style="color:#4070a0"&gt;f&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;#34;elements_per_block: &lt;/span&gt;&lt;span style="color:#70a0d0"&gt;{&lt;/span&gt;elements_per_block&lt;span style="color:#70a0d0"&gt;}&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;# Increase elements_per_block to make sure that each kernel has same threads_per_block&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; run_benchmark(lib&lt;span style="color:#666"&gt;.&lt;/span&gt;elementwise_add_f32, A, B, &lt;span style="color:#4070a0"&gt;&amp;#34;basic&amp;#34;&lt;/span&gt;, C, elements_per_block)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; run_benchmark(lib&lt;span style="color:#666"&gt;.&lt;/span&gt;elementwise_add_f32x4, A, B, &lt;span style="color:#4070a0"&gt;&amp;#34;f32x4&amp;#34;&lt;/span&gt;, C, elements_per_block &lt;span style="color:#666"&gt;*&lt;/span&gt; &lt;span style="color:#40a070"&gt;4&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; run_benchmark(lib&lt;span style="color:#666"&gt;.&lt;/span&gt;elementwise_add_f16x2, A_fp16, B_fp16, &lt;span style="color:#4070a0"&gt;&amp;#34;f16x2&amp;#34;&lt;/span&gt;, C_fp16, elements_per_block &lt;span style="color:#666"&gt;*&lt;/span&gt; &lt;span style="color:#40a070"&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; run_benchmark(lib&lt;span style="color:#666"&gt;.&lt;/span&gt;elementwise_add_f16x8, A_fp16, B_fp16, &lt;span style="color:#4070a0"&gt;&amp;#34;f16x8&amp;#34;&lt;/span&gt;, C_fp16, elements_per_block &lt;span style="color:#666"&gt;*&lt;/span&gt; &lt;span style="color:#40a070"&gt;8&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020"&gt;print&lt;/span&gt;(&lt;span style="color:#4070a0"&gt;f&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;#34;--&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Results:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; lib: /workspace/project/superjomn.github.io/content-org/_build/build/lib.linux-x86_64-cpython-312
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Compiling Torch kernel
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; running benchmark
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Running benchmark for shape: (2096, 4096)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; elements_per_block: 256
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_basic: [0.62899578, -3.16506243], time:0.04013546ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_f32x4: [0.62899578, -3.16506243], time:0.03716669ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_f16x2: [0.62890625, -3.1640625], time:0.02376186ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_f16x8: [0.62890625, -3.1640625], time:0.02382634ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; --
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Running benchmark for shape: (2048, 2048)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; elements_per_block: 256
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_basic: [0.98603237, -2.21596098], time:0.02142691ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_f32x4: [0.98603237, -2.21596098], time:0.01876467ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_f16x2: [0.98632812, -2.21484375], time:0.01224410ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_f16x8: [0.98632812, -2.21484375], time:0.01231584ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; --
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Running benchmark for shape: (2048, 1024)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; elements_per_block: 256
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_basic: [-1.68364513, 0.07630849], time:0.00754973ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_f32x4: [-1.68364513, 0.07630849], time:0.00768909ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_f16x2: [-1.68359375, 0.07666016], time:0.00720714ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_f16x8: [-1.68359375, 0.07666016], time:0.00725242ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; --
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Running benchmark for shape: (1024, 1024)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; elements_per_block: 256
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_basic: [0.41730967, -2.56410193], time:0.00473962ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_f32x4: [0.41730967, -2.56410193], time:0.00490102ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_f16x2: [0.41723633, -2.56445312], time:0.00471936ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_f16x8: [0.41723633, -2.56445312], time:0.00484266ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; --
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Running benchmark for shape: (512, 512)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; elements_per_block: 256
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_basic: [-0.84098238, -0.51086581], time:0.00448192ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_f32x4: [-0.84098238, -0.51086581], time:0.00438221ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_f16x2: [-0.84130859, -0.51074219], time:0.00443571ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_f16x8: [-0.84130859, -0.51074219], time:0.00444208ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; --
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Running benchmark for shape: (256, 256)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; elements_per_block: 256
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_basic: [2.76621795, 1.71955645], time:0.00438445ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_f32x4: [2.76621795, 1.71955645], time:0.00440973ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_f16x2: [2.765625, 1.71972656], time:0.00444272ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; out_f16x8: [2.765625, 1.71972656], time:0.00445354ms
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; --
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description></item><item><title>Reduce kernel in CUDA</title><link>/posts/reduce-cuda/</link><pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate><guid>/posts/reduce-cuda/</guid><description>&lt;h2 id="question-definition"&gt;Question definition&lt;/h2&gt;
&lt;p&gt;Given an array of \(n\) integers, the goal is to compute the sum of all elements within the array.&lt;/p&gt;
&lt;h2 id="solutions"&gt;Solutions&lt;/h2&gt;
&lt;p&gt;The implementations for all kernel versions can be found at &lt;a href="https://github.com/Superjomn/cuda-from-scratch/blob/dev/2-reduce.cu"&gt;2-reduce.cu on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="naive-version-with-atomicadd"&gt;Naive Version with &lt;code&gt;atomicAdd&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The simplest approach involves utilizing each thread to perform an &lt;code&gt;atomicAdd&lt;/code&gt; operation on the output variable. Here&amp;rsquo;s how the kernel is defined:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-C++" data-lang="C++"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;__global__ &lt;span style="color:#902000"&gt;void&lt;/span&gt; &lt;span style="color:#06287e"&gt;reduce_naive_atomic&lt;/span&gt;(&lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; g_idata, &lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; g_odata, &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; n)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; idx &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; gridSize &lt;span style="color:#666"&gt;=&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;*&lt;/span&gt; gridDim.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; sum &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;for&lt;/span&gt; (&lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; i &lt;span style="color:#666"&gt;=&lt;/span&gt; idx; i &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; n; i &lt;span style="color:#666"&gt;+=&lt;/span&gt; gridSize)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; sum &lt;span style="color:#666"&gt;+=&lt;/span&gt; g_idata[i];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; atomicAdd(g_odata, sum);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And the kernel launcher is straightforward, invoking the kernel a single time:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-C++" data-lang="C++"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#902000"&gt;int&lt;/span&gt; &lt;span style="color:#06287e"&gt;launch_reduce&lt;/span&gt;(&lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; g_idata, &lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; g_odata, &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; n, &lt;span style="color:#902000"&gt;int&lt;/span&gt; block_size, kernel_fn kernel, cudaStream_t stream)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; idata &lt;span style="color:#666"&gt;=&lt;/span&gt; g_idata;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; odata &lt;span style="color:#666"&gt;=&lt;/span&gt; g_odata;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;uint32_t&lt;/span&gt; num_warps &lt;span style="color:#666"&gt;=&lt;/span&gt; block_size &lt;span style="color:#666"&gt;/&lt;/span&gt; &lt;span style="color:#40a070"&gt;32&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; smem_size &lt;span style="color:#666"&gt;=&lt;/span&gt; num_warps &lt;span style="color:#666"&gt;*&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;sizeof&lt;/span&gt;(&lt;span style="color:#902000"&gt;int&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; num_blocks &lt;span style="color:#666"&gt;=&lt;/span&gt; ceil(n, block_size);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// Launch the kernel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; kernel&lt;span style="color:#666"&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;num_blocks, block_size, smem_size, stream&lt;span style="color:#666"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;(idata, odata, n);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (&lt;span style="color:#666"&gt;!&lt;/span&gt;FLAGS_profile)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; cudaStreamSynchronize(stream);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// Copy the final result back to the host
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; h_out;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; NVCHECK(cudaMemcpyAsync(&lt;span style="color:#666"&gt;&amp;amp;&lt;/span&gt;h_out, odata, &lt;span style="color:#007020;font-weight:bold"&gt;sizeof&lt;/span&gt;(&lt;span style="color:#902000"&gt;int&lt;/span&gt;), cudaMemcpyDeviceToHost, stream));
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;return&lt;/span&gt; h_out;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;When tested on a GTX 4080, this method achieved a throughput of approximately 82GB/s.&lt;/p&gt;
&lt;h3 id="tiled-reduction-with-shared-memory"&gt;Tiled Reduction with Shared Memory&lt;/h3&gt;
&lt;p&gt;A classical approach involves leveraging a thread block to perform local reductions on a tile within shared memory.
This method encompasses several kernel versions, each with different optimizations.&lt;/p&gt;
&lt;h4 id="basic-version"&gt;Basic version&lt;/h4&gt;
&lt;p&gt;The initial implementation is as below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A tile of data is collaboratively loaded into shared memory.&lt;/li&gt;
&lt;li&gt;A partial reduction on this data tile is executed within a thread block, get the sum of the tile.&lt;/li&gt;
&lt;li&gt;The sum is then written to a designated spot in the global memory&amp;rsquo;s output slot. It&amp;rsquo;s important to note that this kernel requires a temporary buffer for writing partial results from each thread block.&lt;/li&gt;
&lt;li&gt;The process repeats with the size \(n\) reduced to \(\frac{n}{blockSize}\), continuing until \(n=1\).&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-C++" data-lang="C++"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;__global__ &lt;span style="color:#902000"&gt;void&lt;/span&gt; &lt;span style="color:#06287e"&gt;reduce_smem_naive&lt;/span&gt;(&lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; g_idata, &lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; g_odata, &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; n)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;extern&lt;/span&gt; __shared__ &lt;span style="color:#902000"&gt;int&lt;/span&gt; sdata[];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; tid &lt;span style="color:#666"&gt;=&lt;/span&gt; threadIdx.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; i &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// Read a block of data into shared memory collectively
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; sdata[tid] &lt;span style="color:#666"&gt;=&lt;/span&gt; (i &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; n) &lt;span style="color:#666"&gt;?&lt;/span&gt; g_idata[i] &lt;span style="color:#666"&gt;:&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; __syncthreads();
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;for&lt;/span&gt; (&lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; stride &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;1&lt;/span&gt;; stride &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; blockDim.x; stride &lt;span style="color:#666"&gt;*=&lt;/span&gt; &lt;span style="color:#40a070"&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// ISSUE: divergent warps
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (tid &lt;span style="color:#666"&gt;%&lt;/span&gt; (&lt;span style="color:#40a070"&gt;2&lt;/span&gt; &lt;span style="color:#666"&gt;*&lt;/span&gt; stride) &lt;span style="color:#666"&gt;==&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; sdata[tid] &lt;span style="color:#666"&gt;+=&lt;/span&gt; sdata[tid &lt;span style="color:#666"&gt;+&lt;/span&gt; stride];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; __syncthreads(); &lt;span style="color:#60a0b0;font-style:italic"&gt;// need to sync per level
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// Write the result for this block to global memory
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (tid &lt;span style="color:#666"&gt;==&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; g_odata[blockIdx.x] &lt;span style="color:#666"&gt;=&lt;/span&gt; sdata[&lt;span style="color:#40a070"&gt;0&lt;/span&gt;];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Launching this kernel multiple times involves a slightly more complex launcher:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-C++" data-lang="C++"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#902000"&gt;int&lt;/span&gt; &lt;span style="color:#06287e"&gt;launch_reduce&lt;/span&gt;(&lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; g_idata, &lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; g_odata, &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; n, &lt;span style="color:#902000"&gt;int&lt;/span&gt; block_size, kernel_fn kernel, cudaStream_t stream,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;uint32_t&lt;/span&gt; num_blocks, &lt;span style="color:#902000"&gt;uint32_t&lt;/span&gt; smem_size)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; idata &lt;span style="color:#666"&gt;=&lt;/span&gt; g_idata;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; odata &lt;span style="color:#666"&gt;=&lt;/span&gt; g_odata;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (smem_size &lt;span style="color:#666"&gt;==&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; smem_size &lt;span style="color:#666"&gt;=&lt;/span&gt; block_size &lt;span style="color:#666"&gt;*&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;sizeof&lt;/span&gt;(&lt;span style="color:#902000"&gt;int&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// Calculate the number of blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; num_blocks &lt;span style="color:#666"&gt;=&lt;/span&gt; (num_blocks &lt;span style="color:#666"&gt;&amp;gt;&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;) &lt;span style="color:#666"&gt;?&lt;/span&gt; &lt;span style="color:#002070;font-weight:bold"&gt;num_blocks&lt;/span&gt; : (n &lt;span style="color:#666"&gt;+&lt;/span&gt; block_size &lt;span style="color:#666"&gt;-&lt;/span&gt; &lt;span style="color:#40a070"&gt;1&lt;/span&gt;) &lt;span style="color:#666"&gt;/&lt;/span&gt; block_size;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (&lt;span style="color:#666"&gt;!&lt;/span&gt;FLAGS_profile)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; printf(&lt;span style="color:#4070a0"&gt;&amp;#34;- launching: num_blocks: %d, block_size:%d, n:%d&lt;/span&gt;&lt;span style="color:#4070a0;font-weight:bold"&gt;\n&lt;/span&gt;&lt;span style="color:#4070a0"&gt;&amp;#34;&lt;/span&gt;, num_blocks, block_size, n);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; level &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// Launch the kernel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; kernel&lt;span style="color:#666"&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;num_blocks, block_size, smem_size, stream&lt;span style="color:#666"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;(idata, odata, n);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (&lt;span style="color:#666"&gt;!&lt;/span&gt;FLAGS_profile)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; cudaStreamSynchronize(stream);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; level&lt;span style="color:#666"&gt;++&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// Recursively reduce the partial sums
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;while&lt;/span&gt; (num_blocks &lt;span style="color:#666"&gt;&amp;gt;&lt;/span&gt; &lt;span style="color:#40a070"&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; std&lt;span style="color:#666"&gt;::&lt;/span&gt;swap(idata, odata);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; n &lt;span style="color:#666"&gt;=&lt;/span&gt; num_blocks;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; num_blocks &lt;span style="color:#666"&gt;=&lt;/span&gt; (n &lt;span style="color:#666"&gt;+&lt;/span&gt; block_size &lt;span style="color:#666"&gt;-&lt;/span&gt; &lt;span style="color:#40a070"&gt;1&lt;/span&gt;) &lt;span style="color:#666"&gt;/&lt;/span&gt; block_size;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; kernel&lt;span style="color:#666"&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;num_blocks, block_size, smem_size, stream&lt;span style="color:#666"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;(idata, odata, n);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (&lt;span style="color:#666"&gt;!&lt;/span&gt;FLAGS_profile)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; cudaStreamSynchronize(stream);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// Copy the final result back to the host
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; h_out;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; NVCHECK(cudaMemcpyAsync(&lt;span style="color:#666"&gt;&amp;amp;&lt;/span&gt;h_out, odata, &lt;span style="color:#007020;font-weight:bold"&gt;sizeof&lt;/span&gt;(&lt;span style="color:#902000"&gt;int&lt;/span&gt;), cudaMemcpyDeviceToHost, stream));
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;return&lt;/span&gt; h_out;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;All tiled reduction kenrels utilize the aforementioned launcher, achieving a throughput of 54GB/s. This is less efficient compared to the atomic naive version, which reaches 82GB/s.&lt;/p&gt;
&lt;h4 id="avoid-thread-divergence"&gt;Avoid thread divergence&lt;/h4&gt;
&lt;p&gt;The basic version encounters significant thread divergence, particularly noticeable at &lt;code&gt;if (tid % (2 * stride) == 0)&lt;/code&gt;.
Here is an optimized variant:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-C++" data-lang="C++"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;__global__ &lt;span style="color:#902000"&gt;void&lt;/span&gt; &lt;span style="color:#06287e"&gt;reduce_smem_1_avoid_divergent_warps&lt;/span&gt;(&lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; g_idata, &lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; g_odata, &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; n)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;extern&lt;/span&gt; __shared__ &lt;span style="color:#902000"&gt;int&lt;/span&gt; sdata[];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; tid &lt;span style="color:#666"&gt;=&lt;/span&gt; threadIdx.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; i &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; sdata[tid] &lt;span style="color:#666"&gt;=&lt;/span&gt; (i &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; n) &lt;span style="color:#666"&gt;?&lt;/span&gt; g_idata[i] &lt;span style="color:#666"&gt;:&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; __syncthreads();
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;for&lt;/span&gt; (&lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; stride &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;1&lt;/span&gt;; stride &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; blockDim.x; stride &lt;span style="color:#666"&gt;*=&lt;/span&gt; &lt;span style="color:#40a070"&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; index &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;2&lt;/span&gt; &lt;span style="color:#666"&gt;*&lt;/span&gt; stride &lt;span style="color:#666"&gt;*&lt;/span&gt; tid;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (index &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; blockDim.x)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// Issue: bank conflict
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; sdata[index] &lt;span style="color:#666"&gt;+=&lt;/span&gt; sdata[index &lt;span style="color:#666"&gt;+&lt;/span&gt; stride];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; __syncthreads();
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (tid &lt;span style="color:#666"&gt;==&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; g_odata[blockIdx.x] &lt;span style="color:#666"&gt;=&lt;/span&gt; sdata[&lt;span style="color:#40a070"&gt;0&lt;/span&gt;];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The optimization yields a 70GB/s throughput, marking a 29% improvement over the basic version.&lt;/p&gt;
&lt;h4 id="read-two-elements-one-time"&gt;Read two elements one time&lt;/h4&gt;
&lt;p&gt;The preceding version&amp;rsquo;s DRAM throughput was only 20.63%, likely due to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Insufficient grid size for small inputs, leading to underutilized thread resources.&lt;/li&gt;
&lt;li&gt;Each thread reading a single element at a time, given the fixed number of resident thread blocks per SM for a specific kernel, results in a limited number of load instructions issued.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To enhance DRAM throughput, especially for smaller grid sizes, threads can be configured to read more than one element at a time.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-C++" data-lang="C++"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;__global__ &lt;span style="color:#902000"&gt;void&lt;/span&gt; &lt;span style="color:#06287e"&gt;reduce_smem_3_read_two&lt;/span&gt;(&lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; g_idata, &lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; g_odata, &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; n)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;extern&lt;/span&gt; __shared__ &lt;span style="color:#902000"&gt;int&lt;/span&gt; sdata[];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; tid &lt;span style="color:#666"&gt;=&lt;/span&gt; threadIdx.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; i &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; (blockDim.x &lt;span style="color:#666"&gt;*&lt;/span&gt; &lt;span style="color:#40a070"&gt;2&lt;/span&gt;) &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#define GET_ELEM(__idx) ((__idx) &amp;lt; n ? g_idata[(__idx)] : 0)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; sdata[tid] &lt;span style="color:#666"&gt;=&lt;/span&gt; GET_ELEM(i) &lt;span style="color:#666"&gt;+&lt;/span&gt; GET_ELEM(i &lt;span style="color:#666"&gt;+&lt;/span&gt; blockDim.x);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; __syncthreads();
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;for&lt;/span&gt; (&lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; stride &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;1&lt;/span&gt;; stride &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; blockDim.x; stride &lt;span style="color:#666"&gt;*=&lt;/span&gt; &lt;span style="color:#40a070"&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; index &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;2&lt;/span&gt; &lt;span style="color:#666"&gt;*&lt;/span&gt; stride &lt;span style="color:#666"&gt;*&lt;/span&gt; tid;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (index &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; blockDim.x)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// Issue: bank conflict
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; sdata[index] &lt;span style="color:#666"&gt;+=&lt;/span&gt; sdata[index &lt;span style="color:#666"&gt;+&lt;/span&gt; stride];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; __syncthreads();
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (tid &lt;span style="color:#666"&gt;==&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; g_odata[blockIdx.x] &lt;span style="color:#666"&gt;=&lt;/span&gt; sdata[&lt;span style="color:#40a070"&gt;0&lt;/span&gt;];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This approach improves the DRAM Throughput to 33.78%, a significant 63.72% increase over the previous method.
The overall throughput reaches 96.51GB/s, demonstrating 37.87% enhancement from the 70GB/s achieved earlier.&lt;/p&gt;
&lt;h3 id="tiled-reduction-with-warp-shuffle"&gt;Tiled Reduction with Warp Shuffle&lt;/h3&gt;
&lt;p&gt;Modern GPUs facilitate direct data exchange within a warp, bypassing the need for shared memory.&lt;/p&gt;
&lt;p&gt;The function below demonstrates how to conduct a reduction within a single warp using the warp shuffle instruction, as highlighted in the book &amp;lt;Professional CUDA C Programming&amp;gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-C++" data-lang="C++"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;// using warp shuffle instruction
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;// From book &amp;lt;Professional CUDA C Programming&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt;__inline__ __device__ &lt;span style="color:#902000"&gt;int&lt;/span&gt; &lt;span style="color:#06287e"&gt;warpReduce&lt;/span&gt;(&lt;span style="color:#902000"&gt;int&lt;/span&gt; mySum)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; mySum &lt;span style="color:#666"&gt;+=&lt;/span&gt; __shfl_xor(mySum, &lt;span style="color:#40a070"&gt;16&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; mySum &lt;span style="color:#666"&gt;+=&lt;/span&gt; __shfl_xor(mySum, &lt;span style="color:#40a070"&gt;8&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; mySum &lt;span style="color:#666"&gt;+=&lt;/span&gt; __shfl_xor(mySum, &lt;span style="color:#40a070"&gt;4&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; mySum &lt;span style="color:#666"&gt;+=&lt;/span&gt; __shfl_xor(mySum, &lt;span style="color:#40a070"&gt;2&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; mySum &lt;span style="color:#666"&gt;+=&lt;/span&gt; __shfl_xor(mySum, &lt;span style="color:#40a070"&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;return&lt;/span&gt; mySum;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Utilizing shared memory to store the sum computed by each warp and subsequently reducing these sums as previously described enables the calculation of a thread block&amp;rsquo;s total sum.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-C++" data-lang="C++"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;__global__ &lt;span style="color:#902000"&gt;void&lt;/span&gt; &lt;span style="color:#06287e"&gt;reduce_warp_shlf&lt;/span&gt;(&lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; g_idata, &lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; g_odata, &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; n)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// Helps to share data between warps
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// size should be (blockDim.x / warpSize)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;extern&lt;/span&gt; __shared__ &lt;span style="color:#902000"&gt;int&lt;/span&gt; sdata[];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; idx &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// Necessary to make sure shfl instruction is not used with uninitialized data
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; mySum &lt;span style="color:#666"&gt;=&lt;/span&gt; idx &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; n &lt;span style="color:#666"&gt;?&lt;/span&gt; g_idata[idx] &lt;span style="color:#666"&gt;:&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; lane &lt;span style="color:#666"&gt;=&lt;/span&gt; threadIdx.x &lt;span style="color:#666"&gt;%&lt;/span&gt; warpSize;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; warp &lt;span style="color:#666"&gt;=&lt;/span&gt; threadIdx.x &lt;span style="color:#666"&gt;/&lt;/span&gt; warpSize;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; mySum &lt;span style="color:#666"&gt;=&lt;/span&gt; warpReduce(mySum);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (lane &lt;span style="color:#666"&gt;==&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; sdata[warp] &lt;span style="color:#666"&gt;=&lt;/span&gt; mySum;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; __syncthreads();
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// last warp reduce
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; mySum &lt;span style="color:#666"&gt;=&lt;/span&gt; (threadIdx.x &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;/&lt;/span&gt; warpSize) &lt;span style="color:#666"&gt;?&lt;/span&gt; sdata[lane] &lt;span style="color:#666"&gt;:&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (warp &lt;span style="color:#666"&gt;==&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; mySum &lt;span style="color:#666"&gt;=&lt;/span&gt; warpReduce(mySum);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (threadIdx.x &lt;span style="color:#666"&gt;==&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; g_odata[blockIdx.x] &lt;span style="color:#666"&gt;=&lt;/span&gt; mySum;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Despite reading only a single element per thread, this kernel can achieve a throughput of 96GB/s, outperforming the shared memory version&amp;rsquo;s 70GB/s.
Furthermore, the kernel can be modified to read \(NT\) elements at a time for enhanced efficiency:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-C++" data-lang="C++"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;template&lt;/span&gt; &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt;&lt;span style="color:#902000"&gt;int&lt;/span&gt; NT&lt;span style="color:#666"&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;__global__ &lt;span style="color:#902000"&gt;void&lt;/span&gt; reduce_warp_shlf_read_N(&lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; g_idata, &lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; g_odata, &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; n)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// Helps to share data between warps
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// size should be (blockDim.x / warpSize)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;extern&lt;/span&gt; __shared__ &lt;span style="color:#902000"&gt;int&lt;/span&gt; sdata[];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; blockSize &lt;span style="color:#666"&gt;=&lt;/span&gt; NT &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; idx &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; blockSize &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;// Necessary to make sure shfl instruction is not used with uninitialized data
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt;&lt;span style="color:#007020"&gt;#define GET_ELEM(__idx) ((__idx) &amp;lt; n ? g_idata[(__idx)] : 0)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; mySum &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#pragma unroll
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;for&lt;/span&gt; (&lt;span style="color:#902000"&gt;int&lt;/span&gt; i &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;; i &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; NT; i&lt;span style="color:#666"&gt;++&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; mySum &lt;span style="color:#666"&gt;+=&lt;/span&gt; GET_ELEM(idx &lt;span style="color:#666"&gt;+&lt;/span&gt; i &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; lane &lt;span style="color:#666"&gt;=&lt;/span&gt; threadIdx.x &lt;span style="color:#666"&gt;%&lt;/span&gt; warpSize;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; warp &lt;span style="color:#666"&gt;=&lt;/span&gt; threadIdx.x &lt;span style="color:#666"&gt;/&lt;/span&gt; warpSize;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; mySum &lt;span style="color:#666"&gt;=&lt;/span&gt; warpReduce(mySum);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (lane &lt;span style="color:#666"&gt;==&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; sdata[warp] &lt;span style="color:#666"&gt;=&lt;/span&gt; mySum;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; __syncthreads();
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// last warp reduce
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; mySum &lt;span style="color:#666"&gt;=&lt;/span&gt; (threadIdx.x &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;/&lt;/span&gt; warpSize) &lt;span style="color:#666"&gt;?&lt;/span&gt; sdata[lane] &lt;span style="color:#666"&gt;:&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (warp &lt;span style="color:#666"&gt;==&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; mySum &lt;span style="color:#666"&gt;=&lt;/span&gt; warpReduce(mySum);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (threadIdx.x &lt;span style="color:#666"&gt;==&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; g_odata[blockIdx.x] &lt;span style="color:#666"&gt;=&lt;/span&gt; mySum;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Performance varies with different \(N\) values, as summarized below:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;NT&lt;/th&gt;
&lt;th&gt;throughput (GB/s)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;96.3187&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;96.2341&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;96.8153&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;107.226&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="warp-shuffle-combined-with-atomic-operations"&gt;Warp Shuffle Combined with Atomic Operations&lt;/h3&gt;
&lt;p&gt;Compared to tiled reduction solutions, utilizing &lt;code&gt;atomicAdd&lt;/code&gt; eliminates the need for a temporary buffer and requires only a single kernel launch.
This segment explores combining warp shuffle and atomic operations for efficient reduction.&lt;/p&gt;
&lt;p&gt;The kernel template below demonstrates this approach, utilizing warp shuffle instructions to enhance the warp reduction performance, and leveraging atomic operations to write directly to the output slot without the need for temporary buffer and multiple kernel launches.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-C++" data-lang="C++"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;template&lt;/span&gt; &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt;&lt;span style="color:#902000"&gt;int&lt;/span&gt; NT&lt;span style="color:#666"&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;__global__ &lt;span style="color:#902000"&gt;void&lt;/span&gt; reduce_warp_shlf_read_N_atomic(&lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; g_idata, &lt;span style="color:#902000"&gt;int&lt;/span&gt;&lt;span style="color:#666"&gt;*&lt;/span&gt; g_odata, &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; n)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// Helps to share data between warps
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// size should be (blockDim.x / warpSize)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;extern&lt;/span&gt; __shared__ &lt;span style="color:#902000"&gt;int&lt;/span&gt; sdata[];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; blockSize &lt;span style="color:#666"&gt;=&lt;/span&gt; NT &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;unsigned&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; idx &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; blockSize &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;// Necessary to make sure shfl instruction is not used with uninitialized data
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;// This only needs one turn of launch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt;&lt;span style="color:#007020"&gt;#define GET_ELEM(__idx) ((__idx) &amp;lt; n ? g_idata[(__idx)] : 0)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; mySum &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;#pragma unroll
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020"&gt;&lt;/span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;for&lt;/span&gt; (&lt;span style="color:#902000"&gt;int&lt;/span&gt; i &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;; i &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; NT; i&lt;span style="color:#666"&gt;++&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; mySum &lt;span style="color:#666"&gt;+=&lt;/span&gt; GET_ELEM(idx &lt;span style="color:#666"&gt;+&lt;/span&gt; i &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; lane &lt;span style="color:#666"&gt;=&lt;/span&gt; threadIdx.x &lt;span style="color:#666"&gt;%&lt;/span&gt; warpSize;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; warp &lt;span style="color:#666"&gt;=&lt;/span&gt; threadIdx.x &lt;span style="color:#666"&gt;/&lt;/span&gt; warpSize;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; mySum &lt;span style="color:#666"&gt;=&lt;/span&gt; warpReduce(mySum);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (lane &lt;span style="color:#666"&gt;==&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; sdata[warp] &lt;span style="color:#666"&gt;=&lt;/span&gt; mySum;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; __syncthreads();
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// last warp reduce
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; mySum &lt;span style="color:#666"&gt;=&lt;/span&gt; (threadIdx.x &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;/&lt;/span&gt; warpSize) &lt;span style="color:#666"&gt;?&lt;/span&gt; sdata[lane] &lt;span style="color:#666"&gt;:&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (warp &lt;span style="color:#666"&gt;==&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; mySum &lt;span style="color:#666"&gt;=&lt;/span&gt; warpReduce(mySum);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (threadIdx.x &lt;span style="color:#666"&gt;==&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; atomicAdd(g_odata, mySum);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Remarkably, this kernel achieves a throughput of 121.777 GB/s under the same conditions.&lt;/p&gt;
&lt;h2 id="benchmark"&gt;Benchmark&lt;/h2&gt;
&lt;p&gt;The benchmark results illustrate the performance of different CUDA optimization strategies under varying conditions.&lt;/p&gt;
&lt;figure&gt;&lt;img src="/ox-hugo/2024-04-06_16-47-39_screenshot.png"&gt;
&lt;/figure&gt;
&lt;p&gt;Note that the optimal kernel configuration may vary depending on the size of the input data(\(n\)).&lt;/p&gt;
&lt;h2 id="reference"&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf"&gt;Optimizing Parallel Reduction in CUDA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developer.nvidia.com/blog/faster-parallel-reductions-kepler/"&gt;Faster Parallel Reductions on Kepler | NVIDIA Technical Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Get GPU Properties</title><link>/posts/gpu-get-props/</link><pubDate>Mon, 11 Mar 2024 00:00:00 +0000</pubDate><guid>/posts/gpu-get-props/</guid><description>&lt;p&gt;In `cuda_runtime.h`, there are several APIs for retrieving properties for the installed GPU.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1gb22e8256592b836df9a9cc36c9db7151"&gt;cudaDeviceGetAttribute(int* value, cudaDeviceAttr attr, int device)&lt;/a&gt;: a C api&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g1bf9d625a931d657e08db2b4391170f0"&gt;cudaGetDeviceProperties ( cudaDeviceProp* prop, int device ) &lt;/a&gt;: a C++ api&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://github.com/Superjomn/cuda-from-scratch/blob/dev/dump-gpu-props.cpp"&gt;Here&lt;/a&gt; is the code of the example.&lt;/p&gt;
&lt;p&gt;On a Nvidia GTX 3080 GPU, the properties are as below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-text" data-lang="text"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;Device 0 properties:
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Max block dimensions: 1024 x 1024 x 64
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Max grid dimensions: 2147483647 x 65535 x 65535
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Shared memory bank size: 4 bytes
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Max shared memory per block: 49152 bytes
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Max registers per block: 65536
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Warp size: 32
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Multiprocessor count: 68
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Max resident threads per multiprocessor: 1536 = 48 warps
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; L2 cache size: 5242880 bytes
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Global L1 cache supported: yes
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Total global memory: 9 GB
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Processor clock: 1 MHZ
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; Memory clock: 9 MHZ
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description></item><item><title>Memory coalescing in CUDA (2) – Matrix Transpose</title><link>/posts/cuda-memory-coalescing-access-matrix-transpose/</link><pubDate>Tue, 05 Mar 2024 00:00:00 +0000</pubDate><guid>/posts/cuda-memory-coalescing-access-matrix-transpose/</guid><description>&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;In the &lt;a href="https://superjomn.github.io/posts/cuda-memory-coalescing-access/"&gt;VecAdd&lt;/a&gt; page, we&amp;rsquo;ve introduced the memory coalescing in global memory access. This post will follow the topic with another interesting application: Matrix transposing.&lt;/p&gt;
&lt;p&gt;The following content will briefly touch on the following topics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tiles in matrix, this is the basis of optimization matrix computation&lt;/li&gt;
&lt;li&gt;A simple trick to avoid bank conflict in shared memory access&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="kernels"&gt;Kernels&lt;/h2&gt;
&lt;p&gt;The code for all the kernels locates in &lt;a href="https://github.com/Superjomn/cuda-from-scratch/blob/dev/1-matrix-transpose-coalesce.cu"&gt;1-matrix-transpose-coalesce.cu&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="read-coalesced"&gt;Read coalesced&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-C++" data-lang="C++"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;template&lt;/span&gt; &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;typename&lt;/span&gt; T&lt;span style="color:#666"&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;__global__ &lt;span style="color:#902000"&gt;void&lt;/span&gt; transpose_read_coalesce(
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;const&lt;/span&gt; T&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ input,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; T&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ output,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; n,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; m) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; i &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.x; &lt;span style="color:#60a0b0;font-style:italic"&gt;// the contiguous tid
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; j &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.y &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.y &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.y;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (i &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; n &lt;span style="color:#666"&gt;&amp;amp;&amp;amp;&lt;/span&gt; j &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; m) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; output[i &lt;span style="color:#666"&gt;*&lt;/span&gt; m &lt;span style="color:#666"&gt;+&lt;/span&gt; j] &lt;span style="color:#666"&gt;=&lt;/span&gt; input[j &lt;span style="color:#666"&gt;*&lt;/span&gt; n &lt;span style="color:#666"&gt;+&lt;/span&gt; i];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="write-coalesced"&gt;Write coalesced&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-C++" data-lang="C++"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;template&lt;/span&gt; &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;typename&lt;/span&gt; T&lt;span style="color:#666"&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;__global__ &lt;span style="color:#902000"&gt;void&lt;/span&gt; transpose_write_coalesce(
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;const&lt;/span&gt; T&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ input,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; T&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ output,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; n,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; m) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; i &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.x; &lt;span style="color:#60a0b0;font-style:italic"&gt;// the contiguous tid
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; j &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.y &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.y &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.y;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (i &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; n &lt;span style="color:#666"&gt;&amp;amp;&amp;amp;&lt;/span&gt; j &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; m) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; output[j &lt;span style="color:#666"&gt;*&lt;/span&gt; n &lt;span style="color:#666"&gt;+&lt;/span&gt; i] &lt;span style="color:#666"&gt;=&lt;/span&gt; input[i &lt;span style="color:#666"&gt;*&lt;/span&gt; m &lt;span style="color:#666"&gt;+&lt;/span&gt; j];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="both-read-and-write-coalesced-by-tiling-with-shared-memory"&gt;Both read and write coalesced by tiling with shared memory&lt;/h3&gt;
&lt;p&gt;The tiling method is a common methodology for optimizing matrix operation. It divides the matrix into smaller, manageable blocks or &amp;ldquo;tiles&amp;rdquo; that can fit into shared memory.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s divide the matrix into tiles of size \(TILE \times TILE\), and the overall transpose could be decoupled into two sub-levels:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the inter-tile transpose, that is move the tile to the target position; and secondly,&lt;/li&gt;
&lt;li&gt;the intra-tile transpose, that is transpose the elements within a single tile&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id="inter-tile-transpose"&gt;Inter-tile transpose&lt;/h4&gt;
&lt;figure&gt;&lt;img src="/ox-hugo/inter-tile.png"&gt;
&lt;/figure&gt;
&lt;p&gt;Each tile is processed by a thread block, so the tile coordinate is &lt;code&gt;(blockIdx.y, blockIdx.x)&lt;/code&gt;, and the target coord is &lt;code&gt;(blockIdx.x, blockIdx.y)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can continue to process the elements within each tile.&lt;/p&gt;
&lt;h4 id="intra-tile-transpose"&gt;Intra-tile transpose&lt;/h4&gt;
&lt;figure&gt;&lt;img src="/ox-hugo/intra-tile.png"&gt;
&lt;/figure&gt;
&lt;p&gt;Within a tile, we will read the elements, store the transposed version in the shared memory, and then store the tile in global memory, with the coord determined by the intra-tile transpose phase.&lt;/p&gt;
&lt;p&gt;There are two copies:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Copying the tile from the input matrix and storing a transposed version into shared memory&lt;/li&gt;
&lt;li&gt;Copying the tile from shared memory into the output matrix in global memory&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Only one side is in global memory in both copies, so it can perform a memory coalescing access pattern. Both copies are performed by all the threads collectively within a thread block.&lt;/p&gt;
&lt;p&gt;To make a coalesced memory access, in the first copy, a thread reads element of coord of &lt;code&gt;(threadIdx.y, threadIdx.x)&lt;/code&gt;, and the memory offset &lt;code&gt;threadIdx.y * M + threadIdx.x&lt;/code&gt; is contignuous for adjacent threads.
In the second copy, the thread block needs to copy a tile to global memory, similarly, a thread should process the element of &lt;code&gt;(threadIdx.y, threadIdx.x)&lt;/code&gt; in the output tile.&lt;/p&gt;
&lt;h4 id="kernel-with-constant-tile-size"&gt;Kernel with constant tile size&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-C++" data-lang="C++"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;template&lt;/span&gt; &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;typename&lt;/span&gt; T, &lt;span style="color:#902000"&gt;int&lt;/span&gt; TILE&lt;span style="color:#666"&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;__global__ &lt;span style="color:#902000"&gt;void&lt;/span&gt; transpose_tiled_coalesce0(
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;const&lt;/span&gt; T&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ input,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; T&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ output,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; n,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; m) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; assert(blockDim.x &lt;span style="color:#666"&gt;==&lt;/span&gt; blockDim.y &lt;span style="color:#666"&gt;&amp;amp;&amp;amp;&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;==&lt;/span&gt; TILE_DIM);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// TILE + 1 to avoid bank conflict
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// By padding the shared memory array with an extra element, the consecutive threads access
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; &lt;span style="color:#60a0b0;font-style:italic"&gt;// memory locations that fall into different banks to avoid bank conflict
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#60a0b0;font-style:italic"&gt;&lt;/span&gt; __shared__ T tile[TILE][TILE &lt;span style="color:#666"&gt;+&lt;/span&gt; &lt;span style="color:#40a070"&gt;1&lt;/span&gt;];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; i &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.y &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.y &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.y;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; j &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (i &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; m &lt;span style="color:#666"&gt;&amp;amp;&amp;amp;&lt;/span&gt; j &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; n) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; tile[threadIdx.x][threadIdx.y] &lt;span style="color:#666"&gt;=&lt;/span&gt; input[i &lt;span style="color:#666"&gt;*&lt;/span&gt; n &lt;span style="color:#666"&gt;+&lt;/span&gt; j];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; __syncthreads();
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; i &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.y;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; j &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.y &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.y &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (i &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; n &lt;span style="color:#666"&gt;&amp;amp;&amp;amp;&lt;/span&gt; j &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; m) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; output[i &lt;span style="color:#666"&gt;*&lt;/span&gt; m &lt;span style="color:#666"&gt;+&lt;/span&gt; j] &lt;span style="color:#666"&gt;=&lt;/span&gt; tile[threadIdx.y][threadIdx.x];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that, since each thread processes only one element, so both &lt;code&gt;blockDim.x&lt;/code&gt; and &lt;code&gt;blockDim.y&lt;/code&gt; should equal to &lt;code&gt;TILE&lt;/code&gt;, and &lt;code&gt;TILE&lt;/code&gt; is a constant value.&lt;/p&gt;
&lt;h4 id="kernel-with-dynamic-tile-size"&gt;Kernel with dynamic tile size&lt;/h4&gt;
&lt;p&gt;It is possible to allocate the shared memory dynamically, making the &lt;code&gt;TILE&lt;/code&gt; a variable that could be assigned with &lt;code&gt;blockDim.x&lt;/code&gt; or &lt;code&gt;blockDim.y&lt;/code&gt; on the fly.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-C++" data-lang="C++"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;template&lt;/span&gt; &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;typename&lt;/span&gt; T&lt;span style="color:#666"&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;__global__ &lt;span style="color:#902000"&gt;void&lt;/span&gt; transpose_tiled_coalesce1(
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;const&lt;/span&gt; T&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ input,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; T&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ output,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; n,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; m) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;const&lt;/span&gt; size_t TILE &lt;span style="color:#666"&gt;=&lt;/span&gt; blockDim.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; assert(blockDim.x &lt;span style="color:#666"&gt;==&lt;/span&gt; blockDim.y);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;extern&lt;/span&gt; __shared__ T tile[];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; i &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.y &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.y &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.y;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; j &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (i &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; m &lt;span style="color:#666"&gt;&amp;amp;&amp;amp;&lt;/span&gt; j &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; n) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; tile[threadIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; (TILE &lt;span style="color:#666"&gt;+&lt;/span&gt; &lt;span style="color:#40a070"&gt;1&lt;/span&gt;) &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.y] &lt;span style="color:#666"&gt;=&lt;/span&gt; input[i &lt;span style="color:#666"&gt;*&lt;/span&gt; n &lt;span style="color:#666"&gt;+&lt;/span&gt; j];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; __syncthreads();
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; i &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.y;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; j &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.y &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.y &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (i &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; n &lt;span style="color:#666"&gt;&amp;amp;&amp;amp;&lt;/span&gt; j &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; m) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; output[i &lt;span style="color:#666"&gt;*&lt;/span&gt; m &lt;span style="color:#666"&gt;+&lt;/span&gt; j] &lt;span style="color:#666"&gt;=&lt;/span&gt; tile[threadIdx.y &lt;span style="color:#666"&gt;*&lt;/span&gt; (TILE &lt;span style="color:#666"&gt;+&lt;/span&gt; &lt;span style="color:#40a070"&gt;1&lt;/span&gt;) &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.x];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="performance"&gt;Performance&lt;/h2&gt;
&lt;p&gt;In NVIDIA GTX 3080, these kernels have a pretty close performance:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Kernel&lt;/th&gt;
&lt;th&gt;Latency&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Read coalesced&lt;/td&gt;
&lt;td&gt;0.0476&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Write coalesced&lt;/td&gt;
&lt;td&gt;0.0474&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tiled&lt;/td&gt;
&lt;td&gt;0.0478&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="reference"&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://leimao.github.io/blog/CUDA-Coalesced-Memory-Access/"&gt;CUDA Coalesced Memory Access - blog of Lei Mao&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Memory coalescing in CUDA (1) – VecAdd</title><link>/posts/cuda-memory-coalescing-access/</link><pubDate>Sun, 25 Feb 2024 00:00:00 +0000</pubDate><guid>/posts/cuda-memory-coalescing-access/</guid><description>&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Memory coalescing&lt;/strong&gt; is a crucial optimization technique in CUDA programming that allows optimal usage of the &lt;strong&gt;global memory bandwidth&lt;/strong&gt;. When threads in the same warp running the same instruction access to &lt;strong&gt;consecutive locations&lt;/strong&gt; in the global memory, the hardware can coalesce these accesses into a single transaction, significantly improving performance.&lt;/p&gt;
&lt;p&gt;Coalescing memory access is vital for achieving high performance. Besides PCIe memory traffic, accessing global memory tends to be the largest bottleneck in GPU&amp;rsquo;s memory hierarchy.
Non-coalesced memory access can lead to underutilization of memory bandwidth.&lt;/p&gt;
&lt;p&gt;In the following post, we will delve deeper into memory coalescing with CUDA code for the classical vector adding.&lt;/p&gt;
&lt;h2 id="vecadd"&gt;VecAdd&lt;/h2&gt;
&lt;p&gt;There are three kernels in below. The complete code locates &lt;a href="https://github.com/Superjomn/cuda-from-scratch/blob/dev/0-vecadd-memory-coalesce.cu"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="naive-vecadd-kernel-with-memory-coalescing-enabled"&gt;Naive VecAdd kernel with memory coalescing enabled&lt;/h3&gt;
&lt;p&gt;The first program is simple but follows the coalescing memory access pattern:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;tid&lt;/th&gt;
&lt;th&gt;element&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The thread 0,1,2,3 visits elements 0,1,2,3, which is contiguous, and results in a coalescing memory accessing.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-cpp" data-lang="cpp"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;template&lt;/span&gt; &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;typename&lt;/span&gt; T&lt;span style="color:#666"&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;__global__ &lt;span style="color:#902000"&gt;void&lt;/span&gt; add_coalesced0(
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;const&lt;/span&gt; T&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ a,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;const&lt;/span&gt; T&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ b,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; T&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ c,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; n) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; i &lt;span style="color:#666"&gt;=&lt;/span&gt; blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;+&lt;/span&gt; threadIdx.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (i &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; n) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; c[i] &lt;span style="color:#666"&gt;=&lt;/span&gt; a[i] &lt;span style="color:#666"&gt;+&lt;/span&gt; b[i];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The only issue is that, the number of the elements should be no larger than the number of threads, so the launching parameters of the kernel should be carefully designed.&lt;/p&gt;
&lt;h3 id="optimized-one-strided-with-less-threads"&gt;Optimized one: strided with less threads&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-cpp" data-lang="cpp"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;template&lt;/span&gt; &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;typename&lt;/span&gt; T&lt;span style="color:#666"&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;__global__ &lt;span style="color:#902000"&gt;void&lt;/span&gt; add_coalesced1(
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;const&lt;/span&gt; T&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ a,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;const&lt;/span&gt; T&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ b,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; T&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ c,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; N) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; tid &lt;span style="color:#666"&gt;=&lt;/span&gt; threadIdx.x &lt;span style="color:#666"&gt;+&lt;/span&gt; blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; num_threads &lt;span style="color:#666"&gt;=&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;*&lt;/span&gt; gridDim.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;while&lt;/span&gt; (tid &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; N) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; c[tid] &lt;span style="color:#666"&gt;=&lt;/span&gt; a[tid] &lt;span style="color:#666"&gt;+&lt;/span&gt; b[tid];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; tid &lt;span style="color:#666"&gt;+=&lt;/span&gt; num_threads;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This one simplifies the calculation of the launch thread number, it should fit any number of elements with a arbitrary number of threads.&lt;/p&gt;
&lt;h3 id="uncoalesced-memory-accessing-one"&gt;Uncoalesced memory accessing one&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-cpp" data-lang="cpp"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;template&lt;/span&gt; &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt;&lt;span style="color:#007020;font-weight:bold"&gt;typename&lt;/span&gt; T&lt;span style="color:#666"&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;__global__ &lt;span style="color:#902000"&gt;void&lt;/span&gt; add_uncoalesced(
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;const&lt;/span&gt; T&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ a,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;const&lt;/span&gt; T&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ b,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; T&lt;span style="color:#666"&gt;*&lt;/span&gt; __restrict__ c,
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; n) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; tid &lt;span style="color:#666"&gt;=&lt;/span&gt; threadIdx.x &lt;span style="color:#666"&gt;+&lt;/span&gt; blockIdx.x &lt;span style="color:#666"&gt;*&lt;/span&gt; blockDim.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; num_threads &lt;span style="color:#666"&gt;=&lt;/span&gt; blockDim.x &lt;span style="color:#666"&gt;*&lt;/span&gt; gridDim.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; num_tasks &lt;span style="color:#666"&gt;=&lt;/span&gt; nvceil(n, num_threads);
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;for&lt;/span&gt; (&lt;span style="color:#902000"&gt;int&lt;/span&gt; i &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#40a070"&gt;0&lt;/span&gt;; i &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; num_tasks; &lt;span style="color:#666"&gt;++&lt;/span&gt;i) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#902000"&gt;int&lt;/span&gt; idx &lt;span style="color:#666"&gt;=&lt;/span&gt; tid &lt;span style="color:#666"&gt;*&lt;/span&gt; num_tasks &lt;span style="color:#666"&gt;+&lt;/span&gt; i;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#007020;font-weight:bold"&gt;if&lt;/span&gt; (idx &lt;span style="color:#666"&gt;&amp;lt;&lt;/span&gt; n) {
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; c[idx] &lt;span style="color:#666"&gt;=&lt;/span&gt; a[idx] &lt;span style="color:#666"&gt;+&lt;/span&gt; b[idx];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This one doesn&amp;rsquo;t follow the coalescing access pattern, lets assume that we have 4 threads with 8 elements, then the `num_tasks=2`&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;tid&lt;/th&gt;
&lt;th&gt;0-th element&lt;/th&gt;
&lt;th&gt;1-st element&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In the first step of the for-loop, these four threads visit 0,2,4,6 elements, which is not contiguous, this results in an uncoalesced memory accessing.&lt;/p&gt;
&lt;h2 id="performance"&gt;Performance&lt;/h2&gt;
&lt;p&gt;All the kernels are tested with double data type, and the block size is 256, for the last kernels, each thread are setted to consume 8 elements.
The performance is tested on GTX 3090, with the clocks locked as below:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;GPU clocks&lt;/th&gt;
&lt;th&gt;Memory clocks&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2100 MHZ&lt;/td&gt;
&lt;td&gt;9501MHZ&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The latency of each kernel:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;kernel&lt;/th&gt;
&lt;th&gt;latency&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;coalesced0&lt;/td&gt;
&lt;td&gt;0.04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;coalesced1&lt;/td&gt;
&lt;td&gt;0.04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;uncoalesced&lt;/td&gt;
&lt;td&gt;0.14&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The uncoalesced kernel is 3x slower than the two coalesced kernel.&lt;/p&gt;
&lt;p&gt;The Nsight also report the Uncoalescing Global Accesses in the uncoalesced kernel:&lt;/p&gt;
&lt;figure&gt;&lt;img src="/ox-hugo/2024-02-28_19-37-47_screenshot.png"&gt;
&lt;/figure&gt;
&lt;p&gt;It reports that 75% of the sectors are excessive, IIUC, since only 8 bytes(a double) out each 32 byte transition is valid, so the overall efficiency is \(\frac{8}{32}=25\%\) .&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Professional CUDA C Programming&lt;/li&gt;
&lt;/ul&gt;</description></item></channel></rss>