<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=stylesheet type=text/css href=/css/bootstrap.min.css><link rel=stylesheet type=text/css href=/css/style.css><title>Superjomn's blog | Cuda</title><meta name=google-site-verification content="kO2XszgjIr1OyaOubGpeb0M34ADBz27vyI1dLETarCM"></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-Z60BDN3JGH"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Z60BDN3JGH")</script><script src=https://cdn.jsdelivr.net/npm/cash-dom/dist/cash.min.js></script><script>$(function(){$("article table").addClass("table")})</script><body><div id=nav-border class=container><nav id=nav class="nav justify-content-center"><a class=nav-link href=/><i data-feather=home></i>
Home
</a><a class=nav-link href=/tags/tech><i data-feather=terminal></i>
Tech
</a><a class=nav-link href=/tags/life><i data-feather=coffee></i>
Life
</a><a class=nav-link href=/posts/><i data-feather=archive></i>
Archive
</a><a class=nav-link href=/tags/><i data-feather=tag></i>
Tags
</a><a class=nav-link href=/posts/about/><i data-feather=info></i>
About</a></nav></div><div class=container><main id=main><h1>Cuda</h1><p><h3><a class=title href=/posts/reduce-cuda/>Reduce kernel in CUDA</a></h3><i data-feather=calendar></i> <time datetime=2024-03-25>Mar 25, 2024</time>
<span id=social></span><i data-feather=tag></i>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/cuda>cuda</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/basics>basics</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/tech>tech</a><p><h2 id=question-definition>Question definition</h2><p>Given an array of \(n\) integers, the goal is to compute the sum of all elements within the array.</p><h2 id=solutions>Solutions</h2><p>The implementations for all kernel versions can be found at <a href=https://github.com/Superjomn/cuda-from-scratch/blob/dev/2-reduce.cu>2-reduce.cu on GitHub</a>.</p><h3 id=naive-version-with-atomicadd>Naive Version with <code>atomicAdd</code></h3><p>The simplest approach involves utilizing each thread to perform an <code>atomicAdd</code> operation on the output variable. Here&rsquo;s how the kernel is defined:</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C++ data-lang=C++><span style=display:flex><span>__global__ <span style=color:#902000>void</span> <span style=color:#06287e>reduce_naive_atomic</span>(<span style=color:#902000>int</span><span style=color:#666>*</span> g_idata, <span style=color:#902000>int</span><span style=color:#666>*</span> g_odata, <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> n)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> idx <span style=color:#666>=</span> blockIdx.x <span style=color:#666>*</span> blockDim.x <span style=color:#666>+</span> threadIdx.x;
</span></span><span style=display:flex><span>    <span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> gridSize <span style=color:#666>=</span> blockDim.x <span style=color:#666>*</span> gridDim.x;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> sum <span style=color:#666>=</span> <span style=color:#40a070>0</span>;
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>for</span> (<span style=color:#902000>unsigned</span> <span style=color:#902000>int</span> i <span style=color:#666>=</span> idx; i <span style=color:#666>&lt;</span> n; i <span style=color:#666>+=</span> gridSize)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        sum <span style=color:#666>+=</span> g_idata[i];
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    atomicAdd(g_odata, sum);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>And the kernel launcher is straightforward, invoking the kernel a single time:</p></p></p><p><h3><a class=title href=/posts/cuda-memory-coalescing-access-matrix-transpose/>Memory coalescing in CUDA (2) – Matrix Transpose</a></h3><i data-feather=calendar></i> <time datetime=2024-03-05>Mar 5, 2024</time>
<span id=social></span><i data-feather=tag></i>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/cuda>cuda</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/basics>basics</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/tech>tech</a><p><h2 id=background>Background</h2><p>In the <a href=https://superjomn.github.io/posts/cuda-memory-coalescing-access/>VecAdd</a> page, we&rsquo;ve introduced the memory coalescing in global memory access. This post will follow the topic with another interesting application: Matrix transposing.</p><p>The following content will briefly touch on the following topics:</p><ul><li>Tiles in matrix, this is the basis of optimization matrix computation</li><li>A simple trick to avoid bank conflict in shared memory access</li></ul><h2 id=kernels>Kernels</h2><p>The code for all the kernels locates in <a href=https://github.com/Superjomn/cuda-from-scratch/blob/dev/1-matrix-transpose-coalesce.cu>1-matrix-transpose-coalesce.cu</a>.</p><h3 id=read-coalesced>Read coalesced</h3><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C++ data-lang=C++><span style=display:flex><span><span style=color:#007020;font-weight:700>template</span> <span style=color:#666>&lt;</span><span style=color:#007020;font-weight:700>typename</span> T<span style=color:#666>&gt;</span>
</span></span><span style=display:flex><span>__global__ <span style=color:#902000>void</span> transpose_read_coalesce(
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>const</span> T<span style=color:#666>*</span> __restrict__ input,
</span></span><span style=display:flex><span>    T<span style=color:#666>*</span> __restrict__ output,
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> n,
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> m) {
</span></span><span style=display:flex><span>  <span style=color:#902000>int</span> i <span style=color:#666>=</span> blockIdx.x <span style=color:#666>*</span> blockDim.x <span style=color:#666>+</span> threadIdx.x; <span style=color:#60a0b0;font-style:italic>// the contiguous tid
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>  <span style=color:#902000>int</span> j <span style=color:#666>=</span> blockIdx.y <span style=color:#666>*</span> blockDim.y <span style=color:#666>+</span> threadIdx.y;
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>if</span> (i <span style=color:#666>&lt;</span> n <span style=color:#666>&amp;&amp;</span> j <span style=color:#666>&lt;</span> m) {
</span></span><span style=display:flex><span>    output[i <span style=color:#666>*</span> m <span style=color:#666>+</span> j] <span style=color:#666>=</span> input[j <span style=color:#666>*</span> n <span style=color:#666>+</span> i];
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h3 id=write-coalesced>Write coalesced</h3><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C++ data-lang=C++><span style=display:flex><span><span style=color:#007020;font-weight:700>template</span> <span style=color:#666>&lt;</span><span style=color:#007020;font-weight:700>typename</span> T<span style=color:#666>&gt;</span>
</span></span><span style=display:flex><span>__global__ <span style=color:#902000>void</span> transpose_write_coalesce(
</span></span><span style=display:flex><span>    <span style=color:#007020;font-weight:700>const</span> T<span style=color:#666>*</span> __restrict__ input,
</span></span><span style=display:flex><span>    T<span style=color:#666>*</span> __restrict__ output,
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> n,
</span></span><span style=display:flex><span>    <span style=color:#902000>int</span> m) {
</span></span><span style=display:flex><span>  <span style=color:#902000>int</span> i <span style=color:#666>=</span> blockIdx.x <span style=color:#666>*</span> blockDim.x <span style=color:#666>+</span> threadIdx.x; <span style=color:#60a0b0;font-style:italic>// the contiguous tid
</span></span></span><span style=display:flex><span><span style=color:#60a0b0;font-style:italic></span>  <span style=color:#902000>int</span> j <span style=color:#666>=</span> blockIdx.y <span style=color:#666>*</span> blockDim.y <span style=color:#666>+</span> threadIdx.y;
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>if</span> (i <span style=color:#666>&lt;</span> n <span style=color:#666>&amp;&amp;</span> j <span style=color:#666>&lt;</span> m) {
</span></span><span style=display:flex><span>    output[j <span style=color:#666>*</span> n <span style=color:#666>+</span> i] <span style=color:#666>=</span> input[i <span style=color:#666>*</span> m <span style=color:#666>+</span> j];
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h3 id=both-read-and-write-coalesced-by-tiling-with-shared-memory>Both read and write coalesced by tiling with shared memory</h3><p>The tiling method is a common methodology for optimizing matrix operation. It divides the matrix into smaller, manageable blocks or &ldquo;tiles&rdquo; that can fit into shared memory.</p></p></p><p><h3><a class=title href=/posts/cuda-memory-coalescing-access/>Memory coalescing in CUDA (1) – VecAdd</a></h3><i data-feather=calendar></i> <time datetime=2024-02-25>Feb 25, 2024</time>
<span id=social></span><i data-feather=tag></i>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/cuda>cuda</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/basics>basics</a>
<a class="btn btn-sm btn-outline-dark tag-btn" href=/tags/tech>tech</a><p><h2 id=background>Background</h2><p><strong>Memory coalescing</strong> is a crucial optimization technique in CUDA programming that allows optimal usage of the <strong>global memory bandwidth</strong>. When threads in the same warp running the same instruction access to <strong>consecutive locations</strong> in the global memory, the hardware can coalesce these accesses into a single transaction, significantly improving performance.</p><p>Coalescing memory access is vital for achieving high performance. Besides PCIe memory traffic, accessing global memory tends to be the largest bottleneck in GPU&rsquo;s memory hierarchy.
Non-coalesced memory access can lead to underutilization of memory bandwidth.</p></p></p><p class="footer text-center">Copyright (c) 2025 Chunwei Yan</p></main></div><script src=/js/feather.min.js></script><script>feather.replace()</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script></body></html>