\relax 
\citation{mikolov2013linguistic}
\citation{huang2012improving}
\@writefile{toc}{\contentsline {title}{A Model for Learning Distributed Representation for Word and Sentence}{1}}
\@writefile{toc}{\authcount {1}}
\@writefile{toc}{\contentsline {author}{No Author Given}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{rumelhart1988learning}
\citation{bengio2006neural}
\citation{morin2005hierarchical}
\citation{mikolov2013distributed}
\citation{mikolov2013linguistic}
\citation{collobert2008unified}
\citation{socher2010learning}
\citation{salton1975vector}
\citation{mikolov2013distributed}
\citation{socher2010learning}
\citation{socher2011semi}
\citation{huang2012improving}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}The framework of WRGC}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces the demostration of the two-layer neural network.}}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:nn}{{1}{3}}
\newlabel{eq:max-score}{{1}{3}}
\newlabel{eq:local-context}{{3}{3}}
\newlabel{eq:global-context}{{4}{3}}
\newlabel{final-score}{{5}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}New model with sentence as global context}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Pretrain Word Vector}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An overview of the model. The part in the dashed box is the model that learns word vector representation. The model uses fixed-length sliding window as local context and sentence as global context to compute a score that should be large for the actual next word(bank in the example), compared to the score for other randomly selected words. The overall model (ignore the dashed box) is the part learning sentence vector representation. Two grey boxes are mapped to the same sentence vector. The one on the left side acts like a super item of local context which is shared across all the windows in this sentence, the one on the right side acts as the global context. }}{4}}
\newlabel{fig:model}{{2}{4}}
\newlabel{eq:new-max-score}{{6}{4}}
\newlabel{eq:new-global-context}{{7}{4}}
\citation{mnih2012fast}
\citation{mikolov2013distributed}
\newlabel{eq:h}{{8}{5}}
\newlabel{final-score}{{9}{5}}
\newlabel{eq:word-partial}{{10}{5}}
\newlabel{eq:nn-partial}{{11}{5}}
\newlabel{eq:model-update}{{12}{6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces the alogorithm of learning word vector using sentence as the global context}}{6}}
\newlabel{alg:word-vector}{{1}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Learning Sentence Vector}{6}}
\citation{quasthoff2006corpus}
\citation{agirre2009study}
\newlabel{eq:sentence-vector}{{14}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{7}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces the algorithm of learning sentence vector}}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Word Vector}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces OurModel-0.5 is our model with $\lambda =0.5$ in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 9\hbox {}\unskip \@@italiccorr )}} which is the final score formula in WRGC. Ourmodel-0.3 is our model with $\lambda =0.3$.}}{8}}
\newlabel{table:word-vector}{{1}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces adjust parameter for $\lambda $ in OurModel on mixed-corpus}}{9}}
\newlabel{fig:word-vectro-lambda-parameter}{{3}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Sentence Vector}{9}}
\@writefile{toc}{\contentsline {subsubsection}{Data Sets and Evaluation Systen}{9}}
\citation{zhang2013topicdsdr}
\citation{blei2003latent}
\citation{socher2011dynamic}
\citation{mikolov2013efficient}
\citation{salton1988term}
\citation{klein2003accurate}
\@writefile{toc}{\contentsline {subsubsection}{Implemented Systems and Result Analysis}{10}}
\bibstyle{plain}
\bibdata{re}
\bibcite{agirre2009study}{1}
\bibcite{bengio2006neural}{2}
\bibcite{blei2003latent}{3}
\bibcite{collobert2008unified}{4}
\bibcite{huang2012improving}{5}
\bibcite{klein2003accurate}{6}
\bibcite{mikolov2013efficient}{7}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces OurModel-0.5 is our model with $\lambda =0.5$ in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 9\hbox {}\unskip \@@italiccorr )}} which is the final score formula in WRGC. OurModel-0.6 is our model with $\lambda =0.6$.}}{11}}
\newlabel{table:word-vector}{{2}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Comparison results on DUC2006}}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{11}}
\bibcite{mikolov2013distributed}{8}
\bibcite{mikolov2013linguistic}{9}
\bibcite{mnih2012fast}{10}
\bibcite{morin2005hierarchical}{11}
\bibcite{quasthoff2006corpus}{12}
\bibcite{rumelhart1988learning}{13}
\bibcite{salton1988term}{14}
\bibcite{salton1975vector}{15}
\bibcite{socher2011dynamic}{16}
\bibcite{socher2010learning}{17}
\bibcite{socher2011semi}{18}
\bibcite{zhang2013topicdsdr}{19}
