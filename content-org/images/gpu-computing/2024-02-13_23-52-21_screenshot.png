\documentclass[runningheads,a4paper]{llncs}%[runningheads,a4paper]
\usepackage{makeidx}  % allows for indexgeneration
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{indentfirst}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\setcounter{tocdepth}{3}
\usepackage{url}
\begin{document}
\title{A New Model for Sentence Distributed Representation}
\maketitle

\begin{abstract}
Vector-space distributed representation of word can capture syntactic and semantic regularities in language and help learning algorithms to achieve better performance in neural language processing tasks by grouping similar words.
With progress of machine learning techniques in recent years, much attention has been paid on this field.
But many NLP tasks such as text summary and sentence matching treat sentences as atomic units.
In this paper, we introduce a new model which can generate distributed representations for variable-sized sentences.
Feature vectors for sentences are learned on the basis of their probability of co-occurring of the window and  sentence using a neural network,
We use the feature vectors in text summary task with a method based on data reconstruction. Extensive experiments on summarization benchmark datasets DUC 2006 and DUC 2007 demonstrate the effectiveness of vectors generated by our proposed model.
\end{abstract}

\section{Introduction}
With the rapid progress of machine learning techniques, training more complex models on much larger data set becomes possible. Recently, neural probabilistic language models has been drown much attention.
They model the distribution for the next word as a smooth function of learned multi-dimensional real-valued representations of the context words and the target word.
By training a neural network language model, one obtains not just the model itself, but also the learned word representations, which can be used for other tasks.

Similar words will have similar representations and \cite{mikolov2013linguistic} find that these representations are good at capturing syntactic and semantic regularities in language, and relationships are characterized by relation-specific vector offsets.

To represent sentences with vectors, we think two points are very important: the sentence's representation should 1) contain all the meanings of its words. 2) record the words' sequence.

In this paper, we introduce a single neural network architecture that generate fixed-width vector representations for both words and sentences.
Sets of feature vectors

\section{Related work}
Recently,



\section{The Proposed Model}
\begin{center}
  % Requires \usepackage{graphicx}
  \includegraphics[width=230pt]{images/model.png}
\end{center}
In this section, we describe the details of our proposed model.
The model has two sets of feature vectors for words and sentences.
The Words' vectors are shared between all sentences, but sentences' vectors are different between each other. We tried to model the co-occurrence frequencies of $w$ with the words occurring in text around the occurrence of $w$ with the sentence which it is in.
%\subsection{Word's Representation}

The training set is a set of sentences $s_t \in S$, where the vocabulary $V$ is a large but finite set. our goals are as follows: First, the model assigns to each word and each sentence a continuous representation in an $d$-dimensional embedding space.
These representations should capture syntactic information.
Second, given a local context and the sentence vector, we use a neural network to induce a probability for the target word.

We use the Continuous Bag-of-Words (CBOW) method introduced in \cite{mikolov2013efficient}, and use the context to predict the target word.
Define $n$ words nearby in a sentence as a window, $n$ is a fixed number.
The objective is to maximum the target probability: $P(w_t | w_{t-1}, w_{t-2}, \cdots, w_{t-n+1}, s_j)$, where $w_t$ is the vector of the $t$-th word in vocabulary, $s_j$ is the current sentence's vector which index is $j$.

\begin{algorithm}
\caption{training algorithm}
Input:
\begin{enumerate}[topsep=0pt, partopsep=0pt]
    \item[\labelitemii] Sentences set: $S = \{s_j|j \in |S|\}$
    \item[\labelitemii] Learning rate $\varepsilon$
\end{enumerate}
Output:
\begin{enumerate}[topsep=0pt, partopsep=0pt]
    \item[\labelitemii] Sentence vectors $S$
    \item[\labelitemii] Word vectors $W$
\end{enumerate}

\begin{algorithmic}[1]
\Procedure : {}{} \\
    initialize $S$, $W$ with samples from the standard normal distribution
    \Repeat
    \For{$j=1$ to $|S|$}
        \For{$t=1$ to $length(sentence_j)-n+1$}
            \State $W_t \leftarrow \sum \{w_{t-n+1}, w_{t-n+2}, \cdots, w_t\}$
            \State generate $k$ negative samples $D' \leftarrow \{w_t | t=t^{(1)}, \cdots, t^{(k)}\}$
            \State update corresponding vectors in $S$, $W$ with function \eqref{eq:update_theta}
        \EndFor
    \EndFor
    \Until {converge}
\EndProcedure

\end{algorithmic}
\end{algorithm}


%\subsection{Sentence's Represention}
We represent each sentence with a unique vector, the vectors will be updated during the period of training sentence by sentence.
We use sentence's vector as hidden layer in neural network, the probability is calculated as
\begin{equation}
P(w_t | w_{t-1}, \cdots, w_{t-n+1};\theta) = \frac{\exp(s_j^T x_t)}
{\sum_{i=1}^{|W|}\exp(s_j^T x_i)}
\end{equation}
where $x_t$ is the sum of vectors of the words of the local context and the target word $w_t$ in a window: $x_t = \sum_{i=t-n+1}^t w_i$, $|W|$ is the size of vocabulary.
The parameters $\theta$ are words' vectors $w_{\cdots}$ and sentences' vectors $s_{\cdots}$: $\theta = (S, W)$  (a total of $|S| \times |V| \times d$ parameters), where $d$ is the vector size.
Like \cite{bengio2006neural}, we tried to traverse the entire vocabulary in the denominator to get a softmax output as the target probability.
Taking the log and switch from product to sum and we get the target
\begin{equation}
\label{eq:full_denominator}
\begin{split}
J   = & arg \max_\theta \sum_{(t,s)\in D} \log \frac{\exp(s_j^T x_t)}
{\sum_{i=1}^{|W|}\exp(s_j^T x_i)} \\
    = &\sum \left[ \exp (s_j^T x_t) - log \sum_{i=1}^{|W|} \exp(s_j^T x_i)\right]
\end{split}
\end{equation}

To accelerate the computation of the denominator, we use a negative sampling method described in \cite{gutmann2010noise}, and \cite{mikolov2013distributed} present it is a efficient way of deriving word embeddings. For each target word we generated $k$ words as data set of noise $y$ with distribution $p_n(.)$,
approximating the finite word set to the entire vocabulary, we can simplify the computation of denominator of equation \eqref{eq:full_denominator}, and get a new target function
\begin{equation}
\tilde{J} = arg \max_\theta \sum_j
\left[ \sum_{t \in D} \log \sigma (s_j^T \cdot x_t) + \sum_{t\in D'} \log \sigma (-s_j^T \cdot x_t) \right]
\end{equation}
where $D'$ is the artificially generated noise set, and $\sigma$ is the logistic function $\sigma(x) = \frac{1}{1+exp(-x)}$.

We use stochastic gradient ascent to maximum the target function, and update the parameter $\theta$ with function:
\begin{equation}
\label{eq:update_theta}
\theta \leftarrow \theta + \varepsilon \frac{\partial \tilde{J}} {\partial \theta}
\end{equation}


\section{Experiments}
\subsection{Data Sets and Evaluation System}
To evaluate the generated word vectors and sentence vectors, we

we performed on text from the Associated Press (AP) News from 1999 to 2000. The training contains about




\bibliographystyle{plain}%Choose a bibliograhpic style
\bibliography{re}




\end{document}
