\documentclass[runningheads,a4paper]{llncs}%[runningheads,a4paper]
\usepackage{makeidx}  % allows for indexgeneration
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{indentfirst}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\setcounter{tocdepth}{3}
\usepackage{url}
\urldef{\mailsa}\path|yanchunwei@outlook.com| 
\urldef{\mailsb}\path|fan.zhgf@gmail.com|
\urldef{\mailsc}\path|hle@net.pku.edu.cn|
\begin{document}
\bibliographystyle{splncs03}
\mainmatter

% TODO final add
\titlerunning{Distributed Representations for Words and Sentences}
\title{DRWS: A Model for Learning Distributed Representations for Words and Sentences}
\author{Chunwei Yan, Fan Zhang
,Lian'en Huang\thanks{Corresponding author}}
\authorrunning{}
\institute{The Shenzhen Key Lab for Cloud Computing Technology and Application \\
    Peking University Shenzhen Graduate School, Shenzhen, Guangdong P.R. China\\
\mailsa, \mailsb, \mailsc
}

\toctitle{DRWS: A Model for Learning Distributed Representations for Words and Sentences}
\tocauthor{Chunwei Yan}

\maketitle

\begin{abstract}
Vector-space distributed representations of words can capture syntactic and semantic regularities in language and help learning algorithms to achieve better performance in natual language processing tasks by grouping similar words.
With progress of machine learning techniques in recent years, much attention has been paid on this field.
However, many NLP tasks such as text summary and sentence matching treat sentences as atomic units.
In this paper, we introduce a new model called DRWS which can learn distributed representations for words and variable-length sentences.
Feature vectors for words and sentences are learned based on their probability of co-occurrance of word and sentence using a neural network.
To evaluate feature vectors learned by our model, we applied our model on the tasks of detecting word similarity and text summarization.
Extensive experiments demonstrate the effectiveness of our proposed model in learning vector representations for words and sentences.
% TODO final add
\keywords{Neural network, Distributed representation, Global context, Word vector, Sentence vector}
\end{abstract}


\section{Introduction}
With the rapid progress of machine learning techniques, training more complex models on much larger data set becomes possible. Recently, neural probabilistic language models has been drown much attention.
These models model the distribution for the next word as a smooth function of learning multi-dimensional real-valued representations of the context words and the target word.
By training a neural network language model, one obtains not just the model itself, but also the learned word representations, which can be used for other tasks.
Similar words will have similar representations and \cite{mikolov2013linguistic} find that these representations are good at capturing syntactic and semantic regularities in language, and relationships are characterized by relation-specific vector offsets.

Huang\cite{huang2012improving} introduces a new neural-network-based language model that learns word's representation  with both local and global contexts via a joint training objective.  Directly inspired by the concept of global context,
    we treat sentence as the global context,
    and get a new model which can generate better words' representations and fixed-width sentences' representations.

To represent sentences with vectors,
    we assume that the sentence's representation should 1) contain all the meanings of words in it. 2) record the words' order.
Therefore, we proposed a model that learns distributed representations for words and sentences called DRWS.
The model learns word vector representation first, and frozes model parameters.
    Sentence's vector is then embedded to both the local context and global context and continue the training process.
    In this way, the semantics captured in word vectors will be incorporated into sentences' representations.

We evaluate our new model on the standard WordSim-353 dataset which includes human similarity judgments on pairs of words,
showing that combining both local and global context(sentence) can learn better representation of words.
To evaluate the performance of sentences' representations, we take the task of text summarization, and test the vectors on the data set DUC 2006.
Results show the effectiveness of the syntax and semantics embedded in sentence vectors.

\section{Related Works}
The idea of distributed representations for words were first proposed in \cite{rumelhart1988learning}, and \cite{bengio2006neural}\cite{morin2005hierarchical}\cite{mikolov2013distributed} further extended the work. One product of these models is word vector representation, which captures the syntax and semantics of corpus, and \cite{mikolov2013linguistic} found that these representations are good at capturing syntactic and semantic regularities in language.
Word vectors has been widely used in NLP tasks such as part-of-seech tagging, chunking, named-entity recognition and semantic role-labeling\cite{collobert2008unified}\cite{socher2010learning}.

The traditional way to represent sentence in vector-space is vector space model (VSM)\cite{salton1975vector},
sentence can be represented as $s_i = (w_{1,i}, w_{2,i}, \cdots, w_{t,i})$, where $w_{t,i}$ is the weight of word.
As one of the well-known disadvantages, VSM do not record the word order of the sentence, so it failed to record the syntax.
\cite{mikolov2013distributed} introduced a method to represent phrase,
    in their model, each frequent phrase is replaced with a unique token in the vocabulary and is trained as a special word.
However, the method doesn't work for sentence-level representations as it is unable to capture the co-occurrance of sentences in the same way.

Recently, representing sentences received much attention.
Socher\cite{socher2010learning} introduced a new model called Recursive Neural Network(RNN) which could learn fixed-width vector representations for variable-length sentences, but this model need to recursively merge children to build a tree like Huffman tree algorithm leading to a complexity of $O(n\log n)$.
\cite{socher2011semi} introduced Recursive Autoencoder (RAE), which is similar to RNN except using a parse tree to build the network, and the parse process is time-consuming, too.

%TODO   .... as far as we know, .......





\section{Model Description}
Training process of our model has two periods,
    it learns word vector representations in the first period,
    and generates sentence vector representations in the second period.

\subsection{Learning Word Vector}
In this section, we describe the training process of word vector representations.
The word vector learning process of our model is based on Huang\cite{huang2012improving}, and the structure is shown in Figure \ref{fig:word-vector}.

While Huang\cite{huang2012improving} utilizes the entire document to provide global context for a word sequence,
    we consider that sentence is a more appropriate choice for global context,
    because sentence has a closer relationship with the local context,
    and can be viewed as a superset of local contexts in it.

The local contexts are fixed-length and sampled from sliding windows over the sentence.
Given a sequence of words $s$ (local context) and sentence $h$ (global context) in which the sequence occurs,
    DRWS's objective is to discriminate the correct last word in $s$ from other random selected words $s^*$.
Scores $g(s,h)$ and $g(s^*, h)$ are computed where $s^*$ is $s$ with the last word replaced by random word $w^*$,
and $g(.,.)$ is a neural network.
To discriminate $s$ from $s^*$, $g(s,h)$ should be larger than $g(s^*, h)$ by a margin of 1,
which corresponds to the training objective of minimizing the ranking loss for each $(s,d)$ found in the corpus:
\begin{figure}[htbp]
  \centering
   \includegraphics[width=340pt]{images/word-vector-2.pdf}
   \caption{ An overview of the part of our model that learns word vectors.
      The model uses fixed-width sliding window as local context and sentence as global context to compute a score that should be large for the actual next word
          ("bank" shown in the example),
      compared to the score for other randomly selected words.
      Paramter $\lambda$ is the weight of the score of global context.
  }
  \label{fig:word-vector}
\end{figure}
\begin{equation}
\label{eq:max-score}
C_{v,s} = \sum_{w\in V} \max(0, 1-g(s,h) + g(s^*, h)).
\end{equation}

In our model, we treat words in a sentence as the global context of the word sequences in it.
The vector representation of the sentence is the weighted average of all the word vectors in it:
\begin{equation}
    \label{eq:h}
    h = \sum_{i=1}^{k} w(t_i) x_{t_i}
\end{equation}
where $t_i$ is the $i$-th word in the sentence,
$w(.)$ is the weight function for a word like idf-weighting and $x_{t_i}$ is the vector representation of $t_i$.
The framework use two two-layer neural networks to compute the scores of local context and global context:
\begin{equation}\label{eq:local-context}
  score_l  =  W_2 f(W_1[x_1;x_2;\cdots;x_m]+b_1) + b_2
\end{equation}
\begin{equation}\label{eq:global-context}
  score_g  =  W^g_2 f(W^g_1[h;x_m]+b^g_1) + b^g_2
\end{equation}
where $score_l$ is the score of local context,
$score_g$ is the score of global context,
$x_i$ is the embedding of word $i$ in the sequence, which is a column in the embedding matrix $L \in \mathbb{R}^{n \times |V|}$ and $|V|$ is the size of the vocabulary.
$x_i$ is will be learned and updated during training.
$[x_1;x_2;\cdots;x_m]$ is the concatenation of the $m$ words.
$W^{(.)}_1 \in \mathbb{R}^{h\times (mn)}, W^{(.)}_2 \in \mathbb{R}^{h \times 1}$ are respectively the first and second layer weights of the neural network,
and $b^{(.)}_1 \in \mathbb{R}^{m\times 1}$ and $b^{(.)}_2 \in \mathbb{R}^{h\times 1}$ are the bias of each layer.
$f$ is an element-wise activation function such as $tanh$.

The local score measures the model's ability to predict the words' sequence,
while the global score demonstrates the ability to distinguish the words contained in the sentence.
To adjust the degree of both sides, we add a weight parameter $\lambda$ for the global context, and the final score is calculated as
\begin{equation}\label{eq:final-score}
    score = (1-\lambda)score_l + \lambda score_g
\end{equation}
For different tasks, $\lambda$ will be adjusted to learn more suitable word representation.

We use Noise Contrastive Estimation (NCE)\cite{mnih2012fast}\cite{mikolov2013distributed} to sample noise word $w^*$ and generate a wrong word sequence $s^*$ in
    Equation\eqref{eq:final-score}.
The word vectors are trained using stochastic gradient descent and the gradient is obtained via backpropagation.

Define $X = [x_1, \cdots, x_m]$,
$X^* = [x_1, \cdots, x^*_m]$,
$X_g = [h;x_m]$, $X^*_g = [h;x^*_m]$.
% TODO cases that max is 0
If $1-g(s,d)+g^w(s,d)\le 0$ in Equation\eqref{eq:max-score}, all the partial derivatives are assigned to zero, otherwise the partial derivatives are as follows:

\begin{equation}
    \label{eq:word-partial}
\begin{split}
\frac{\partial C} {\partial X} & =
    (\lambda -1)W_2 f(W_1 X + b_1) W_1 \\
\frac{\partial C} {\partial X^g} & =
    - \lambda W_2^g f'(W_1^g X^g + b_1^g) W_1^g \\
\frac{\partial C} {\partial X^*} & =
    (1-\lambda)W_2  f'(W_1 X^* + b_1) W_1 \\
\frac{\partial C} {\partial X_g^*} & =
    \lambda W^g_2 f'(W_1^g X^*_g + b_1^g) W_1^g \\
\end{split}
\end{equation}


\begin{equation}
    \label{eq:nn-partial}
\begin{split}
\frac{\partial C} {\partial W_2} &=
    (1-\lambda) [f(W_1 X^* + b_1) - f(W_1 X + b_1)] \\
\frac{\partial C} {\partial W_1} & =
    (1-\lambda) W_2 [ f'(W_1 X^* + b_1) - f'(W_1 X + b_1) X] \\
\frac{\partial C} {\partial b_1} & =
    (1-\lambda) W_2 [ f'(W_1 X^* + b_1) - f'(W_1 X + b_1)] \\
\frac{\partial C} {\partial W^g_2} & =
    \lambda [f(W_1^g X^*_g + b_1^g) - f(W_1^g X^g + b_1^g)] \\
\frac{\partial C} {\partial W^g_1} & =
    \lambda W_2^g[ f'(W_1^g X^*_g + b_1^g) X_g^*
        - f'(W_1^g X_g + b_1^g) X^g \\
\frac{\partial C} {\partial b_1^g} & =
    \lambda W_2^g [f'(W_1^g X^*_g + b_1^g) - f'(W_1^g X_g + b_1^g)] \\
\end{split}
\end{equation}

\begin{algorithm}
\caption{the alogorithm of learning word vector using sentence as the global context}
\label{alg:word-vector}
Input:
\begin{enumerate}[topsep=0pt, partopsep=0pt]
    \item[\labelitemii] Sentence set: $S = \{s_j|j \in |S|\}$
    \item[\labelitemii] Learning rate $\alpha$
    \item[\labelitemii] weight of global context $\lambda$
    \item[\labelitemii] size of  sliding window $n$
\end{enumerate}
Output:
\begin{enumerate}[topsep=0pt, partopsep=0pt]
    \item[\labelitemii] Word embedding matrix $L$
\end{enumerate}

\begin{algorithmic}[1]
\Procedure : {}{} \\
    initialize $L$ with random samples from the standard normal distribution
    \Repeat
    \For{$j=1$ to $|S|$}
        \State calculate sentence representation $h$ as global context using Equation\eqref{eq:h}
        \For{$t=1$ to $length(sentence_j)-n+1$}
            \State word sequence $s_t \leftarrow (w_{t-n+1}, w_{t-n+2}, \cdots, w_t)$
            \State generate $k$ negative samples $D \leftarrow \{w_t | t=t^{(1)}, \cdots, t^{(k)}\}$
            \State generate $k$ wrong sequences using $s_t$ and $D$.
            \State train the model with model shown in Figure \ref{fig:word-vector}
            \State update neural network parameters and $L$ via Equation\eqref{eq:nn-partial}\eqref{eq:word-partial}\eqref{eq:model-update}.
        \EndFor
    \EndFor
    \Until {converge}
\EndProcedure

\end{algorithmic}
\end{algorithm}


All the model parameters are denoted as $\theta$:
\begin{equation}
    \theta = \{ L,     W_1, W_2, W^g_1, w^g_2, b_1, b_2, b^g_1, b^g_2 \}
\end{equation}
Then the model parameters are updated during training as follows:
\begin{equation}
    \label{eq:model-update}
    \theta := \theta - \alpha \frac{\partial C}
                        {\partial \theta}
\end{equation}
where $\alpha$ is the learning paramter.


The process of learning word vector representations is shown in Algorithm \ref{alg:word-vector}.



\subsection{Learning Sentence Vector}

After words' vectors are learned,
    we get a neural network and a word embedding matrix $L$ that captured semantic and syntax of the corpus.
We treat sentence as the global context in the previous period which learns word representation,
    but sentence's vector is directly filled with weighted average of all words' vectors those it contains using Equation\eqref{eq:h}.

In this section,
    we describe the approach of learning sentence vector representation.
    Different from Equation\eqref{eq:h},
    in the following period,
    each sentence is mapped to a unique vector,
    and all the sentence vectors are learned by the model.

A randomly initialized matrix $H$ is created,
    and each sentence in corpus is mapped to a unique vector,
    represented by a column in matrix $H$.
We frozen the parameters of the model learned in the previous process  and concatenate sentence vector to each window in it as a super item of the local context,
    the sentence vector is shown as the grey box in the left side of Figure \ref{fig:sentence-vector}.
    which records the information of the whole sentence and helps to predict the next word of the local context.

\begin{figure}[htbp]
  \centering
   \includegraphics[width=340pt]{images/paper.pdf}
   \caption{An overview of the model.
      The the part of DRWS which learns sentence vector representations.
  }
  \label{fig:sentence-vector}
\end{figure}
The overall framework of learning sentence vector representations is shown in Figure \ref{fig:sentence-vector}.
    both the grey boxes in the left and right parts are mapped to the same sentence vector.
On the left side of Figure \ref{fig:sentence-vector},
    sentence's vector acts like a super item in a local context which is shared across the local contexts in the same sentence and helps to predict the last word of each window.
    On the right side,
    sentence's vector acts as a global context, which records the meaning of the whole sentence and discriminates word in it.

The score of the local context is computed like
\begin{equation}
  score_l  =  W_2 f(W_1[s;x_1;x_2;\cdots;x_{m-1}]+b_1) + b_2
\end{equation}
and sentence vector $h$'s partial derivatives is
\begin{equation}
\begin{split}
\frac{\partial C} {\partial h}   =  & (1-\lambda)\{W_2 f'([h,X'])W_1 -W_2 f'(h,x'^*)W_1\}^{[1:|h|]}   \\
                                    & + \lambda\{W^g_2 f'([h,x^*_m])W^g_1 -W^g_2 f'(h,x_m^*)W^g_1\}^{[1:|h|]} \\
\end{split}
\label{eq:sentence-vector}
\end{equation}
where $(.)^{[1:|h|]}$ means take the first $|h|$ elements as a new vector.

\begin{algorithm}
\caption{the algorithm of learning sentence vector}
\label{alg:sentence-vector}
Input:
\begin{enumerate}[topsep=0pt, partopsep=0pt]
    \item[\labelitemii] Sentences set: $S = \{s_j|j \in |S|\}$
    \item[\labelitemii] Learning rate $\alpha$
    \item[\labelitemii] Word embedding matrix $L$ which have been learned.
    \item[\labelitemii] Neural network parameters which have been learned.
    \item[\labelitemii] size of  sliding window $n$
\end{enumerate}
Output:
\begin{enumerate}[topsep=0pt, partopsep=0pt]
    \item[\labelitemii] Sentence embedding matrix $H$
\end{enumerate}

\begin{algorithmic}[1]
\Procedure : {}{} \\
    initialize senetence embedding matrix $H$.
    \Repeat
    \For{$j=1$ to $|S|$}
        \For{$t=1$ to $length(sentence_j)-n+1$}
            \State word sequence $s_t \leftarrow (w_{t-n+1}, w_{t-n+2}, \cdots, w_t)$
            \State generate $k$ negative samples $D \leftarrow \{w_t | t=t^{(1)}, \cdots, t^{(k)}\}$
            \State generate $k$ wrong sequences using $s_t$ and $D$.
            \State train the model using neural network in Figure \ref{fig:sentence-vector} (ignore the dashed box)
            \State update sentence embedding matrix $H$ via Equation\eqref{eq:sentence-vector} Equation\eqref{eq:model-update}.
        \EndFor
    \EndFor
    \Until {converge}
\EndProcedure

\end{algorithmic}
\end{algorithm}


Through the sentence's vectors are initialized randomly,
    based on the word vectors and parameters of neural networks learned before,
    the model will follow the target function\eqref{eq:final-score} and force sentence vectors to record the syntax and semantics of the corpus during the training period,
    and finally we get well-trained sentence vectors.

The process of learning vector representations for sentences is shown in Algorithm \ref{alg:sentence-vector}.





\section{Experiments}
In this section, we used a mixed corpus for experiment including DUC2002-2007, science news from Australian Broadcasting Commission 2006, 2009-news-300K\cite{quasthoff2006corpus}.
The mixed corpus has a total of 558k sentences and 10M words.

We get a cleaned  corpus by cleaning the mixed-corpus sentence by sentence using processing methods as follows:
\begin{enumerate}
    \item split punctuations from words, and treat punctuations as words
    \item lowercase all the words
    \item remove stop words and words that appear less than 5 times
\end{enumerate}

\subsection{Evoluation on Word Vector}
We took the task of word similarity to test the performance of word vector learned by DRWS using the WordSim-353 data set\cite{agirre2009study},
    which consists of 353 pairs of nouns.
Each pair of words is presented with human judgments on similarity on a scale from 0 to 10.

Table \ref{table:word-vector} shows our results compared to Huang\cite{huang2012improving}.
Both model use 5 as the window size.
We cross validate the weight parameter $\lambda$ and find that $\lambda = 0.3$ is a better choice for final score\eqref{eq:final-score} in this task.

\begin{table}[h]
    % TODO final add \rho
    \caption{ Spearmanâ€™s $\rho$ correlation on WordSim-353, DRWS-0.5 is our model with $\lambda=0.5$ in Equation\eqref{eq:final-score} which is the final score formula in Huang\cite{huang2012improving}. DRWS-0.3 is our model with $\lambda=0.3$.}
\centering
\setlength{\tabcolsep}{9pt}
\begin{tabular}{lll}
   \hline\noalign{\smallskip}
    Model   &   Corpus  &   $\rho \times 100 $ \\
    \noalign{\smallskip}
\hline
\noalign{\smallskip}
Huang\cite{huang2012improving}            &   mixed-corpus        &   $63.1$  \\
Huang\cite{huang2012improving}           &   cleaned-corpus     &   $69.7$  \\
    DRWS-0.5    &   mixed-corpus        &   $65.2$  \\
    DRWS-0.3    &   mixed-corpus        &   $68.3$  \\
    DRWS-0.3    &   \textbf{cleaned-corpus}     &   $\textbf{71.6}$  \\
    \hline
\end{tabular}
\label{table:word-vector}
\end{table}
% TODO add a image for weight selection
\begin{figure}[htbp]
  \centering
   \includegraphics[width=280pt]{images/parameter.pdf}
   \caption{adjust parameter for $\lambda$ in DRWS on mixed-corpus}
  \label{fig:word-vectro-lambda-parameter}
\end{figure}
Our model is able to learn more semantic word embeddings and noticeably improves upon Huang\cite{huang2012improving}.
Note that trained on mixed-corpus, DRWS-0.5 has the same final score formula in Huang\cite{huang2012improving}, but get higher score(65.2), showing the effectness of sentence as global context.
The global context weight $\lambda$ has an adjust effection,
we cross validate $\lambda$ by the step of $0.3$ and get a best result when $\lambda=0.3$,
the result is shown in Figure \ref{fig:word-vectro-lambda-parameter}.
Notice that, our model only cares about local context when $\lambda = 0$, and it training only with global context when $\lambda=1.0$.
It shows that trains only with local context has a better performance than only with global context,
    because the model tries to capture the semantics and syntax of a sequence of words via predicting a word with local context,
    while it needs less information to determine whether a word is contained in the sentence with global context, thus less information is learned.
The cleaned corpus helps our model to get the best score $71.6$, which is a $4.8\%$ improvement compared to the uncleaned corpus.

\subsection{Evaluation on Sentence Vector}
Sentence vectors generated by our model were evaluated by taking the task of text summary.

\subsubsection{Data Sets and Evaluation System}
We use the standard summarization benchmark data sets DUC 2006, which  contains 50 documents. And every sentence is either used in its entirety or not at all for constructing a summary.
The length of a result summary is limited by 250 tokens(whitespace delimited).

The final result is evaluated by the ROUGE(Recall-Oriented Understudy for Gisting Evaluation) toolkit.
It measures the quality of a summary by counting the unit overlaps between the candidate summary and a set of references summaries.
ROUGE measures summary quality by counting overlapping units such as the n-gram, word sequences and word pairs between the peer summary (produced by algorithms) and the model summary (produced by humans).

\subsubsection{Implemented Systems and Result Analysis}
We used data reconstruction based summary algorithm (NonTopicDSDR) \cite{zhang2013topicdsdr} to generate summary.
NonTopicDSDR attempts to find an optimal set of representative sentences $X$ to approximate the entire document $V$.
The optimal objective function of NonTopicDSDR like:
\begin{eqnarray}
\min_{a_i}\enspace \sum_{i=1}^{N}||v_i-f_i(X;a_i)||^{2}
\end{eqnarray}
where $a_i$ is the parameter determine wheather sentence $i$ will be selected to summary. $f_i(X;a_i)$ is a linear function and $||.||$ is the $L_2$-norm.

% TODO tell the framework of DSDR
The input of NonTopicDSDR can be any fixed-width vectors that can represent the meaning of sentences.
The original algorithm used Latent Dirichlet allocation(LDA)\cite{blei2003latent} to detect the topic distribution of sentences,
    and put the sentence-to-topic probabilities as input(NonTopicDSDR-LDA).

To compare with other methods of generating sentence representation, we implement the following sentence representation methods as the baseline systems.
(1) VSM. (2) RAE\cite{socher2011dynamic}. (3) average of the word vectors(AWV) generate by word2vec\footnote{https://code.google.com/p/word2vec/}\cite{mikolov2013efficient} in a sentence.
All the three methods learn vector-space representations for sentences, we just put the vectors to NonTopicDSDR and generate the summary.
% TODO add more description of baseline models
In method (1), we filled the sentence vector with word's TFIDF\cite{salton1988term}, the vector length is the size of the vocabulary.
In method (2), We used the Stanford parser\cite{klein2003accurate} to create the parse trees for all
sentences. For initial word embeddings for RAE we used the 100-dimensional vectors computed via word2vec.
All the models are trained on cleaned-corpus.

\begin{table}[h]
    \caption{DRWS-0.5 is our model with $\lambda=0.5$ in Equation\eqref{eq:final-score} which is the final score formula in Huang\cite{huang2012improving}. DRWS-0.6 is our model with $\lambda=0.6$.}
\label{table:word-vector}
\caption{Comparison results on DUC2006}
\centering
\setlength{\tabcolsep}{9pt}
\begin{tabular}{lllll}
\hline
Algorithm       &   Rouge-1 &   Rouge-2 &   Rouge-3 &   Rouge-L \\ \hline
NonTopicDSDR-LDA    &   0.37635 &   0.07073 &   0.34172 &   0.1319  \\
VSM             &   0.33168 &   0.06047 &   0.01482 &   0.29850 \\
RAE             &   0.38032 &   0.06723 &   0.32741 &   0.12048 \\
AWV             &   0.38273 &   0.07121 &   0.34942 &   0.13597 \\
DRWS-0.5    &   0.38822 &   0.07662 &   0.36271 &   0.14201 \\
DRWS-0.6    &   \textbf{0.39022}&   \textbf{0.07931} &   \textbf{0.37212} &   \textbf{0.14873} \\
\hline
\end{tabular}
\end{table}

We show two kinds of $F$ measures of ROUGE metrics: ROUGE-N and ROUGE-L.
ROUGE-N is an $n$-gram recall between a candidate summary and a set of reference summaries and ROUGE-L uses the longest common subsequence (LCS) matric.
% TODO add the analysis of result
Table \ref{table:word-vector} shows the experiment results between our sentence vector between other sentence vectors.
From the results, we can observe that
VSM get the lowest score, because it is very space for each sentence, and can't capture the relationship between words in a sentence.
NonTopicDSDR-LDA focus on the distribution of sentence, without the syntax. Scores of RAE and AWV are very close and are better than NonTopicDSDR-LDA.
DRWS with $\lambda=0.6$ get the best result.


\section{Conclusions and Future Work}
% TODO final add
In this paper, we propose a new model named DRWS which can learn distributed representations for words and setences.
The vectors are learned via learning to discriminate the next word given a short word sequence (local context) and the sentence (global context) in which
    the word sequence occurs.
DRWS has two periods to learn the vectors,
    in the first period,
    it learns word vector representations and in the second period,
    the model learns sentence vectors based on the parameters learned in the first period.
We take the tasks of word similarity and text summarization to evaluate the word vectors  and sentence vectors learned by DRWS.
The good performance demonstrates the effectiveness of DRWS in capturing the semantics of words and sentences.
In future work, we will try to cluter the setences of data set semantically, and the cluster may be a better choice for each sentence.

\subsubsection*{Acknowledgments.} We thank the anonymous reviewers for their valuable and constructive comments. This work is partially supported by NSFC Grant No.61272340.


\bibliographystyle{plain}%Choose a bibliograhpic style
\bibliography{re-second-version}




\end{document}
