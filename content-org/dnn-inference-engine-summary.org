#+title: 推理引擎之伤
#+author: Chunwei Yan
#+subtitle:
#+date: 2022-10-15
#+hugo_tags: "inference engine"
#+hugo_draft: true
#+hugo_base_dir: ../
#+hugo_section: ./posts
#+toc: headlines 2

时间飞逝，离开 DL Framework 方向已经接近半年，中间火速扑到新的技术方向上， 脑海里之前工作的很多细节都已经逐渐淡忘，只依稀记得一些困扰比较多的难点，想想也是时候做个总结了，正好遗留下来的琐碎的记忆偏少，逻辑化的思考居多。

我进入 DL Framework 大概是在 2017年，比较久远了。 那个时候国内开源的 DL Framework 貌似就一两家，PaddlePaddle 属于其中比较早的了。
而 2022 年的今天，市场上有包括 PaddlePaddle, MindSpore, OneFlow, MegEngine 等等多家国产的 DNN Framework ，而且每个都生气盎然，寻求到了自己在生态或业务中的不可缺少的位置。
有幸在新的岗位接触到了这些 DNN Framework 来的同事，交流下来发现越来各家内部技术也都很能打，很庆幸经过几年的迭代依旧是百家争鸣的态势，依旧会有更多有趣的技术出来。

大家在使用 DL Framework 时，几乎99%以上的人都不需要太关心底层的架构系统。
各个 Framework 也都很关注用户体验，尽量将繁琐的细节隐藏在 Python 漂亮的界面下，营造出一种上手很简单的感觉。
但隐藏在底层的框架技术也是日益复杂，远如 PyTorch 和 TensorFlow，不光带动了业内的 DL Framework 的革命，同时也在 Eager mode，Big model 和 AI Compiler 上自我革命。近如国内的各家 DL Framework，每个技术特点都有所专长并且长期深耕。

Training 尚可以有 Python 的界面让大家有掌控的感觉。Inference 则大部分时候只能有 C++ 甚至 object-C 等并不轻松的用户界面，加上部署端服务器或者手机上部署的琐碎问题。
工业化的推理部署任务，在非专业人员面前（带有一定的经验和心理准备），大概率是会要抓瞎踩坑的。

本文不同于各家的PR稿里面对 Inference 技术的云淡风轻，作为直接从业者，我觉得 DL Inference 的技术很难，很有挑战，当然也非常有趣味。

在我之前的经历里，一个完整的 DL Framework 的生态中迭代多年的推理系统，零零碎碎面临的一些问题甚至会让人觉得匪夷所思，但是如果充分了解了背景和历史负担，尝试深入地去理解本质，头脑风暴出一些深远的方法，这个过程又会觉得很有趣。

这篇总结的风格我想侧重在推理技术迭代中个人发现的难点及问题，一些经验思考及之前工作的不足。不算是吐酸水吧，只是交流一些有趣的技术和演进的思考。

其中，我不会涉及过多的细节（也记不住），所有的信息都直接来源于公开的代码信息，以及一些已经有代码开源出来的的解决方法。

* 关于我
本人在 PaddlePaddle 工作五年，参与最近一代 Fluid 架构的原型工作。后续集中在推理部署领域工作四年，负责了 PaddlePaddle 推理部署方向大部分设施的架构。

* DL Inference Engine 简介
深度学习推理引擎，这个名词相比所谓“大模型”，“超大规模”，“动态图”这些训练上的名词要小众一些，更多聚焦在工业级服务圈子里了。

我们知道深度学习，至少需要先编写一个模型配置，之后执行训练产出一个模型；而推理部署则在这个过程之后，对应的 DL Framework 会提供一个 SDK，便于将产出的模型文件应用到业务的 Service 中。

# TODO image explain the process

其中的 SDK 就是本文的主角 —— Inference Engine，一般作为 DL Framework 比较底层的能力（产品）释放出来。由于要直接嵌入到业务已有的 Service 中，也算是桥接了 DL Framework 的核心能力与业务的 Service，两个 End 的变量和需求叉乘起来出现的各类问题基本够成了本文会讨论的一系列的问题。


* 七伤

** 稳定性，稳定性，稳定性
*** 功亏一篑
- API
- Pass
- Op
*** CE

*** autoscan



** 分分合合，一套还是两套
*** 推理引擎，Server-class 和 on-device engine
*** Training 和 Inference

** 性能和体积
*** 硬件优化
*** 框架性能
*** 体积控制

** 硬件集成
*** 硬件井喷
*** TensorRT子图

** 算子兼容性
*** Training 和 Inference
*** 不同版本间

** 业务支持 vs 新 Feature
*** 业务驱动
*** 历史负担

** 框架技术，从来都是牛刀小试

** 何去何从
产品化的思考

* 尾声
